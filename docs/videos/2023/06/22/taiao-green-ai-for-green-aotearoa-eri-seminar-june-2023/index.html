<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><title>We Are The University</title><link rel="stylesheet" href="/styles.css"></head><body><header><h1 style="color: #fff;font-family: 'Arial Black', Gadget, sans-serif;font-style: italic;font-weight: 900;text-transform: uppercase;">We Are The University    </h1><nav><ul><li><a href="/">Home</a></li><li><a href="/about">About</a></li><li><a href="/contact">Contact</a></li><li><a href="/blog">Blog</a></li><li><a href="/videos">Videos</a></li><li><a href="/authors">Authors</a></li></ul></nav></header><main><h2 style="text-align: center;">TAIAO - Green AI for Green Aotearoa - ERI Seminar - June 2023 [46:43]</h2><p style="text-align: center;"><a href="https://www.youtube.com/watch?v=aQj3Zm75L2U" target="_blank">Watch on Youtube</a></p><p style="text-align: center;"><a href="https://www.youtube.com/channel/UCYzot7AKCB5paA4vp8wXuvw" target="_blank">The University of Waikato</a></p><img src="https://i.ytimg.com/vi/aQj3Zm75L2U/maxresdefault.jpg" alt="Thumbnail for video titled: TAIAO - Green AI for Green Aotearoa - ERI Seminar - June 2023" style="width: 100%;"><div class="tags"><span style="background: #0003; border-radius: 0.3em; padding: 0.3em; display: inline-block; margin: 0.2em; font-size: 0.8em">#artificial intelligence</span><span style="background: #0003; border-radius: 0.3em; padding: 0.3em; display: inline-block; margin: 0.2em; font-size: 0.8em">#AI research</span><span style="background: #0003; border-radius: 0.3em; padding: 0.3em; display: inline-block; margin: 0.2em; font-size: 0.8em">#TAIAO</span><span style="background: #0003; border-radius: 0.3em; padding: 0.3em; display: inline-block; margin: 0.2em; font-size: 0.8em">#sustainability</span></div><h2>Description</h2><p>Albert will present the TAIAO research project, focusing specifically on the dual core strategies of Green AI: employing AI to tackle sustainability issues and adopting AI in a more sustainable way. With AI increasingly serving as a significant asset in Aotearoa, it is enhancing problem-solving capabilities and boosting research efficiency.<br><br>Professor Albert Bifet is the Director of the Te Ipu o Te Mahara AI Institute at the University of Waikato, New Zealand, and leader of the TAIAO Data Science Program funded by MBIE. He is the co-author of a book on Machine Learning from Data Streams published at MIT Press. He is one of the leaders of MOA, scikit-multiflow and SAMOA software environments for implementing algorithms and running experiments for online learning from evolving data streams. He will be the general co-chair of ECML-PKDD 2024.</p><h2>Transcript</h2><p style="opacity: 0.9; font-size: 0.8em">Transcripts may be automatically generated and may not be 100% accurate.</p><p>uh and also at the same time I'd like to<br>talk about green AI because I think<br>that's a a very interesting topic right<br>now and I think that yeah it combines<br>you see that it's a perfect combination<br>and also specific for New Zealand<br>because that's uh I think that's one of<br>the main features of New Zealand is this<br>uh care for for the environment<br>so yeah as Charlie mentioned uh uh I'd<br>like to introduce you a little bit about<br>what's the entire project so so what we<br>do this is a project funded by mbie<br>seven years but this is with University<br>of Auckland Canterbury Wellington and<br>also back and Med service<br>and yeah the main goal is how we can use<br>AI in environmental science how we can<br>use data science to improve How We Do<br>environmental science and uh yeah I<br>think that's really really<br>um perfect for for New Zealand in the<br>sense that uh as if you ask anybody<br>about overseas about New Zealand they<br>will always imagine this paradise that<br>is green with all of this uh nice uh<br>um yeah these nice pictures and so I<br>think that's uh that's something that uh<br>yeah it's very important to to keep New<br>Zealand uh agreed and environment and<br>also the same time I like to talk about<br>green AI not only in the sense that we<br>would work with environment but also How<br>We Do AI right so we can do AI spending<br>a lot of resources or we can do that<br>using a small amount of resources and I<br>think also New Zealand is perfect for<br>that because there is this number eight<br>wire mentality the idea is that uh yeah<br>let's try to be<br>smart and let's try to do things using<br>the the small amount of resources and I<br>think that yeah this number eight wire<br>is a perfect example of that<br>so as yeah as you know uh yeah AI is<br>becoming very very important especially<br>after uh November last year when GPT was<br>released and now everybody's talking<br>about Ai and how AI is going to be<br>really really changing everything and<br>that could be something like internet<br>could be an important important<br>Revolution<br>so yeah for<br>this time uh GPT was really fast in<br>taking<br>um in having one million users so that<br>was something that never happened before<br>so there's always things are much slower<br>but this time it came very very fast so<br>yeah I think it it was uh yeah in five<br>days<br>um yeah 20pd was able to get already one<br>million and and um yeah it's it's<br>changing how how we do things so we're<br>thinking of AI as something that was<br>coming and I think that now everybody<br>knows that AI is already here and uh<br>yeah we need to be<br>um<br>smart of how we use this this technology<br>so what I'm going to try is to First<br>explain a little bit what is AI and what<br>is not Ai and then from there I'll I'll<br>see how we can use AI to to solve<br>sustainable challenges and that's the<br>entire project and then I like to to<br>also mention about how we do AI in a<br>green way how we can do Ai and not using<br>so to start what is AI so the<br>yeah you there are many many definitions<br>I think at the moment what we can see is<br>that AI is a better way to do computer<br>science it's a uh and let's say an<br>automated way of doing computer science<br>right so it's a it's a really uh uh<br>computer science on the steroid so it's<br>how we can do these automations in a<br>much much faster way and you can see<br>that in terms of popularity yes in the<br>release of uh GPT AI has uh increased<br>its popularity and uh yeah it's it's<br>many people so for example Donald so<br>he's one of the<br>most<br>prestigious computer scientists he is<br>already saying that yeah but basically<br>computer science is going to be renamed<br>machine learning in the sense that uh<br>yeah if you want to do cool things right<br>now in computer science you need to be<br>doing that on machine learning and yeah<br>so there are many definitions on there<br>so I really like this one so that's the<br>one from the European commission and<br>it's very simple but I think it's going<br>to be very helpful to understand what AI<br>is and what AI is not so AI is a<br>collection of technology that combine<br>data algorithm and computing power and<br>that's I think is the key because this<br>is what we need or AI we need data we<br>need algorithms and we need computing<br>power and this is going to to give a lot<br>of insights on how we can do Ai and how<br>we cannot do Ai and what are the risks<br>and challenges of AI<br>so let's start with data so<br>at the moment all the Technologies on on<br>AI are mainly based on on data so we can<br>see that<br>yeah we are generating so much amount of<br>data continuously and uh that may be<br>that's the reason why we are talking<br>about AI right now and we are not<br>talking about AI 10 years ago 20 years<br>ago is because now we have a lot a lot<br>of data and a lot of computing power to<br>to process this data so that what makes<br>the difference<br>and of course it's not a coincidence<br>that the organizations that are living<br>on AI or the countries that are dealing<br>on AI are the ones that they have more<br>data so let's say that uh we can see<br>that the core of AI is data yeah if you<br>don't have the right data you're not<br>going to have the right AI so AI is is<br>based really really in data so and and<br>that's why it's very important to<br>understand that when we are talking<br>focusing on how to get the the the right<br>data<br>so just to mention uh Harari the the<br>author of the book sapiens<br>um yeah he said that if you want to make<br>a country Colony don't send the tanks in<br>just get that out so it's really really<br>important right now the data is<br>something that if you have control about<br>the data you have control about<br>everything<br>and this is something that also that<br>applies here in in New Zealand where<br>there is uh yeah from the Maori<br>perspective data sovereignty is really<br>really important and this is why uh yeah<br>from technical media says yeah data is<br>the Last Frontier of colonization so<br>again it's very important to understand<br>that data is the most important thing in<br>terms of AI<br>okay so now we have the data so now how<br>we Analyze That So how we use it to to<br>extract information and to make to make<br>predictions or to create new things so<br>that's why the algorithms are the<br>SE important factor so let me show you<br>uh some examples so imagine that we have<br>a website we are working in New Zealand<br>and we want to we have visitors to the<br>website and we want to predict if a<br>visitor is going to be a customer or not<br>so how we do that well mainly what we<br>the standard ways that we have someone<br>that has a lot of experience and then<br>they can make their predictions with<br>domain knowledge no if someone is going<br>to be a customer or not<br>um what about if we have the data then<br>we can try to to make a an algorithm and<br>create a model that is the one that is<br>going to be able to make a prediction so<br>in this case yeah we want to we have the<br>data points and there are customers and<br>non-customers and then given a new point<br>a new visitor we want to detect that<br>this is going to be a customer or not<br>and we're going to do that based on the<br>data so<br>let's start by the most simple way to do<br>that is<br>okay we<br>we look at what is the majority<br>um visitor so in this case we see that<br>majority visitors are non-customers or<br>we make the prediction is a non-customer<br>very easy we are not learning many<br>things so it's very very basic but we<br>always need to compare with that because<br>at least we need to be as good as this<br>then of course we can try to improve<br>that we can have a linear classifier<br>basically it's a line now where we<br>separate between customers and<br>non-customers and then from that we can<br>decide if if the visitor is going to be<br>a customer or not<br>another model that we can create is the<br>nearest neighbors so the idea is that<br>okay let's look at the similarity so we<br>see here what are the nearest visitors<br>and see and take the majority class of<br>of that for example in this case we take<br>look at the two nearest neighbors the<br>nearest visitors and then with the tech<br>we predict the class of these visitors<br>for example in this case they are<br>customers so we predict that's going to<br>be custom<br>we have decision trees<br>um that's nice thing of a decision tree<br>that is very easy to understand how the<br>predictions are going to be made uh for<br>example in this case yeah we can<br>decide if it's from North Island or<br>South Island and then in the North<br>Island language region and then we can<br>see that uh yeah we have a prediction<br>for each one of this so it's uh when I<br>think of these decision trees is that we<br>really understand how decisions are made<br>right and this is very very important uh<br>when we are making decisions to to<br>really be able to explain how these<br>decisions are made<br>and um yeah way to improve that is<br>instead of having only one uh model is<br>having a combination of different models<br>and for example there is this famous<br>Ensemble that is called random Forest<br>basically the idea is that instead of<br>buildings we are going to combine<br>decision trees but instead of combining<br>perfect decision trees we are going to<br>combine random decision trees so that's<br>the easy way to to do that<br>finally deep learning<br>so yeah as you know the the big thing<br>right now on AI is uh mainly based on<br>deep learning so and deep learning is<br>based on this neural network so let me<br>give you an example so imagine that we<br>are detecting uh images the problem with<br>images is that then we don't know what<br>are the features that we need to use so<br>yeah imagine in this case if you want to<br>so how we are going to do that so well<br>we need to find some features right and<br>then based on the features we are able<br>to decide if this is going to be one<br>animal or not the problem is how we<br>detect this feature right so how we are<br>going to do this feature and then this<br>is why these neural networks are so so<br>useful because they can do that<br>automatically<br>so what these neural networks are going<br>to do are going to build and find these<br>intermediate features that are the ones<br>that we're going to decide uh when we<br>um yeah<br>and finally yeah in the case that we<br>don't have the labels so we don't know<br>if there are customers or not customers<br>what we can do is that we can group that<br>and then this is something that is<br>called clustering and there are<br>different methods like it means and DB<br>scan the yes that okay what we do is<br>that we put all the uh similar<br>visitors together and from there we can<br>uh make some predictions<br>okay so now what we can do and what we<br>cannot do so data is really really<br>important so for example imagine a<br>self-driving car so what we can do we<br>can do whatever was in the data so for<br>example if we we can predict uh yeah we<br>can detect cars we can see what are the<br>cars that are coming everything that was<br>on the data it's perfect we can do what<br>we cannot do is things that were<br>um that yeah they were not in the in the<br>data for example yeah imagine there's a<br>self-driving car we have a person that<br>is starting making signs<br>uh if it's only one or two yeah we can<br>the model is not able to learn<br>so yeah<br>that's the limitation right now so the<br>limitation is that everything is based<br>on data so if we need to improve that we<br>need more data and until we don't have<br>more data we are not able to to to to do<br>that so this is why if you look at these<br>self-driving cars what they are doing is<br>basically getting data data data so that<br>over time they can improve what they are<br>doing<br>okay and then finally computing power so<br>um yeah this is one of the main<br>challenges we are building all of these<br>big models but these big models are very<br>very expensive in terms of energy so for<br>example yeah that was<br>um<br>or years ago that creating an AI can be<br>five times worse for the planet than a<br>car but yes they were getting worse and<br>worse so this is the new one we uh<br>here the<br>Village<br>63 building gptc was 500 and this is not<br>gpt4 so you can imagine that the amount<br>using is really really increasing and<br>the reason for that is because the<br>models and the data is bigger and bigger<br>so the the size of the models are are<br>increasing exponentially and the the<br>data that we are using also is uh<br>increasing so that's why yeah we are<br>using a lot a lot of energy for that<br>and uh yeah so now that I I explained a<br>little bit about this AI now we can see<br>for example check GPT what it is in<br>terms of these uh definition right so if<br>you look at GPT basically it's a lot of<br>data with a very smart algorithm and a<br>lot a lot of computational power so this<br>is what is basically its LGBT okay so<br>what is the data with the node this is<br>uh it's closed so it's interesting this<br>is open AI but uh in this case it's<br>close we don't know what's there and<br>this is why the new legislation in<br>Europe is trying to not allow this so we<br>really need to know what uh in the in<br>the the data I used to build the model<br>and yeah the algorithm so it's using<br>this new Transformer architecture and<br>it's very interesting that the the<br>algorithm is basically predicting what's<br>the next word<br>so yeah and then what is getting is uh<br>the probability distribution of uh what<br>is going to be the next word and then<br>from that it makes this prediction so<br>this is what is this large language<br>model this is what is based on this<br>Transformer architecture the most<br>interesting thing is that that's not<br>only that said gbd also has this<br>reinforcement learning from Human<br>feedback and this is really important<br>because this is the one that allows to<br>do a lot of mistakes<br>the previous large numbers models were<br>doing a lot of mistakes they were moving<br>they were were not polite they were<br>saying very a lot of things that were<br>not really uh<br>let's say not correct but also doesn't<br>then they were not uh<br>could cause a lot of trouble so then I<br>think of this very enforcement learning<br>is that it's helping to avoid these<br>cases so still there are some cases and<br>these models they hallucinate but it's<br>getting it's really really helping to to<br>improve how how this is done<br>okay so yeah so that was<br>some explanation on AI so now like to<br>talk specific about Tire<br>um yeah it's it's I think it's a project<br>that started in 20<br>20. and uh yeah the I think was really<br>really interesting at the time because<br>that was before the pandemic and then<br>everybody was talking about that climate<br>change was the biggest challenge of of<br>the time still I think it's that so some<br>people will say that there are other big<br>challenges but I think still climate<br>changes is one of the the most important<br>ones and um yeah the entire project uh<br>what he's doing is basically creating a<br>fundamental data science so we create<br>new algorithms new methods but also at<br>the same time we want to to provide<br>tools and to grow capabilities and<br>specifically have applications case<br>studies in environmental science<br>so yeah the team is really is<br>heterogeneous we have<br>researchers machine learning data<br>scientists environmental scientists over<br>engineers and we are dealing with a lot<br>of different data sets a large special<br>and temporals<br>um we have a<br>different work packages one is on time<br>series and streaming so on evolving data<br>and then we have other narrower package<br>and extreme events and another word<br>package on deep learning how we analyze<br>images<br>and uh yeah in terms of the benefit for<br>New Zealand so yeah I think that the<br>core is that we really want to to<br>deliver excellent research so it means<br>that we need to do something that is<br>done at the<br>and fundamental data science<br>environmental science that is excellent<br>internationally so this is our main goal<br>we are very interested in growing<br>capabilities of the next generation of<br>uh<br>researchers and<br>and developers and<br>um in in both sides in data science and<br>in environmental science and<br>um yeah and uh one of the main goals is<br>how to<br>grow these collaboration how we we<br>really make everybody work together so<br>that we can improve what we do and yeah<br>I finally would like also to<br>to give effect to the vision mataranga<br>so in terms of what we did so we have<br>this nice Community website so this is<br>is it's daya.ai you go to the website<br>you will see that<br>we have different things we have data<br>sets we have notebooks we have software<br>and then we have discussions<br>so the idea is that we are not storing<br>data sets but what we're doing is that<br>we are a catalog of different data sets<br>um yeah you can find there all the data<br>sets the different descriptions on how<br>we have the notebooks so this is our<br>notebooks in Jupiter so<br>the main idea of notebooks is that we<br>have in the same<br>document we have the source code<br>documentation and results and this is<br>really really convenient in terms of<br>data science<br>yeah it's ready to have this link with<br>the data set and notebook so that people<br>can use these to<br>um yeah to to improve how they do data<br>science<br>um we are working also on a specific<br>software so we have this River software<br>this is a new python software for<br>streaming data so this is data that is<br>evolving over time and that we are not<br>storing the data we are processing<br>examples by by example so I'll talk<br>about this data<br>we have these species identifier the<br>species classifier so this is specific<br>for New Zealand and this is something<br>that we have the website we have also on<br>iPhone app and we have an Android app<br>and you can see the idea that we take a<br>picture and then we get uh what's the<br>predictions of what's the what is the<br>species and then also we have somebody<br>where we can understand the the<br>explanation of why the model is<br>we have been working also with Nvidia on<br>the speeding up machine learning models<br>with accelerated worker so the idea here<br>is to where can principle is not using<br>GPU so the idea was to have a<br>accelerated worker that was able to to<br>use gpus and this is a collaboration<br>that we did with Nvidia<br>and one of the<br>I think one of the contributions that we<br>want to to have in this project is this<br>data major architecture to use this data<br>mesh architecture in applications so the<br>idea is that instead of having all the<br>data analytics centralized in one place<br>we like to have that data distributed<br>that architecture and for that like this<br>data mesh architecture so what's the<br>idea is that we are going to to consider<br>data as products<br>okay then each data product is going to<br>have a a domain owner<br>we're going to have a platform that is<br>going to serve all of these<br>usage is and finally we are going to<br>have this Federated computational<br>algorithms so what's the the idea here<br>the idea is we would like to have<br>something that is efficient<br>and something that is distributed at the<br>same time and these databases are really<br>New Concept that appear I think three<br>years ago and that is really based for<br>Big Data applications so this is<br>something that is being applied in<br>finance in big organizations with a lot<br>of data because is the most efficient<br>way to do things faster but also having<br>less mistakes than having everything<br>centralized in one place<br>okay and then in terms of the projects<br>that we are working so one of the<br>the main focus is explainable AI so we<br>are working on how to<br>um yeah explain methods we really think<br>yeah we all have all these models that<br>they need the prediction but it's very<br>important to have an explanation because<br>we don't have the explanation these<br>models may be doing mistakes and we need<br>feedback to check that everything is<br>working properly so this is why explain<br>Roi is really really important the thing<br>right now is that there are many many<br>different methods on explain o Ai and we<br>are interested and see what are the best<br>ones and and how to get the most of this<br>technology<br>we have this collaboration with the<br>forest flow project from Scion and the<br>idea there is that they have these<br>sensors in this forest and they would<br>like to analyze that on real time so<br>what we are doing in this case is we are<br>playing this data Mage architecture<br>there so we can have all of these data<br>that is<br>analyzing in in real time and we are<br>using this machine learning for data<br>streams and I'm serious to see how we<br>can get all of these predictions on on<br>real time<br>we have another application of this<br>database that is this flood impact<br>impact prediction so this is a project<br>with waikato Regional Council and we are<br>using data from from from them and also<br>data from metservice and the idea is to<br>have a system that is able to make<br>predictions on on floods<br>um yeah how we do that so the idea is<br>that we get this data from rain radar<br>from that service this data from waikato<br>Regional Council and we consider this as<br>DACA products in the data mesh we get<br>the data and then we use this to make<br>the prediction and then to visualize in<br>that dashboard<br>so yeah I think that that's a nice<br>application on where we can use these<br>Technologies or for predictions we<br>started using lstns but now we are<br>already starting using the time series<br>Transformers<br>um another application is in in deep<br>learning where we look at uh<br>uh in this case we'll look at the<br>investigative<br>we're looking at sisters and the idea<br>that we don't have to to too much data<br>another idea here is to use this work<br>week supervision to be able to to find<br>that so again this is something very<br>useful and also it's it's good because<br>it's based on explainability so we can<br>understand how these models work<br>another project is this machine learning<br>for algal blunt detection this is by<br>University of Auckland and Sharon so the<br>idea is that<br>yeah the traditional way to to detect<br>how Bloom is going to the lake and<br>measure that so the idea is can we use<br>uh satellite images to do that so that's<br>the idea so the idea is how we can use<br>machine learning so that we<br>um yeah we we save having to go to the<br>places to measure we can have these<br>measures the estimations uh using<br>detecting from from images so in this<br>case yeah we have these images from the<br>to do that<br>and finally another one is an extreme<br>event so we like to to measure these<br>Marine heat waves and to to get<br>predictions on that<br>um yeah this is something that is uh<br>quite uh challenging the data is<br>complicated so you'll see that in many<br>many applications uh we need to spend a<br>lot of time with the data because the<br>data is really the one that we need to<br>to get it right uh but still we have so<br>many algorithm and methods that we can<br>deal with when data is not not perfect<br>for in this case for example in this<br>case data is in Balance there's a small<br>number of observations so yeah we can<br>use different algorithms in this case we<br>use this graph<br>graph neural networks and that's a way<br>that we can use to to build these models<br>and then we can use to make this<br>predictions and this is from University<br>right so yeah that was a more or less<br>some of the projects that we're working<br>in<br>yeah maybe now I like to<br>finish talking about how we can do<br>machine learning in a Greenway how we<br>can do that in a more efficient way and<br>in this case uh yeah it's very<br>interesting that this is uh what we call<br>incremental learning or streaming<br>learning is perfect for that so let me<br>show you<br>um yeah there's an example so the<br>standard way to do machine learning is<br>that we have all the data in one place<br>becoming memory or in disk and then we<br>build the model and then we use this<br>model to make predictions<br>so that's the standard way so what's the<br>main problem with that first that we<br>need to have all the data in one place<br>and the second thing is that the the<br>data needs to be static the data cannot<br>be changing over time<br>so for that uh<br>um when data is changing the standard<br>way to do that was to train a model<br>periodically so let's see okay we are<br>going to train the model every week<br>and we are going to use the the data of<br>the last month<br>so what's the problem of that the<br>problem is that<br>yeah we don't know<br>how often we need to retrain the model<br>how much data we need to to use in the<br>model and that depends on the change<br>right because if the changes every day<br>doesn't make sense that will build the<br>model every month or if the data is<br>changing every month also make sense<br>that we build the model every day<br>so yeah this is all the things that make<br>this difficult and this is why this uh<br>data stream approach is quite good<br>because it's deciding all of this in an<br>automatic way<br>so the idea is very simple so instead of<br>storing the data what we are going to do<br>is that every time a new example arrives<br>we are going to update the model and we<br>are going to do that continuously<br>so this is yeah this data stream uh<br>approach so the the nice thing is that<br>it's very efficient<br>and the second thing is that it's<br>adaptive so if thing changes over time<br>the the models are going to be a change<br>over time so for example imagine we have<br>a decision tree so if there is change on<br>the decision tree the branches are going<br>to disappear new branches are going to<br>be created everything is done<br>automatically<br>so yeah this is our the main properties<br>of the streaming machine learning and<br>this is why it's uh<br>yeah I think it's very useful is because<br>it's very efficient in terms of memory<br>and time and also is adaptive so if<br>there are changes on the streams this is<br>going to be<br>as we are dealing with um with um<br>with a lot of data we sometimes we we're<br>going to be happy with approximate<br>Solutions so if Computing the exact<br>solution will require I don't know one<br>week of computation and we can have<br>something that works in seconds we'll<br>prefer something that works in seconds<br>so this is why what we say is that um<br>yeah we're going to have a solution<br>because there's more error<br>Epsilon with high probability one minus<br>Delta so means that we are going to be<br>happy if we have a small error if with<br>this we really really speed up how we do<br>things and usually the nice thing is<br>that we can decide<br>um<br>the size of the error right so we can<br>really really choose a small errors<br>so yeah let me show you an example so<br>imagine that yeah we want to store in<br>eight bits we want to to count imagine<br>that we are counting events and but we<br>only have eight bits so how many events<br>we can count in eight in eight bits so<br>we compute that exactly this is going to<br>be 255.<br>but the that's very interesting because<br>that was the first paper on on streams<br>um the idea is that there was this<br>counting large number of events in small<br>registers by Bob Morris he was able only<br>having eight bits uh he was able to to<br>Store 130 000 events<br>that's that's a huge amount so how he<br>was doing that well basically the idea<br>is that instead starting the number we<br>store the logarith of the number<br>and and the the way to do that was very<br>very simple so the idea is the standard<br>way is that we have a counter and every<br>time there is an event we add one<br>so if we want to have an approximate<br>solution we instead of uh adding one<br>always we're going to add one only with<br>probability dtp<br>so it means that we toss a coin and then<br>we decide when we add one or not and<br>that's the nice thing is that doing that<br>what we are storing is the logarithm of<br>the number<br>and this is something that has been used<br>by for example by Facebook there's this<br>famous counters that they they use to<br>complete the degrees of separation of<br>all the users in Facebook<br>so imagine the the<br>number of users and then you want to to<br>compute the average of the distance of<br>each user to all the other users in<br>Facebook<br>that's really really a large amount of<br>computation but doing using these<br>approximate methods they were able to do<br>that in a very very efficient way<br>um yeah that that comes from this<br>experiment from Milgram in the 60s that<br>the uh he did this experiment in the U.S<br>and then to send one letter from one<br>Coast to the other coast of the US was<br>uh it needed six persons so the degrees<br>of separation was six but now yeah if<br>you want to compute that that Facebook<br>is the best way to do that and it's very<br>interesting because the first the first<br>computation in 2011 was about four<br>degrees of separation and in 2016 was<br>three and a half Degrees of Separation<br>so yeah that's a nice thing of of this<br>technology that allows us to to do that<br>very efficient so the impressive thing<br>is that in 2011 they were able to do<br>that using these streaming methods using<br>only one machine so yeah and that's for<br>all the numbers of users on Facebook<br>okay and then<br>um yeah how we can speed up things so<br>for example a decision tree so decision<br>three as you know it's very nice method<br>because it's explainable we can<br>understand how the decisions are made<br>and they are very powerful because when<br>we combine them in ensembles the<br>resulting methods are very very good the<br>problem of the decision is that it's not<br>a streaming method<br>um because when we need to decide how to<br>grow the tree we need to look at the<br>data that is in memory and then use<br>um yeah look at what is the best feature<br>to use to grow the the tree<br>so this is why<br>um yeah there was this a streaming<br>decision tree that were that is called<br>very fast decision trip that the idea is<br>that okay when we want to decide if we<br>grow the tree or not if we split or not<br>because we are going to do that we are<br>going to wait to have new examples<br>once we have new examples then we decide<br>if we split or not so in this way we<br>don't need to store anything in memory<br>if we need to make a decision we wait to<br>new examples to arrive now the question<br>is how many examples<br>and for that we use the housing bound<br>and this is why these decision trees are<br>called half increase<br>so it's very simple it's it's a very<br>Fascination tree that is grown and trees<br>are are grown in a way that yeah when we<br>need to decide if we split or not we<br>look at the difference between the the<br>benefits of splitting and then we use<br>this housing bound to decide if we split<br>or not<br>so that's for one specific decision free<br>uh what we did is that we the random<br>Forest is a method that is very very<br>popular so we built a streaming version<br>of that and we call it adapted random<br>Forest because it's streaming and it's<br>adaptive and um yeah the idea is very<br>very simple so the idea is that we<br>combine uh what is called bugging and<br>random tree so what what are random<br>trees random trees are are trees that<br>when we need to decide if we split or<br>not if we grow the tree or not instead<br>of using all the attributes we're going<br>to use only a random number of<br>attributes and this is how we grow the<br>thing is that the trees are not good<br>because they are basically random but<br>the combination is much better than<br>having a perfect trigger this is why<br>these Ensemble methods are getting very<br>very popular so here for this adapted<br>random Forest we combine what is called<br>bugging means that also we do we want<br>the trees to be different so what we do<br>is that instead of providing the trees<br>with the same data we are going to<br>provide the<br>with uh we are going to provide each<br>three A variation of the data that is<br>something with replacement so it's from<br>the same distribution of the data but<br>it's different data sets and this is<br>going to help also to have diverse trees<br>that are different and this is going to<br>to help to have a strong classifier at<br>the end<br>and uh yeah the thing of these uh random<br>Forest is that it works only with trees<br>right and what we did is that we extend<br>this to random patches means that we can<br>use it for any classifier what we are<br>doing is that we are<br>um deciding which features which random<br>features we are going to use for all the<br>classifier and then yeah we build these<br>different classifiers we combine them<br>and then we get a good combination<br>so yeah this is what is this is a<br>streaming random patches and it's very<br>interesting because in terms of results<br>the results are much better than uh<br>having um<br>um yeah then using the random trees so<br>same idea but at the end seems that at<br>least in streaming is working better<br>and finally a method that is also very<br>popular is based on New Year's neighbors<br>so means in similarities so there was<br>this uh best paper award that icdm 2016.<br>that the idea was to have like two types<br>of memory a short-term memory and a<br>long-term memory the short-term memory<br>is basically the last examples they have<br>arrived and the longer memory is where<br>we try to using clustering we try to get<br>what are the most representative<br>examples that we are going to use for<br>this long-term memory and then using<br>these two we are really really good and<br>um yeah so what we did is that we we put<br>that into ensembles so instead of using<br>one model we use several models together<br>and these uh<br>um yeah these games are very good uh<br>results<br>okay and yeah just to conclude just<br>um mention some open source too so as<br>you know mycato is very well known<br>internationally for weka the software on<br>machine learning in Java with more than<br>10 million downloads and more than 18<br>000 research<br>citations<br>um yeah has been been the most popular<br>software in Java we have more that is<br>like uh wake up at least specific for<br>streaming so we are not storing the data<br>so what we are doing is that we are<br>um<br>yeah we are updating the models<br>continuously and we are adaptive so<br>is in Java and he is yeah it's very<br>close it can<br>connect to weka and it's quite good we<br>have this book not MIT press that is<br>called machine learning for data strange<br>with practical examples in in more<br>that is open available so you can read<br>it online<br>and finally as python is becoming very<br>very popular we have now this River<br>software package so this is like more<br>but in in Python so you can use it in<br>junk books and and it's also something<br>that is<br>um yeah it's quite recently I think<br>we've released that in 2020 and<br>uh yeah interesting thing is that there<br>are already books that uh it is not us<br>or someone else that already write the<br>book in Reverse so that's very<br>impressive means that it's becoming<br>popular when other people write a<br>books about the software so that's<br>really really good<br>and um yeah I think that uh that's a<br>summary of what I wanted to talk talk a<br>little bit about the green AI how we<br>is these two things right work on on<br>green environmental topics problems but<br>also at the same time how we do AI in a<br>environmentally friendly way energy<br>efficient<br>and uh yeah just to mention that we have<br>these AI resources Association<br>that we have this annual conference in<br>last year and we are also quite active<br>on writing discussion and white paper so<br>we have this white paper on New Zealand<br>strategic approach for the national<br>strategy but recently this year we<br>launched this GPT and large language<br>models what are the implications for<br>policy makers<br>and yeah and finally just that we are<br>here<br>to talk and discuss and help you so<br>remember that uh<br>yeah data is really the core so we can<br>talk about what is the data what is the<br>the what are the best algorithms and<br>what how much computational power we<br>need and also yeah we are happy to help<br>on growing capabilities and see what are<br>the missing skills and how we can help<br>you on on this<br>brilliant thank you very much<br>thanks very much Albert um are there any<br>um I do want to acknowledge that you you<br>really try to cater for a very wide<br>spectrum of audience<br>um you know because we had a very nice<br>introduction to the basics of machine<br>learning the artificial intelligence but<br>at the end we sort of really had a<br>glimpse at The Cutting Edge stuff that's<br>being worked on it right IT addresses<br>one of the biggest challenges with<br>machine learning is which is of course<br>how do you how do you improve on it<br>without spending a massive amount of um<br>uh energy doing so every time right so<br>um a lot of people may be thinking of<br>questions I I have one and so one of the<br>questions when it comes to Adaptive AI<br>is how do you prevent your models from<br>unlearning things that it could do right<br>because if you had a massive influx of<br>data of a particular nature right in a<br>completely skew your your learning and<br>then the um Learners the things that I<br>knew how to do and toward the end you<br>talked about the same k n which is one<br>way of you know sort of transferring<br>some long-term memory things<br>um so it's transferring short-term<br>memory too long-term memory but I do<br>know that that that would be potentially<br>difficult if you are dealing with<br>something like a decision tree or if<br>it's so so is there a limitation on on<br>what type of algorithms could be used<br>with that approach yeah very good<br>question so I think that the in practice<br>what is happening is that<br>yeah data may be changing continuously<br>and then something yeah we built a model<br>in relevant right now but then things<br>happen and then<br>um but maybe then this again later on we<br>go to the same distribution of the data<br>at the beginning so I I think the main<br>approach then is to have like a pool of<br>classifiers you have a different pool of<br>classifiers and then when something new<br>appears first check if it's something<br>that was already before and then we can<br>reuse one of the classifiers and maybe<br>improve that or it's something that is<br>completely new and then we really need<br>to start collecting data and build<br>something new because we haven't seen<br>that before but yeah the key is uh this<br>trade-off between forgetting right and<br>learning because we don't want to forget<br>everything we want to forget only the<br>things that we are not going to use but<br>all the things that we can reuse yeah we<br>like to keep that in in a way that we<br>can we can use so I think that what I<br>have seen is that the people who do that<br>having these pool of classifiers on uh<br>some people they call it recurrent<br>cancer brief so you can say that it it<br>model of the data is like a concept and<br>then what we do is that we detect the<br>drift in the concepts but then we have<br>these recovering Concepts we store them<br>and when we see that something new is<br>appearing we try to see which one uh can<br>be the best one to use okay great<br>um are there any questions online<br>um if not I mean I think this is this is<br>not really meant to be a seminar where<br>people sort of understand<br>um you know everything that IL does that<br>it's really aiming to highlight the<br>capabilities here at the University both<br>within the um AI Institute as well as<br>those specifically facilitated through<br>Tire<br>um and so you know I do want to take<br>this opportunity to acknowledge that<br>this year we lost one of our Giants uh<br>in machine learning yeah and you know<br>Whitton passed away earlier this year<br>but we're very happy to see that<br>um machine learning research and<br>application is still very much active<br>and you know still Cutting Edge at the<br>University of Hawaii the people on who<br>are not at the University you know for<br>wherever you are if you have research<br>needs now you know who to come to<br>um and you know you don't have to be an<br>AI researcher or a machine learning<br>researcher you just need to know your<br>data and come and have a conversation<br>with people in the Ia Institute and you<br>know this is what we're here we're here<br>yeah we're very happy to buy you for a<br>cough yeah and just talk well yeah<br>what's the problem what's the data that<br>you have and then see what could be the<br>best way to address the problem yeah<br>yeah very happy to to have these</p></main><footer style="margin-top: 2rem; background: #0001; padding: 2rem; text-align: center;"><p>We Are The University</p><ul style="list-style-type: none; padding: 0; margin: 0;"><li><a href="/">Home</a></li><li><a href="/about">About</a></li><li><a href="/contact">Contact</a></li></ul></footer></body></html>