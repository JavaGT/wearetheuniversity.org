<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><title>We Are The University</title><link rel="stylesheet" href="/styles.css"></head><body><header><h1 style="color: #fff;font-family: 'Arial Black', Gadget, sans-serif;font-style: italic;font-weight: 900;text-transform: uppercase;">We Are The University    </h1><nav><ul><li><a href="/">Home</a></li><li><a href="/about">About</a></li><li><a href="/contact">Contact</a></li><li><a href="/blog">Blog</a></li><li><a href="/videos">Videos</a></li><li><a href="/authors">Authors</a></li></ul></nav></header><main><h2 style="text-align: center;">Leveraging AI for time series forecasting in forestry &amp; flood prediction - ERI Seminar - Sep 2023 [50:31]</h2><p style="text-align: center;"><a href="https://www.youtube.com/watch?v=vKhBAGX24hs" target="_blank">Watch on Youtube</a></p><p style="text-align: center;"><a href="https://www.youtube.com/channel/UCYzot7AKCB5paA4vp8wXuvw" target="_blank">The University of Waikato</a></p><img src="https://i.ytimg.com/vi/vKhBAGX24hs/maxresdefault.jpg" alt="Thumbnail for video titled: Leveraging AI for time series forecasting in forestry &amp; flood prediction - ERI Seminar - Sep 2023" style="width: 100%;"><div class="tags"><span style="background: #0003; border-radius: 0.3em; padding: 0.3em; display: inline-block; margin: 0.2em; font-size: 0.8em">#AI</span><span style="background: #0003; border-radius: 0.3em; padding: 0.3em; display: inline-block; margin: 0.2em; font-size: 0.8em">#Time Series</span><span style="background: #0003; border-radius: 0.3em; padding: 0.3em; display: inline-block; margin: 0.2em; font-size: 0.8em">#Forestry</span><span style="background: #0003; border-radius: 0.3em; padding: 0.3em; display: inline-block; margin: 0.2em; font-size: 0.8em">#Hydrology</span><span style="background: #0003; border-radius: 0.3em; padding: 0.3em; display: inline-block; margin: 0.2em; font-size: 0.8em">#Flood Predictions</span><span style="background: #0003; border-radius: 0.3em; padding: 0.3em; display: inline-block; margin: 0.2em; font-size: 0.8em">#Forecasting</span><span style="background: #0003; border-radius: 0.3em; padding: 0.3em; display: inline-block; margin: 0.2em; font-size: 0.8em">#Neural Network</span><span style="background: #0003; border-radius: 0.3em; padding: 0.3em; display: inline-block; margin: 0.2em; font-size: 0.8em">#Data Streams</span></div><h2>Description</h2><p>Dr Guilherme Weigert Cassales and Dr Nick Lim.<br><br>In plantation forests, where returns take more than a decade to materialize, proactive planning is crucial. Current technologies can often provide real-time spatio-temporal data, which require appropriate methods to successfully extract knowledge and use these large and complex databases. In this talk Guilherme will discuss the utilization of Machine Learning (ML) tools to optimize resource allocation with forestry hydrology data and the exciting opportunities for future research, particularly related to the analysis of intricate edaphoclimatic data collected throughout the forests.<br><br>Predicting when the next flood will happen is no longer a dream. Flood forecasting has been around for decades, but now artificial intelligence can really improve prediction. In this talk we will discuss some of the neural network advancements and the methods employed to improve the flood prediction horizon and the reliability of the forecasts.<br><br>Guilherme Weigert Cassales is a post-doctoral research fellow at the University of Waikato. He received his PhD degree from Federal University of São Carlos, Brazil, in 2021. His research focuses on machine learning for Evolving Data Streams and Distributed Systems.<br><br>Nick Lim is a post-doctoral research fellow doing research in machine learning and artificial intelligence. Nick obtained his MSc (Mathematics) and his PhD (Statistical Learning) from the University of Waikato and his doctoral dissertation is on “Ensemble Learning of High Dimensional Datasets”. Prior to doing his graduate studies, Nick worked in a semiconductors MNC as a component design engineer. Nick is passionate about technology, education, social justice and the welfare of children. In his free time, Nick enjoys photography, music, paper craft, cooking, and the mountains.</p><h2>Transcript</h2><p style="opacity: 0.9; font-size: 0.8em">Transcripts may be automatically generated and may not be 100% accurate.</p><p>right<br>well Welcome to our latest ERI seminar<br>my name is Charles Lee I'm the academic<br>co-director of<br>um<br>the Environmental Research Institute and<br>they have the pleasure to welcome Dr<br>Nick Lim and Dr uretime<br>it's a long name I'm sorry I have to put<br>my phone out you hear me<br>um weiger castles<br>um they're both from the artificial<br>intelligence Institute working with<br>Professor um Albert piffin so today<br>they're going to talk about using<br>machine learning for flood protection<br>and for forestry and both of them are<br>working as part of the Thai El project<br>uh without further Ado I'd like to pass<br>I could just right good morning everyone<br>I'm as Chelsea's mentioned I'm Nick I'm<br>Nick and I'm the AI Institute and over<br>the last three years we're working uh<br>we're working on trying to use machine<br>learning to for for flight progress for<br>flood forecasting and for in uh in a for<br>a short uh for a short term<br>so one of the key motivations behind<br>behind this project was the Queen's<br>birthday flood back in 2021 and over and<br>since then the num the cost of floods<br>and the and the incidences of floods<br>have been increasing so what used to be<br>once a decade or once every 50 years and<br>even records through the business of<br>cycle Gabriel wants every 100 years it's<br>pretty much it's pretty much happening<br>at increasing frequencies and the cost<br>Associated to these events are called um<br>getting uh getting astronomical for<br>example in the case of cyclone Gabriel<br>the the uh cost insurance our call was<br>estimated somewhere on the door on the<br>low bound to be about 7.8 million<br>um<br>all right cool<br>yeah so that so not even not cons even<br>considering the the financial cost the<br>costs of floods to human lives and how<br>much you disrupt uh disrupt uh the<br>disrupt uh the lives of people it's a it<br>becomes a very it becomes a very<br>important problem it comes at very<br>um<br>um very key thing that we want uh that<br>were called that becomes of interest to<br>the regional councils and to the end and<br>so<br>from 20 2021 uh<br>incidents we we looked uh we are looking<br>at the floods uh trying to predict<br>floods in on three major rivers in the<br>uh in the coromandel region namely the<br>Taira River the karenga river and the<br>the opitonui river and with that with<br>those three rivers we have worked out<br>two ring ages that are the associated<br>with us with those uh with those uh<br>um those catchment areas so we work so<br>our initial project our initial our<br>initial push for there for for the risk<br>for the research let's try to use<br>just them this limited uh data and see<br>how far in advance we are able to to<br>predict the the the the the the flat<br>incidences<br>in terms of machine learning methods<br>there are multiple there are many<br>machine learning methods that are they<br>are actually they're available to to<br>machine learning practices for for time<br>series forecasting so some of you maybe<br>um may be familiar with linear<br>regressions which is just what are<br>called Simple models to predict What's<br>called the the future values based on uh<br>based on the the the values of the of<br>the of the rivers in the uh using a<br>linear linear regression and using a<br>linear regression some of you might call<br>familiar more advanced methods such as<br>arima so they are used as a skill model<br>that uses a little model to break down<br>the the other decompose the the<br>predictions to to multiple<br>um<br>multiple components and then we'll call<br>predict the predict those components<br>individual uh separately or so some of<br>the traditional machine learning methods<br>So acrossize Random Forest support<br>director machines or graded gradient<br>boosted ensembles<br>uh for research We Will We Will mostly<br>we're more focused on the on the on the<br>neural networks on the neural networks<br>and on on uh on neural networks method<br>and one of the key reasons why we're<br>focusing on on the neural network<br>matters is that while we found that<br>traditional methods can give us good<br>results for for very short-term<br>forecasting for up to an hour it starts<br>losing it's it starts to fall away when<br>you when try to predict something that's<br>uh this longer term for about three<br>hours or six hours<br>so there is a there is a fundamental<br>limitations to what uh to what's what<br>the what's capability they have and<br>um that's capable by by traditional<br>methods um in terms in in order to for<br>uh in in the context of forecasting the<br>the river levels of the of for for this<br>tree for these three rivers<br>another another shot coming that we also<br>found is that just depending on just a<br>while<br>using the neural network methods we are<br>able to get a better better uh focus of<br>focus Horizon of the river levels there<br>is still there is still a limit to how<br>far we can go so we're able to get to<br>push the result up to about three hours<br>um good Focus uh focusing up to three<br>hours of three or three hours forecast<br>Horizon but beyond six and nine hours we<br>start so it starts so it starts<br>um becoming less less accurate and and<br>we start missing the missing the flood<br>events<br>so for this for this reason<br>we we worked with them we met service to<br>visit uh and met Services has provided<br>us with an API so that we have the<br>direct access to that to their the rain<br>radar<br>so there is what we found here is that<br>there is a nice correlation between the<br>reflectance value on the uh our<br>reflectance value in the uh in the radar<br>and the and the and the uh the amount of<br>rentals uh those uh measured by by the<br>ring ages so if so using that we are<br>able to<br>um we are basically um the idea behind<br>that is that using the the the the the<br>the conditions in the clouds which our<br>theory is that we should be able to get<br>a better better focus Horizon of of the<br>uh of<br>um better understanding of the of the<br>brain condition over a longer period of<br>their in the in the region<br>okay<br>so doing that<br>it kind of improves the model so this a<br>graph on the left here is the six hours<br>is the six hours prediction there are<br>many lines uh values on the on the left<br>um the<br>um these are just different variants of<br>our models but this shows that the<br>um that with the improvements with the<br>improvements that uh and the in the<br>additional uh<br>um uh rain radar data we're able to get<br>uh get our up to about six hours uh<br>prediction uh a prediction Horizon one<br>of the improvements that we that we that<br>I want to highlight here is that we also<br>noticed that the that the river that the<br>uh<br>River Rising events and the flood events<br>tends to be quite rare and and most of<br>the and most of the periods the the<br>region doesn't receive rain and because<br>the rivers pretty much usually in the uh<br>usually either either stable or is on or<br>in Decline so one of the improvements<br>one of the modifications that we did<br>during the for the training of the<br>network is to emphasize uh emphasize the<br>predictions during this during the<br>events where there's uh that's actually<br>real activities<br>so we use so the alpha value on the left<br>here is basically the amount of emphasis<br>we applied to the edge were called uh to<br>the training during the um during the<br>river activities and this hour and from<br>our from our from our results the same<br>we showed that uh we're able to see that<br>by emphasizing mainly on those on those<br>events with uh with River activities<br>we're able to get a better uh we are<br>able to get improvements in our in in<br>one key shot coming about uh with the<br>traditional neural networks is that<br>neural networks tend to give us just a<br>point uh a point forecast<br>so rather rather than giving us a rather<br>than giving us a distribution day of the<br>of the predictions it just gives us a<br>single singular value and we are not uh<br>we are there's no measure of uncertainty<br>there are no measures to answer these<br>today to do those predictions<br>fortunately there are there are<br>approaches in in deep learning such as<br>foreign<br>memory uh networks they're basically<br>rather than giving us a point forecast<br>they we're trying to predict a<br>distribution of the uh the distribution<br>of the of the of of the forecast which<br>because with some predictive<br>uncertainties in there but I embedded in<br>it and and the<br>and the predictions and the predictions<br>as well the graph on the right there is<br>the uh are the uh some results for the<br>of the forecast the yellow the orange<br>lines are the are the mean values of the<br>of the of the predictions the red and<br>green dotted lines are the 95 and 5 and<br>95 intervals are respectively and the<br>blue line since the ground Truth uh the<br>ground truth that's been shifted by six<br>hours so this is a six hours forecast uh<br>six hours forecast of the of of of the<br>of the river levels<br>okay<br>so going closer we can see some<br>distributions of the of of the of the<br>predictive intervals and the<br>and the uh and how reliable it is so the<br>graph on the left<br>are the is the histogram of the of the<br>predictive errors<br>and for for two different cases so one<br>is when women consider when the river is<br>high and second when the uh sorry when<br>the river is uh across the whole<br>predictive range and at the bottom and<br>the bottom lines the bottom graphs are<br>when we consider reverse at the top uh<br>the top 85 percent uh when the numerous<br>River activities when they see when the<br>reverse about their at the top 85 range<br>of of where they're where they where<br>they are and in both cases the<br>cumulative histogram and the and the the<br>QQ plots are called pretty much shows<br>that the errors are roughly are roughly<br>normal<br>there is a student open question we're<br>still trying to work here is that uh how<br>do we how do we actually quantify what<br>is a good uh what is a good good<br>prediction what should actually be the<br>goal of the of the of the of the network<br>should it should it produce a reliable<br>Point uh Point estimate Point estimate<br>or should the predictive interval or is<br>it more important than the predictive<br>interval would be be reliable for<br>example if the 95 if it's possible to<br>get very good predictive intervals by by<br>with very large variants in the in a<br>very large variants and which in which<br>case it will it will cover the 95 95<br>interval anyway or we can have something<br>that's more accurate in the uh more<br>accurate in his point of Point estimate<br>Point estimate button with a narrower<br>narrower variance in the prediction<br>which in which case 95 the percent<br>interval might not be as reliable so<br>that's it so there's an open question<br>that uh that we are that Stillness<br>answering By Us and something that we're<br>called that becomes a bit of a<br>engineering problem the engine problem<br>foreign<br>so some of you might have uh might know<br>about chat GPT uh you might know about<br>chat GPT and over the last six months uh<br>well eight months but called chat GPT<br>has kind of blown up in a blown up in<br>um<br>has kind of blown up so chat GPT is<br>based on a neural network called<br>Transformers and light language light<br>languages are time series prediction is<br>a uh is is a sequential type of<br>sequential type of thoughts<br>so it was<br>um circulated that Transformers such as<br>chatgpt such activities based on can<br>perform really well for four times four<br>times series tasks as well<br>but<br>there are some shortcomings of the of<br>just blindly applying Transformers to<br>the to the tasks so Transformers tends<br>to be very data hungry if you do not<br>have enough that uh enough data called<br>Transformers do not actually scale well<br>the scale well to it we need to have a<br>lot of data before Transformers uh uh<br>perform our call adequately so this is<br>some of the results that we that we<br>tried using just the base using a base<br>Transformer and to to predict the river<br>levels and from what we can see here see<br>is that it does not perform as well as<br>lstms which I will call which was the<br>Baseline model that we that used to<br>compare to compare against<br>fortunately there are bespoke<br>Transformers that are designed for<br>full-time series prediction so this uh<br>Transformer is called EDS former you can<br>think you can think of ATS former as a<br>um as a Transformer test that got that<br>has inspiration from you from arima so<br>rather than uh so rather than looking um<br>So within the Transformer there is what<br>we call a transaction Network so they<br>basically rather than using what we call<br>a basic attention Network they basically<br>were called use the inspiration for<br>arima let's say you use a long-term<br>long-term attention short-term attention<br>and some exponential decaying in in the<br>attention in order to uh Endeavor called<br>apply that apply apply that to<br>Transformers and with that with that uh<br>called Improvement City because some of<br>these are Transformers are competitive<br>to to the Baseline methods or even<br>better than what that then then what we<br>um there are still a lot of work to do<br>you say so they deeper the network the<br>deeper network is the more complex<br>network is the usually the harder the<br>harder it is to interpret and to to<br>under to<br>to uh to understand so there is is there<br>is a need to it there's there is a need<br>to come up with methods to for<br>um for explainabilities in in<br>Transformers especially in the areas of<br>Time series basically why is basically<br>namely why do they come up with the<br>prediction and what and what are the<br>factors that that really that causes the<br>trans the the neural network method to<br>to comment that prediction there is just<br>the need to cut to like similar to<br>division lstm to come up to to produce<br>some predictive uncertainties in the<br>results and to produce a distribution of<br>the of the predictions<br>okay<br>so this is just some uh some uh some uh<br>some recent results that we got so this<br>was some<br>um this was us trying out our models<br>based on the on on the river levels and<br>the and the flat ending and the uh<br>Ringgit symbol call that uh during<br>Auckland day and the and the and the<br>cycling and the second Gabrielle flat so<br>although the model is not trained for<br>this for these events and and some of<br>the river levels that they have seen<br>during Second Gabriel is unprecedented<br>unprecedented pretty much none of our<br>data points have exceeded or called 3.5<br>Fleet 3.5 meters in the uh throughout<br>the the training but the our model was<br>able to the the new network models were<br>able to to predict what's called um go<br>River level staff yeah well there's<br>extreme such as this<br>right<br>I'm not sure if this is relevancy so<br>this is just a background a inner<br>workings of what's uh what's happening<br>and what's happening with the with the<br>flat prediction system we obtained the<br>data the data from the river levels and<br>the uh the river levels and the radar<br>from from both met services and in the<br>future the original councils and then we<br>process them and then we visualize them<br>so this is a visualization tool that we<br>are developing for uh for the original<br>councils<br>and I<br>oh no okay that is the video all right<br>yeah sorry the<br>video the video couldn't play I think<br>that's fine all right so that's that<br>from my part I'll shall pass on to my<br>colleague seriously it says sorry for<br>and<br>hey<br>everyone<br>my name is guillerme I am often asked if<br>I'm from Pakistan India or Iran but it<br>turns out I'm from Brazil<br>yeah my name is also a huge controversy<br>case even Brazilians from some regions<br>have difficulties pronouncing it the<br>right way uh basically because of the<br>the H it gives some trouble in the<br>pronunciation but no worries that's fine<br>um so yeah I'm here to present a little<br>bit about the data science that we are<br>using for Plantation forests uh<br>specifically in the forest flows use<br>case<br>which is done in partnership with Scion<br>and yeah the forest flows project which<br>is also allocated in Zion<br>so a brief contextualization is that we<br>in New Zealand we have<br>planted forests and they are very<br>important for our economy so we have<br>like almost 40 000 jobs that people<br>working in this<br>this area uh a lot kind of a lot one<br>percent of our GDP more than one percent<br>comes from Plantation forests so that's<br>an important business and uh in addition<br>it kind of regulates the climate and it<br>has some impacts on the environment on<br>the nearby areas and basically that's<br>the goal of the project is understanding<br>the uh water usage in Plantation forests<br>uh so this is very important and how do<br>we make sure that we are getting as much<br>as we can that we are optimizing our<br>profits and minimizing our expenses and<br>all that so basically uh there's a huge<br>infrastructure with many sensors for<br>example uh we can monitor tree growth<br>with the andrometer sensors we can<br>monitor water usage with set flow met<br>meter which is basically how much water<br>the tree is consuming we can also<br>monitor the soil moisture or how much<br>water there is in the soil or yeah so<br>things like that also some some other<br>stuff in the soil like the nitrate<br>sensor and and so on so forth so there's<br>a lot of sensors around these forests<br>and<br>they generate a lot of data<br>so this is a picture that kind of<br>depicts uh how big the infrastructure is<br>you can see that there is kind of<br>internal data from this project that's<br>been generated there is also data from<br>newer so it's using data from different<br>sources and trying to combine this in<br>order to extract this this knowledge<br>uh and it turns out there's more than<br>200 sensors very forest and there is<br>like six or seven forests in this<br>project so there's a lot of sensors<br>they have a five minute frequency so<br>every five minutes it's uh reading<br>getting readings from the the things<br>it's monitoring<br>and then this data goes through a very<br>large thing with stations and then going<br>to servers going through the data<br>provision system and the live database<br>and the question then is why do we<br>monitor so many things why do we expand<br>so much so many resources in gathering<br>this data only<br>up to this point we are just Gathering<br>and we're not analyzing and the goal is<br>to create these Forest hydrology model<br>uh and then there are physical models<br>that can simulate these water fluxes but<br>they are quite kind of they uh they take<br>a long time to process and maybe they<br>are one idea is to maybe machine<br>learning models will give<br>approximate solution in a much shorter<br>time once the models are trained so<br>there is a clear objective<br>um<br>and then it's kind of looking at the<br>same problem from a different angle and<br>that gives you other opportunities that<br>I'm going to talk later<br>for example<br>to a kind of secondary objectives is<br>also to<br>promote positive impacts and mitigate<br>negative impacts of plantated plant dead<br>forests and here is the irony of having<br>this talk right after Nick talked about<br>the flood so in Flats you are trying to<br>predict or you are trying to forecast to<br>win there will be too much water and we<br>don't have the capacity to deal with it<br>and in Forest floss project it's the<br>opposite we are trying to understand why<br>water is disappearing basically because<br>this Plantation forests they have a huge<br>intake of water to to make them grow I'm<br>sure you understand it a lot better than<br>me<br>um and then the the second objective is<br>to the is this kind of policy incentive<br>so things that Aid the government and<br>the agencies to kind of promote the<br>adoption of better water water practice<br>or water Forestry regimes<br>uh yeah so everything like I'm going to<br>do something that's forbidden I'm going<br>to go back<br>so in this you have data from different<br>sources you have different sensors you<br>have different agencies so everything<br>goes to one place<br>usually you would create a big data lake<br>or a data warehouse where everything<br>goes inside there and we try to get some<br>knowledge when everything is together<br>um<br>that was quite useful and it was the<br>kind of state of art for a long time<br>unfortunately that creates a big<br>bottleneck because you have one data<br>science team and they are responsible<br>for treating every data for consuming<br>every data for generating all the<br>reports and so on<br>and then nowadays there's a data mesh<br>it's a new data new uh four years old<br>data architecture<br>and it's a distributed data texture so<br>data is now instead of going everything<br>to a lake it's kind of you have small<br>ponds over your uh your whole database<br>and each Pawn is responsible for<br>producing kind of higher quality and<br>summarized data so instead of one data<br>one data science team you would have<br>several that are distributed for each<br>thing and that allows them to be more<br>like experts or have more expertise in<br>that domain uh in this example it's like<br>you have the users you have the artists<br>and you have podcasts so things are<br>separated by team and people that know<br>so whoever is dealing with artists know<br>every artist and they can do a better<br>job in filtering and creating this data<br>and<br>uh giving reports regarding the artists<br>and the same happens for the users and<br>for the podcasts<br>so basically the data mesh is built upon<br>these four principles<br>one is it domain oriented so you have<br>this pawns instead of a big lake that<br>kind of makes it more agile because<br>there's less data to understand there's<br>less data to process and you can<br>generate higher quality data in inside<br>your domain<br>uh in the data mesh we treat data as a<br>product so data is not this like<br>items that have value but the value is<br>hidden now data is a product and it has<br>value by itself because it's already<br>been processed it's already been<br>extracted extracted knowledge and it's<br>made available for every other service<br>in a way that they can readily use it<br>they don't have to clean it they don't<br>have to pre-process it's just get the<br>data use for whatever you want<br>also they have this self-serve platform<br>which is basically the infrastructure<br>that makes everything work together<br>so yeah in this case the user's profile<br>can be accessed in the podcasts uh like<br>it's providing data regarding the users<br>for the podcasts uh and then you have<br>the Federated governance which is kind<br>of this governance uh the this this<br>group of people this committee that<br>decides on the policies like do we need<br>to prove to to to apply this privacy<br>measure or do we need to apply this uh<br>security uh process before making the<br>data available so basically the<br>Federated governance uh kind of makes<br>the rules of how data is going around<br>and how each domain interacts with<br>another<br>so this is the data mesh very briefly uh<br>why is this interesting because we can<br>have different organizations here so we<br>can have the kind of the newer Pond<br>which is a very big pond but still in<br>this case it would be a pond and then<br>you have Scion Pond and then you can<br>have another organization like The<br>Institute Pond and then we are<br>exchanging uh data in a way that's<br>efficient in a way that's secure and<br>yeah that brings a lot of benefits<br>um<br>so this is the backbone of everything<br>it's a data mesh<br>what Nick showed the diagram it's<br>basically how the data mesh is organized<br>yeah so this is to understand a little<br>bit the the how things are working below<br>the the hood uh and then going on uh the<br>dendrometer sensor I suppose most of you<br>know or maybe I'm supposing wrong but<br>anyway I'll try to be brief so the<br>andrometer is this this is the real<br>picture this is the diagram and this is<br>the reading of the one andrometer<br>specifically and basically this needle<br>is kind of measuring the tension that<br>the tree trunk uh is making uh basically<br>when it expands or when it contracts the<br>needle moves and then this movement is<br>measured and we can kind of estimate how<br>much the tree is growing at least<br>um and then basically this is translated<br>to this measurement which is in<br>millimeters and this is over the month<br>of September October November up to uh<br>make<br>you can see that it's it has a trend but<br>there are some<br>weird things if you consider that<br>something is growing because sometimes<br>it's shrinking like this part<br>um<br>and then okay let's see closer what's<br>happening so this is the dendron the<br>same dendrometer but uh in the beginning<br>of September only this is a week<br>so it has this cycle of growing and then<br>shrinking and growing and shrinking and<br>growing and shrinking and you can see<br>that the overall trend is growing but<br>you have this cyclic Behavior<br>and then even more specifically if we<br>get only this day<br>we can see that this is the the behavior<br>it's it's showing<br>also one challenge in this is that this<br>is the dendrometer one day<br>in early September so this is the<br>behavior it shrinks and then grows and<br>so on and this is the same dendrometer<br>the values are different but the the<br>amount of units in both charts are the<br>same so while this reached only this<br>height this was higher and the kind of<br>behavior this is kind of a logarithm<br>thing and this is more like linear<br>so it can change quite a lot along the<br>year<br>and then that's what we want to forecast<br>first because if we can forecast the<br>growth we can start to understand what<br>are the driving factors<br>uh and then we are using this Advanced<br>machine learning uh that Nick just<br>covered and I'm going to skip so you<br>have the lstms<br>the Transformers and the exponential the<br>smoothing Transformers short for ETS<br>farmer<br>um<br>yeah so this is one example of the<br>preliminary experiments we got and here<br>we are using okay so that might be a<br>missing context here so every Forest is<br>divided into plots and every plot every<br>plot has around far dendrometer sensors<br>spread over the plot<br>and so for example this I cannot say<br>which Forest I suppose because of the<br>privacy issues but this Forest has more<br>than 20 plots and we use half of them to<br>train so we are using around<br>40 53s to train our model that gives us<br>three million samples and then we use<br>test set each sensory individually so we<br>train with half and then we test on One<br>sensor specific<br>and this is one of the sensors being<br>forecast uh it looks good in this scale<br>but if we zoom in<br>then it's it's not perfect<br>easy to see there's a lot of room for<br>improvement<br>but it kind of captures the general<br>behavior of the sensor and then most of<br>the time it's and they're predicting<br>under forecasting like the the values<br>are smaller than Rio<br>um but yeah this is a work in progress<br>and<br>then oh<br>and this is also using only the<br>dendrometer sensor so we do not have uh<br>weather information we do not have other<br>types like the the other sensors are not<br>being used in this prediction it's just<br>um why why this information is important<br>because some plots have very different<br>um<br>weather conditions<br>so talking to the engineers at Scion<br>like the higher plots they have a lot<br>more soil moisture than these mother<br>plots or the other way around but they<br>are different uh very different and that<br>may create this difficult when you are<br>trying to predict<br>um<br>so after this preliminary experiments<br>what are the issues one is that we are<br>using a univariate Time series so it's a<br>known fact that growth is non-linear and<br>it changes according to the seasons<br>according to the specific environment<br>environmental data conditions<br>and then we could improve this<br>prediction<br>by including more data regarding like<br>air temperature air pressure and so on<br>you immediate area humidity and<br>sciomizer and using all this data that<br>we have we can improve the<br>the the predictions however uh using<br>five minute frequency uh I suppose it's<br>kind of expected that a tree does not<br>change a lot during a day the variations<br>are quite small and the five minute<br>frequency kind of creates a big overload<br>in a sense because you are processing<br>very a lot of stuff that are very<br>similar<br>but the uh the outcome is not that big<br>that that good<br>so yeah this is just some random numbers<br>this is not very long by any means but<br>we have to take into account that we are<br>including more data and then this is<br>going to multiply<br>so one alternative is one question that<br>Ray we raised from these experiments is<br>do we need five minute frequency<br>so we made a temporal resolution study<br>and basically we are dropping data to<br>make the interval 30 minutes 60 Minutes<br>three hours six hours and<br>uh 12 hours no yeah 12 hours<br>um<br>and here the results you can see that<br>these measures are quite similar the<br>others not so much<br>and then this is the mean overall tested<br>sensors so there's if you are from<br>statistics mathematics there are more<br>than 3 30 sensors so this is<br>statistically okay to get<br>a standard deviation in mean<br>um and then we go down from 400 seconds<br>per episode to 14 seconds into two<br>seconds per Epoch so these two models<br>are very very a lot faster than the five<br>minute uh resolution and the ahor is not<br>increasing by a lot<br>uh<br>so the implications of the five minute<br>uh like economical implications one is<br>the battery maintenance because usually<br>transmitting data over Wi-Fi is very<br>costly so if we can reduce this battery<br>usage the batteries can last longer and<br>there's less costs of sending someone<br>inside the forest to change the<br>batteries and the other is the storage<br>in processing so if you ever try to use<br>the cloud you can see that costs bio web<br>very very fast uh<br>suddenly you have like<br>gigabytes of data and costs start<br>increasing<br>so that's an important thing to consider<br>as well<br>and we are finishing a paper for<br>ecological informatics with these<br>results<br>uh and then we can improve this and I<br>think I'm going to slow maybe but<br>basically we can improve this by using<br>summaries instead of just dropping the<br>data and this is a also something we are<br>working on<br>so for the roadmap road map sorry the<br>next steps is to we are going to<br>increase to a seasonal forecasting so we<br>are going to forecast three months uh in<br>advance of course this is going to<br>increase the horse by a lot<br>um but that's expected uh kind of and<br>the implication of this is that we can<br>have this ability to<br>have high valuable decision makers<br>because they are going to see a snapshot<br>of how the forest is going to be in<br>three months<br>um<br>and then we can also the the next steps<br>is to include Ada for climatic data uh<br>so climatic variables like temperature<br>precipitation and also idaphic variables<br>like soil water content texture<br>topography and so on uh all of these is<br>to finally include or explainable AI<br>methods and then finding out what drives<br>the growth and the resource usage<br>basically uh where's the water going and<br>what makes the forest grow faster what<br>makes the forest like what can create<br>some kind of disease or some problems in<br>the in the trees<br>and yeah this is the platform we are<br>working on uh it's a work in progress as<br>why is there a video playing and mine is<br>not<br>it's a 30 second video we are already<br>spending more time trying to make it<br>work<br>okay<br>so basically what's showing here is that<br>we can choose the temporal resolution we<br>can choose the Deep learning model we<br>are going to use and yeah there's some<br>parameters encoded in the name and then<br>we can choose the sensor we want to<br>forecast and the forecast is going to<br>show here this is this ped up to make it<br>sound update but in theory this would<br>update every five minutes when new data<br>arrives<br>yeah forecast is not that good there's a<br>small error here but<br>yeah I think that's it from my part and<br>um are there any any questions<br>um let me just<br>depending on the online questions I know<br>that these are<br>fairly detailed descriptions of how you<br>guys constructed<br>um your models and the implications of<br>using certain models<br>um are there any questions from the<br>audience<br>hey uh Nick I have a question for you uh<br>I think it's a couple of months ago one<br>of the math students yeah he used a new<br>model called operator neural networks uh<br>uh Json Chris<br>yeah yeah if I heard of it okay it was<br>used by Jason curse yeah and like so<br>these are used in physics and form<br>neural networks did you try to use them<br>because if you're using rain gauge data<br>it's more or less a good approach do you<br>think yeah I've heard of operating uh<br>operator Network<br>um in fact I think that was actually<br>it's one of the research that's done by<br>one of the recent sets professors are<br>recent sets lecturer that just joined um<br>they just joined the university but no<br>I've not used operate I'm not used<br>operator Network although I did try but<br>called try basically a difficult<br>encoding to the uh to the derivative<br>levels<br>um I did have partial success by by<br>transforming the the the river level<br>City abusing for transform we're doing<br>four transforms and then predicting<br>those coefficients of those those<br>Fourier transforms<br>um wheels for transforms<br>um there is<br>a slight mix mixed results in the uh NC<br>so while I was able to to predict the<br>real components therefore Transformers<br>are relatively well the the complex<br>components were not uh were not uh were<br>not were not properly<br>um it was pretty much what called the<br>prediction of the complex the complex<br>parts of the transfers when that's good<br>so we did share some work of face some<br>face delays and and uh and shifts in<br>there and that<br>yep<br>yep I know I know so that that was a<br>presentation by Jason kirst by our call<br>um I think three months ago ZD presented<br>yeah yeah so yeah that in fact the uh<br>just decided to seeing the uh the the<br>any questions from audience online I<br>haven't seen any but I'll ask I have a<br>couple questions<br>um so so Nick when you so so there's a<br>there's a newer Network component to<br>your work on flooding and I'm just<br>wondering have you done some sort of uh<br>any kind of uh interpretation work<br>that's trying to isolate maybe features<br>in the rated data that is actually most<br>predictive of flood as opposed to having<br>to deal with the entire data set because<br>that because because the current<br>time frame for projection is already not<br>a whole lot in terms of what civil<br>defense can work with right and so do<br>you think that by doing some<br>interpretation of the of the model you<br>could maybe<br>um isolate the the signal better and<br>potentially provide a more long-term<br>projection yeah actually that's one<br>that's one of the reasons why I want to<br>look we are trying to look into the<br>implications of the the models namely<br>with our<br>um namely what's the what they've<br>thought about is that if we were to look<br>at the brain uh if you didn't know that<br>that certain rain clouds in over a<br>certain area or call is more important<br>in in certain regions it might work on<br>my show we're called certain catchment<br>areas and then this will protected on<br>incident needs to be<br>um then this is a bit more than this<br>they should not be developed or require<br>small more mitigations currently we saw<br>that most of the uh most the impact of<br>the of the predictions comes directly<br>from the river levels itself and not so<br>much on the on the description of the of<br>the clouds but because it's something<br>that's that is still ongoing uh one of<br>the issues here is that the that the uh<br>that the clouds fly the speed of the<br>clouds is something like something also<br>over 60 kilometers per hour and for six<br>hours forecast 60 kilometers is uh six<br>hours or 60 kilometers is 300 kilometers<br>a day which is much like which is almost<br>larger than than the there's larger than<br>that our our<br>recoverished area so the physics of the<br>physics of the flow of the water flow<br>models is a bit beyond me so it's<br>something that we need to work out which<br>um how much of the how much the fluid is<br>caused by trans uh by by the uh by run<br>by some office runoffs but how much of<br>fluids is caused by the direct well what<br>is the reinforced directly on the river<br>surface itself and that's very much very<br>dependent on every on the on the river<br>and the catchment areas<br>understood all right uh and jeremony<br>yeah Jeremy<br>um my question for you is um you know<br>that I was kind of fascinated by the<br>five-minute interval thing<br>so was that was there any operational<br>justification for collecting data so<br>frequently<br>uh basically they didn't I think it's<br>like a better safe than sorry so you<br>want to have data as frequent as<br>possible and then you can discard it<br>yeah<br>um<br>because for the like science part they<br>wanted I think it was already a goal for<br>the project I'm not in the project since<br>the beginning but I think it was a goal<br>to understand that they they thought<br>that<br>the highest they could go is five<br>minutes like the most frequent they<br>could go with five minutes and yeah uh<br>that could be the reason for getting<br>let's just use the maximum quality we<br>can and then we can go go down as we<br>yeah so there's many ways the the data<br>modeling work you're doing here is kind<br>of telling them helping to shape their<br>operational parameters as well right<br>because it seems like not a lot of not a<br>lot of<br>um sort of a priority design went into<br>that it was just this was the lowest<br>setting so let's go with that yeah it it<br>doesn't like a budget decision this is<br>the highest uh frequency or budget can<br>can cover so let's go with it I suppose<br>okay yeah cool<br>um now the other any other questions<br>from our audience online if not then<br>we're basically<br>um out of time so I like to think please<br>uh<br>I have two little<br>um<br>Memento for you<br>thank you all right thank you um and to<br>those online uh for attending the ERI<br>seminar well we'll look forward to</p></main><footer style="margin-top: 2rem; background: #0001; padding: 2rem; text-align: center;"><p>We Are The University</p><ul style="list-style-type: none; padding: 0; margin: 0;"><li><a href="/">Home</a></li><li><a href="/about">About</a></li><li><a href="/contact">Contact</a></li></ul></footer></body></html>