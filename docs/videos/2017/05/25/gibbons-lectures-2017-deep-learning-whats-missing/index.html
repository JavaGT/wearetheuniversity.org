<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><title>We Are The University</title><link rel="stylesheet" href="/styles.css"></head><body><header><h1 style="color: #fff;font-family: 'Arial Black', Gadget, sans-serif;font-style: italic;font-weight: 900;text-transform: uppercase;">We Are The University    </h1><nav><ul><li><a href="/">Home</a></li><li><a href="/about">About</a></li><li><a href="/contact">Contact</a></li><li><a href="/blog">Blog</a></li><li><a href="/videos">Videos</a></li><li><a href="/authors">Authors</a></li></ul></nav></header><main><h2 style="text-align: center;">Gibbons Lectures 2017: Deep learning - what's missing? [1:09:21]</h2><p style="text-align: center;"><a href="https://www.youtube.com/watch?v=IIz5fejP33Y" target="_blank">Watch on Youtube</a></p><p style="text-align: center;"><a href="https://www.youtube.com/channel/UCUKg41qkUTUQXGDzklgpmlQ" target="_blank">University of Auckland | Waipapa Taumata Rau</a></p><img src="https://i.ytimg.com/vi_webp/IIz5fejP33Y/maxresdefault.webp" alt="Thumbnail for video titled: Gibbons Lectures 2017: Deep learning - what's missing?" style="width: 100%;"><div class="tags"><span style="background: #0003; border-radius: 0.3em; padding: 0.3em; display: inline-block; margin: 0.2em; font-size: 0.8em">#The University of Auckland</span><span style="background: #0003; border-radius: 0.3em; padding: 0.3em; display: inline-block; margin: 0.2em; font-size: 0.8em">#University of Auckland</span><span style="background: #0003; border-radius: 0.3em; padding: 0.3em; display: inline-block; margin: 0.2em; font-size: 0.8em">#UOA</span><span style="background: #0003; border-radius: 0.3em; padding: 0.3em; display: inline-block; margin: 0.2em; font-size: 0.8em">#Auckland University</span><span style="background: #0003; border-radius: 0.3em; padding: 0.3em; display: inline-block; margin: 0.2em; font-size: 0.8em">#Auckland</span><span style="background: #0003; border-radius: 0.3em; padding: 0.3em; display: inline-block; margin: 0.2em; font-size: 0.8em">#New Zealand</span><span style="background: #0003; border-radius: 0.3em; padding: 0.3em; display: inline-block; margin: 0.2em; font-size: 0.8em">#University</span><span style="background: #0003; border-radius: 0.3em; padding: 0.3em; display: inline-block; margin: 0.2em; font-size: 0.8em">#Gibbons</span><span style="background: #0003; border-radius: 0.3em; padding: 0.3em; display: inline-block; margin: 0.2em; font-size: 0.8em">#Lectures</span><span style="background: #0003; border-radius: 0.3em; padding: 0.3em; display: inline-block; margin: 0.2em; font-size: 0.8em">#2017</span><span style="background: #0003; border-radius: 0.3em; padding: 0.3em; display: inline-block; margin: 0.2em; font-size: 0.8em">#Marcus Frean</span></div><h2>Description</h2><p>Associate Professor Marcus Frean, School of Engineering and Computer Science, Victoria University of Wellington<br><br>There have certainly been some spectacular improvements in machine learning over the last couple of years, and one has to wonder, what comes next? The speaker will talk about recent breakthroughs but also focus on their intrinsic limitations in order to make some guesses about where the frontiers might lie. For example, the current paradigm of supervised learning is an important advance – but would unsupervised learning be more interesting if we could make it work? Neural nets have become much better at modelling some aspects of complex temporal data such as human language – but what about the aspects they’re ill-disposed to learn? Traditional neural networks learn fixed mappings from inputs to outputs – what if they could learn to implement the actual algorithms themselves?</p><h2>Transcript</h2><p style="opacity: 0.9; font-size: 0.8em">Transcripts may be automatically generated and may not be 100% accurate.</p><p>my teachers welcome you all to the third<br>talk and the 2017 Gibbons lecture series<br>and for those that don't know me I'm<br>Jill Dobby here in the department of<br>computer science at the University of<br>Auckland our speaker today is Mark<br>experience Marcos is from Victoria<br>University in Wellington<br>I wondered you know how long he'd been<br>working in the neural Nets area so of<br>course I googled and his most cited<br>paper is a paper about methods for<br>constructing and training feed-forward<br>neural networks and it was published in<br>1990 so you know while people think this<br>is all new stuff Marcus has been at the<br>forefront so his research is driven by<br>the challenge of making computers learn<br>by themselves and continuing mysteries<br>of how brains achieve the same thing so<br>hopefully you're going to touch on some<br>of this this evening hopefully we'll see<br>hi thanks for having me it's great to be<br>here what a what a pleasure so what's<br>missing is is a large country so it's a<br>big place and I realized that I've<br>thrown myself a hospital pass when I was<br>asked by Bob what title I should give<br>back in December or whenever you first<br>approached me and I say our deep<br>learning is this say what's missing<br>okay might that was my first mistake<br>because are we take hand if I knew what<br>what was missing I wouldn't be missing<br>anymore so don't hold the whole girdle<br>thing straight off the bat but I'm going<br>to pick up on a couple of specific<br>things that I feel are missing I'm going<br>to partially fill in one of them and the<br>others you know question marks to plant<br>in your head and I'll mention towards<br>the end some some other aspects that I<br>think it's just a I know holes they're<br>really mostly holes in understanding in<br>deep learning so what is this deep<br>learning let's get rolling today<br>something's happening in machine<br>learning if you can read that pipe watch<br>big data saturating on the hype curve<br>and the red guy coming up there is<br>machine learning and I guess the<br>question is is this the beginning of a<br>have like this curve going to go what's<br>going to happen not just in height but<br>in terms of application success start is<br>it going to plateau or we just in the<br>early days and it's hard to know because<br>there's a lot going on so this curve is<br>there's a lot of things behind it<br>there's some new insights and wonderful<br>ideas and then at the same time there's<br>a this applications those ideas that are<br>very worthwhile doing and therefore big<br>corporates jumping in into the space<br>that used to be occupied by universities<br>and huge resources pouring in and then<br>more ideas flowing from that so there's<br>somewhere in that exponential phase<br>there and who knows where we're going<br>but I think part of the excitement is<br>driven by let's get rid of hand by<br>recently I'm talking the last just a<br>couple of years machines achieving near<br>human performance on certain selected<br>tasks right and those tasks being spread<br>across the spectrum from image<br>recognition to speech recognition text<br>translation gameplay dot dot dot you'll<br>have your own things you've read about<br>in the press and so these are exciting<br>and they're also slightly slightly<br>frightening I love this sentence from<br>the listener in April I read that it's<br>about to get worse the job sucking<br>better mock of artificial intelligence<br>stalks the horizon as I think I'm<br>beautiful we know<br>this one stalking that horizon this the<br>silhouette of the job sucking artificial<br>intelligence and so that that's that's<br>scary and there are aspects that we<br>should be thinking about I think some<br>things are a little scary there's also<br>of course tons of promise there's enough<br>promise that lots of academics want to<br>jump in on the game<br>in particular this phenomenon deep<br>learning which is the subject of my talk<br>is the power of machine learning is<br>actually driving the red curve up the<br>hype curve all these achievements that<br>are close to human level performance are<br>deep learners and everyone saying oh<br>yeah of course I've been doing deep<br>learning forever I don't know if you<br>have colleagues like that but okay so<br>let's check it into this what's on your<br>own there so deep learners are what it<br>tok-san probably if you're in a talk<br>like this you probably know a lot of<br>neural letters letters but here's a<br>neural net it's a what we have is a<br>bunch of input features or values so<br>that forms a vector term all on a vector<br>of input values and some target output<br>in this case there are two outputs<br>and in between we're going to have a<br>bunch of neurons what's a neuron that's<br>a little unit that takes a weighted sum<br>of its predecessors in the previous<br>layer computes the weighted sum and then<br>pushes that through a very simple<br>non-linearity so if you like ah let's<br>make this thing go let's give it a<br>problem hard problem<br>make it go okay so what you see on the<br>right there is that that little diagram<br>there if you can see it that's the loss<br>that's the error of the network and<br>attempt to try to get to grips with this<br>with this learning problem I've given it<br>which is actually pretty tough one to<br>learn to classify the DOE points on the<br>file alright so what's happening here if<br>there's a learning algorithm which is<br>changing their connections between all<br>these neurons okay that's all that's<br>happening the function the non-linearity<br>is staying the same the inputs coming<br>from some training set of examples and<br>outputs the an output pattern that goes<br>with each input pen and it's simply<br>learning a mapping from the input to the<br>output okay with some success it's still<br>going okay fine so that's what a neural<br>network is is I should say there's a<br>whole another part of this red curve<br>rising as a bunch of new fantastic tech<br>being built including tensorflow that<br>demonstration there but here's my more<br>reduced picture this is my shorthand for<br>that last picture of what a neural<br>network looks like that I'll use for<br>other bits of the talk so we have an<br>input that's a vector that blue line and<br>an output another blue line vector I<br>hope you can see that it's a little bit<br>apparently we can't sort of dim the<br>lights<br>- halfway it doesn't look like that but<br>anyway so input through the output and<br>it's going through each of these<br>parallel ignore what is that trapezoid<br>something four-sided thing is<br>representing a weights matrix so all the<br>weights from one layer to the next and<br>then the Sigma and a circle is<br>representing that non-linearity so hit<br>the weights matrix happening and then<br>element wise non-linearity happening and<br>that's all the neural network is okay<br>and it's deep if we make your screen a<br>bunch of these together it's deep if you<br>make more than three layers you're<br>allowed to say deep I think and the old<br>days you can see that and now we can<br>train things that are hundreds of layers<br>deep ok so how do we trained us we look<br>at the error so each input pattern has a<br>desired output you stick in the input<br>something else comes out not the desired<br>output there so there's an error we can<br>compute the gradient of the difference<br>between them a loss function compute the<br>gradient of that and push that back<br>through the network and that's basically<br>just saying how would I like to change<br>some parameter inside this matrix some<br>weight value how would I like to change<br>it such that the performance on the<br>training set and fewer errors Arabic<br>that propagation is often called it's an<br>old algorithm from the 80s okay so<br>that's that's a neural net game over<br>really okay so here's a here's an<br>example of a neural net a reason we deep<br>one it's take the input this blue vector<br>here is an image that could be just a<br>huge huge vector right it's pretty big<br>hundreds and hundreds of elements but an<br>image is a vector it's going through<br>this neural net don't worry about the<br>details here and out it's coming out the<br>far side of the output in this case<br>labels words like car truck airplanes<br>ship horse and in this case you know<br>thankfully it's figured out that this<br>thing is more likely to be a car than a<br>horse so full marks and it did that by<br>training on probably millions of<br>pictures of cars<br>okay so all the weights in this big<br>neural net have been trained by this<br>algorithm okay that's that's deep<br>learning awesome that wasn't too hard<br>was it I should tell you about a<br>particular kind of neural network which<br>is in there is an autoencoder which is<br>being used in very interesting ways<br>these days so in order to encode it what<br>to do is you take the input then you<br>push it through the neural net and you<br>train it to reproduce the input at the<br>output so the target is just the input<br>it's called an autoencoder why on earth<br>would we want to do this right the<br>identity map does this it's not that<br>hard to get the input from the input but<br>if we push it through a neural net and<br>the there's a layer and here this is<br>very narrow little blue dot in the<br>middle there so I'm being very slightest<br>rightist here so if we go through that<br>that bottleneck layer then what's the<br>neural network trying to do it has to<br>push this picture of a car through the<br>network and recover car picture at the<br>far side but along the way it's got to<br>go through this really skinny layer in<br>the middle I imagine that was only two<br>units just to be really nasty<br>that means it's got to push it down to a<br>two dimensional vector which represents<br>that car so we see pretty tricky right<br>but perhaps you can imagine that those<br>two dimensions are going to be pretty<br>interesting features of of Carden of the<br>operative training sets if you can<br>achieve it how to achieve but if you can<br>do it it's pretty interesting okay<br>so here's just another picture of the<br>same sort of thing how would you make<br>use of us so if I here's an example<br>where this is a different kind of<br>picture each dot here on the left is a<br>data point I'm just schematically<br>representing a victor picture of a car<br>they're being pushed through this<br>network just like before and outlets<br>coming in output and I'm learning to<br>match that output to the input and so<br>the output is going to be really a<br>distribution<br>and along the way it's being forced<br>through right down into just two<br>dimensions save a very low dimensional<br>version so each dot here actually has<br>its own little dot down the<br>two-dimensional space frame and then it<br>recovers back and you get the car back<br>so each dot the precise position of the<br>dot means a lot okay so now I can<br>imagine imagine you move I can just pick<br>a dot here and I can make up a car<br>picture or I could move on a line<br>through that sort of central through<br>this compressed representation as I move<br>along through that line what do I get I<br>get different pictures of cars and<br>they'll change probably in some<br>discernible way so let's forget cars and<br>dealt with faces instead this is my<br>colleague Tom white from Victoria's<br>Design School actually let's see if I<br>can do this<br>good okay so I just go straight to the<br>horse's mouth he is looking at ours look<br>at earlier one first so this is this is<br>almost literally that that<br>two-dimensional picture of little dots<br>and as we move these are you pick a dot<br>in the two-dimensional space and go and<br>make a face from it so the whole network<br>has been learned has been learning faces<br>millions of faces and faces the place<br>isn't seen so it knows about places you<br>pick a dot here you get a face as you<br>move along a line this place has change<br>okay so it's exactly what I was saying<br>is happening here and you can see what's<br>happening you know there are there are<br>significant semantically or there are<br>there are recognizable things happening<br>as you move through this two-dimensional<br>space and here he's made up more than<br>two-dimensional and he's holding two of<br>them fixed and moving on a on a third<br>one and it seems to be these the concept<br>of vector for receding hairline is what<br>he says loads of fun so these generative<br>models I guess what I'm really saying is<br>these general models are really<br>interesting beasts and that we've we've<br>recently started to get really good at<br>this so for a long time it was all about<br>supervised learning learning to classify<br>pictures of cars maybe cats you know<br>things and now people are moving on to<br>the unsupervised learning problems which<br>means building generative models<br>and I just want to show you a couple of<br>other powerful generative models here<br>you probably heard Addie Bart deep at<br>it's great fun you can put your own<br>photo into this and have it converted<br>into a style so if you take a network<br>that's been trained on a particular<br>style and then you put a real<br>alternative image and as the input what<br>does it do it tries to turn it pushes it<br>through this bottleneck and then tries<br>to recover the pattern but it's it's<br>going to recover a pattern that's like<br>the ones that seen in its training set<br>so heaps of fun just fine what is that<br>let's skip that now this is the one that<br>doesn't work right my god okay sorry I<br>will find the other browser right yeah<br>okay<br>so this is a this is a piece of fun so<br>this application of that general idea of<br>generative models take the sketch and<br>turns it into a turn into a picture<br>which is the extraordinary thing to do<br>isn't it I hope we can agree it's just<br>an amazing thing for a machine to be<br>able to do of course if you give it<br>something that's that's different<br>it'll get a bit let's just move the tail<br>over a bit okay yep needs out that's<br>great<br>so this this learned model knows about<br>cats so that's great if you have a<br>sketch of a cat unfortunately if you if<br>you don't have it skip to a cat I don't<br>know it's it's a little alarming the<br>sort of things you can do you can have a<br>lot of fun with us right let's get rid<br>of the eyeballs it's going to have to<br>put the eyeball somewhere you know it's<br>going to happen because you can't even<br>count your eyeballs yeah<br>yeah fun hey so I don't know I don't<br>this is a toilet there there are some<br>obvious money-making applications but<br>there's a whole heap of fun also out<br>there for in this world of generative<br>models so I'm a big fan top picture<br>there as pictures of bathrooms this is<br>an a net that has seen a lot of<br>bathrooms in its time from the from the<br>interweb and it's now fantasizing<br>bathrooms so those are all fake<br>bathrooms and and that's that's totally<br>extraordinary<br>this is April this year before that<br>you've got blurry things that you might<br>be able to imagine with bathrooms now<br>they really look like bathrooms so I<br>think this is interesting from the point<br>of view we've all kind of got used to<br>the idea of machines talking to us you<br>know automated voices call centers and<br>so on and to some extent we're used to<br>auto-generated text but we're entering<br>us well where we're going to have a lot<br>of Auto generated imagery<br>it seems coming at us and for a species<br>that is so visually dominant it's I know<br>that's that's an interesting challenge<br>isn't it I think it's going to mess with<br>us a bit okay now I want to talk about<br>something that's missing in this picture<br>so that all that stuff is really being<br>achieved with these that really quite<br>straightforward technology underneath<br>there's lots of tricks and learning of<br>course but essentially the engine is<br>these deep neural Nets and they are<br>trained by the error propagation<br>algorithm okay so there's a fundamental<br>learning problem identified in 1991 by<br>Schmidt Hoover and his PhD student hock<br>writer and they were confident enough to<br>call this the fundamental deep learning<br>problem and it's called vanishing or<br>exploding gradients so what happens I<br>take an error signal and I start pushing<br>the gradient of the the loss the<br>gradient of the difference between the<br>output of me and the act then the target<br>okay I start pushing that back through<br>the network and what happens is the<br>signals of the passing back through the<br>network tend to have a very strong<br>tendency I<br>to blow up and start becoming infinite<br>or to shrink down to tiny honey values<br>and this isn't a minor problem you know<br>I think I would just use lots of<br>floating-point arithmetic and we'll be<br>fine<br>no it's a real explosion or a real<br>vanishing it comes down to linear<br>algebra eigenvalues let's not go there<br>is a is a it's a very big greatest<br>feature of of trying to do this task of<br>propagating the gradients okay so this<br>is really supposedly solved by a bunch<br>of things for one thing you replace<br>sigmoids in the old school neural nets<br>of my youth with rectified linear<br>functions which just Beveridge bilinear<br>just says if my if I'm a neuron and my<br>total input is negative I'll put a zero<br>if it's positive then I output just<br>whatever the positive sum was I was<br>receiving simple as that really dumb and<br>that works really well that they work<br>pretty fine so that's part of the trick<br>a care very careful initialization of<br>the weights help can help a lot here and<br>a technique called batch normalization<br>which is basically just trying to keep<br>these activations to have about the<br>right mean and about the right variance<br>all the time constantly rescaling and<br>shifting them to keep them in the right<br>ballpark ok it kind of makes sense right<br>took a long time for someone to make it<br>really work but it makes intuitive sense<br>those all help they really solve that<br>problem ok so here's the thing I reckon<br>there's another fundamental problem and<br>this is work with David I'll do z who's<br>he's really that the lead creative on<br>all this there's another fundamental<br>problem to learning and efnet<br>it's not the exploding or vanishing that<br>we call it shattering David Stern so<br>we've just written the paper called the<br>shatter gradients problem and the title<br>says it all if resonance of the answer<br>what's the question I'll tell you about<br>resonance and just a second so<br>here we go business so residual Nets are<br>as of last year their king there the<br>state-of-the-art in classifying images<br>things and images for example very very<br>effective and they allow you to learn<br>very very deep nets hundreds and<br>hundreds of layers deep this is great<br>what's the trick we replace instead of<br>just having this straight feed-forward<br>connections through two weights matrices<br>you insert these little hop over direct<br>connections which are just the identity<br>map and one of that make any difference<br>well if you drew it differently so<br>you've just got basically a identity map<br>that's the short route going through and<br>these are all hopping out to do a bit<br>more work so you can imagine that it's a<br>bit like replacing a well is actually<br>replacing that the old feed-forward<br>function with X the input minus the new<br>people function so the nonlinearities in<br>the weights matrices are working on the<br>residual fixing up the errors but<br>there's this backbone of very direct<br>connections okay so for whatever reason<br>this is a really good idea lots of<br>varieties of it but basically that's<br>call them residual nets or risen it they<br>work really well okay so it it was<br>originally said that they were helping<br>with this with this explosion or<br>vanishing gradient problem but if that's<br>true why are they working so well now in<br>these deep nets because really that<br>problem was solved by these other<br>techniques so they're not they're doing<br>something else<br>these skipped connections there's always<br>some other problem all right what do you<br>think of could be maybe it pays to just<br>have a look of some some gradient so<br>here's a heads are really simple neural<br>net it's got it's got two layers and<br>this is what I'm doing it's crossing ah<br>it's a really simple net so it has one<br>input<br>what's a scalar input it has a every<br>hidden layer has a hundred hidden nodes<br>in it and they've all got these<br>rectified linear values and then and<br>then it ends up the output is just a<br>single scalar as well so scale is a<br>scalar X to Y which is great because I<br>can just plot<br>and we're doing that propagation on this<br>network so here's a picture of the<br>gradient as I move along the X I'm being<br>rightist again<br>as I move the input value from 0 to 1<br>say how does the gradient change the<br>gradient I would calculate at the first<br>layer and it steps along as a Manhattan<br>skyline it's got flat but that's because<br>of these relative functions let's look<br>at the overall shape jumps around it's<br>sort of a random walk right below it is<br>a random walk otherwise known as brown<br>noise a random walk<br>someone staggering down the road right<br>so one of the things we've shown is that<br>in the limit of an infinite number of<br>hidden units this is a random walk this<br>is the same distribution ok that's two<br>layers here's what 20 layers looks like<br>with this batch normalization thing<br>which is solving the whole exploiting<br>gradient problem this is 20 layers<br>neural net that's white noise random one<br>totally uncorrelated white noise okay so<br>this is that's probably mean by<br>shattered the gradient is being<br>shattered into tiny tiny tiny pieces<br>that are completely unrelated to their<br>neighbors and you can start imagine what<br>they would do to a learner how can you<br>learn when the gradient information<br>which is the learning signal looks like<br>that<br>you can't all right here's what reson it<br>looks like it's somewhere in between<br>it's not brown noise and it's not Y<br>noise it's somewhere in between it's got<br>local structure but it's not it's not<br>just a straight line either all right so<br>here's our idea here's the intriguing<br>facts actually the gradients of a deep<br>feed-forward net look like white noise<br>Wow it's only 20 layers imagine when you<br>get to 200 and ResNet don't shatter the<br>gradient does not get shattered alright<br>so let's look at some more pictures this<br>is what I'm looking at is as I go left<br>to<br>right along one of these lines the black<br>and white lines I'm just saying is this<br>neuron here's a neuron and a hidden<br>layer is it is its activation on means<br>that outputting a real value other than<br>0 or is it outputting 0 I know that's<br>what black and white means I guess just<br>don't worry about the colored ones these<br>how she pictures them okay so they all<br>look the same at the two layer level<br>fine you go to ten layers things are<br>starting to change let's go all the way<br>to the 50 layers at 50 layers what's<br>happening we should be able to<br>understand us that the the feed plan<br>feed-forward network pretty much all the<br>neurons are either on all time or off<br>all the time that's what actually what<br>the histograms are shown in the blue<br>here neurons are either active all that<br>totally enabled in other words they're<br>just being linear which is not very<br>helpful or they're being they're dead<br>they are outputting 0 neither of these<br>things is contributing any computation<br>to the mapping that we're trying to<br>learn so that's not good a feed-forward<br>network with batch normalization they're<br>the chattering has broken up the<br>activity so every neurons pretty much<br>learn correlated but at different points<br>of the input size they have vary the<br>input any one of these neurons is<br>flicking on and off all the time pretty<br>much randomly it's not random it's<br>deterministic that it looks random and<br>there isn't it's somewhere in between<br>that's nice we can prove theorems so we<br>can talk about the covariance as we move<br>along X the X direction what's the<br>covariance in the gradients actually at<br>different points and you can prove that<br>it dies away 1 over 2 to the power of<br>the number of layers so it's<br>exponentially dying away the covariance<br>of 0 means there's there's no signal<br>left you have clearly white noise it's<br>no connection between neighboring points<br>in the input space basically as far as<br>the learner is concerned it which means<br>the problem is unlivable residents go<br>the other way they actually rise but<br>it's only a slow rise<br>square root of the number of layers<br>really different characteristics so<br>what's going on shattering is shattering<br>is occurring as a feature of deep<br>learning it's it's a really important<br>thing if it actually stops you learning<br>things it's solved one way to solve it<br>is to have these skipped connections<br>that's really what the skipped<br>connections are doing okay<br>so with this insight that was a nice<br>payoff immediately to come up with a new<br>initialization you know understanding<br>something provokes further ideas and so<br>we came up with a new instance<br>realization this is a student the next<br>Leary which enables us to Train really<br>deep Nets of up to 200 layers and<br>hundreds of hidden units per layer so<br>really deep without without any skip<br>connections at all so it's the first<br>time anyone's trained a network that<br>deep without using skip collections and<br>skip connections essentially short now<br>reduce the debt okay<br>so I like this we've attracted at read<br>at page which is you know awesome and<br>one of the comments on that there sums<br>it up really nicely so we really don't<br>know understand how to train neural nets<br>do we there's just such a basic thing<br>fundamental aspect it's just it's just<br>coming to light really ok I want to<br>switch tack to the other flavor of<br>neural nets so we've been talking about<br>deep neural nets as feed-forward<br>enterprises they go from input to output<br>people are really interested in modeling<br>structure and time and modeling<br>sequences and you can do that with<br>neural nets that are very similar with<br>one difference so the idea is we have we<br>allow the hidden layer to feedback on<br>itself and makes that<br>first order Markov system out out of the<br>device if you like so now what I'm<br>saying is the hidden layer we're still<br>mapping inputs to outputs but the the<br>representation in the middle these<br>hidden layers I should say more than one<br>but the hidden layers are receiving two<br>kinds of input they receive input from<br>the actual pattern that's coming along<br>at this moment in time they also receive<br>input which is a copy of themselves a<br>moment ago so in that way they are able<br>to capture structure in time metadata in<br>principle they could learn to use their<br>weight to model structure model<br>temporal processes right it's not a done<br>deal that this is going to work but<br>that's the idea sort of minimal model of<br>a system that could model sequences okay<br>so you can think of it like that or you<br>can unroll it over time I think this is<br>a good way to think of it so that's just<br>the same picture and rolled over time<br>and you can see that that is is that is<br>it like a deep net if I try to propagate<br>an error signal something's gone wrong<br>here and propagate that all the way back<br>it's got to come back to change that<br>weights matrix it's got to propagate<br>through a whole lot of weights matrices<br>it's a deep learning problem so we're<br>only unrolling here we're not these<br>weights are all copies they're a matrix<br>or whatever is going on in the a box is<br>the same at all time all right so has<br>been pointed out that this is this<br>really generalizes the family of what<br>sorts of things you can learn so here's<br>the old input goes to output I'm sorry<br>back the other way around now with input<br>at the top the output at the bottom<br>that's one to one standard old<br>feed-forward network if you like but<br>with this temporal process each one is<br>unrolled here we can imagine a<br>one-to-many mapping that's like I say<br>perhaps a signal of word making a<br>sequence here's the opposite you could<br>take a sequence and map it to a single<br>value such as classifying a sequence<br>who is the speaker or what are they<br>saying many-to-many is also possible so<br>this is what's going on in you know<br>perhaps Google Translate which is taking<br>in a sequence and then having had the<br>whole sequence it's now going to<br>generate a new sequence and the most<br>general one I guess is many to many so a<br>streaming input coming along that's the<br>red squares all through time and we're<br>having to produce outputs all through<br>time as well<br>all right so lots of possibilities lots<br>of scope for fun<br>here's one one rendition picture of the<br>fundamental elements inside one of these<br>recurrent Nets so what we're taking is<br>it looks more complicated than that is<br>probably well all we do is we take the<br>input and it actually is this thing of<br>two parts through this so one is that<br>the input is going to the output and on<br>this picture it's going through two<br>layers of neural nets and so that's a<br>two layer neural net going from input to<br>output and this is going to be<br>replicated over time there's a whole<br>bunch of these strung together if you<br>like so the other path going through<br>here is left to right and that's the<br>hidden state which we might as well call<br>a memory now because it's being<br>propagated over time and the memory is<br>able to pass from memory at time T to<br>memory at time T plus one being changed<br>along the way and it's changed in the<br>light of the current input so you take<br>the memories at the input that changes<br>the memory and here's the memory<br>influencing how input maps to output so<br>it's got it's got the bare minimum okay<br>this this has a bunch of problems so it<br>seems like this is sensible way to start<br>unfortunately doesn't really work very<br>well one problem is it has this<br>exploding vanishing gradients problem<br>but there's more to it than that<br>very early on in the 1990s an<br>alternative action architecture was<br>proposed which is called the long short<br>term memory or LST M and here's a<br>picture of it and you can tell me<br>whether<br>how it works the interesting thing is<br>it's pretty much inscrutable but<br>incredibly successful so this is my<br>second missing where's this thing<br>working so well so LS TM long short-term<br>memory ah looks like this I'm not even<br>going to try and tell the story but if<br>you spend enough time with someone who<br>really knows their stuff you can start<br>to believe fit - this thing makes sense<br>but it's hard - it's that's not the same<br>as saying it's the answer or I don't<br>know there's something there's something<br>really weird about this but it's so<br>successful okay<br>in such a long time in 1987 so it's 20<br>years old incredible this thing is still<br>that the state of the art in the<br>recurrent neural net varieties of it<br>slight tweaks admittedly but basically<br>this idea so Google's translation engine<br>runs with a whole bunch of these<br>speechwriting and state-of-the-art<br>speech recognition all kinds of cool<br>tech that's either on your browser or or<br>on your phone or is coming soon built<br>out of these things and let's just look<br>at some examples so here's a language<br>model I feed something I feed one of<br>these recurrent neural Nets everything<br>shakespeare ever wrote and then so it<br>takes what it does is let's jump back to<br>that picture back there okay I'm feeding<br>it characters individual characters from<br>in that scenario more like that one<br>individual characters you know see a tea<br>that's not a good example bas and then<br>I'm trying to predict the network it's<br>being trained to predict the next letter<br>okay so here's a little window it's just<br>got a sliding window and it predicts the<br>next letter fine that's a predictor I<br>can train it and then just for kicks<br>I'm running that model and when it says<br>K for the new prediction I'm just<br>writing K and so now it's now it's able<br>to just write text forever okay so this<br>is making up Shakespeare<br>and if you have a look at it I don't<br>know it's not that bad<br>it's a bit hard to see the the bigger<br>picture blames protocol I mean it's made<br>you know it's making words it's making<br>spaces there's grammatical sentences and<br>look it has the field right lots of fun<br>this is trained on a few hundred essays<br>by someone called Paul Graham founder of<br>Yahoo store you can tell just by looking<br>at the fake text coming out of this<br>generator sorts of things that people<br>ground right huge fun<br>oh if someone wants to ask me a question<br>later I can show you some some fake maps<br>really fantastic so they got the whole<br>latex of a big book and and chained it<br>up and this then they ran the model they<br>got a whole bunch of latex that actually<br>compiled which means you can make a PDF<br>out of it and look at the proofs that<br>junk proofs and you've probably seen<br>them deep jump this is a Twitter account<br>trained on all all Donald Trump's tweets<br>and it just generates tweets all day<br>long you know stuff like those - they're<br>actually fantastic okay all right what<br>other things can we do<br>keeps that's just the start that's just<br>language stuff we could take you can now<br>take an image and push it to a sentence<br>so instead of taking this image and<br>putting labeled just labels on it<br>we'll take that representation the<br>low-level representation of the image<br>and ask one of these recurrent neural<br>incredibly this works you wouldn't think<br>it would work it was pretty well it's a<br>man a black shirt playing guitar it's<br>true so these are these are these are<br>slightly terrific examples but it's<br>pretty it's pretty effective and getting<br>better all the time<br>so it's taking the whole system end to<br>end so taking a raw picture raw image<br>and it's generating the text at the far<br>end and it's all just neural nets all<br>the way along trained from lots of<br>examples there's nothing else in there<br>okay Oh sentence a sentence this is just<br>a picture to remind myself to say that<br>the what's inside Google Translate<br>technology now is just a big<br>hierarchical multi-layered<br>LS TM network that's what's inside it up<br>until last year they used to have all<br>kinds of clever linguistic machinery in<br>there developed painstakingly over years<br>and years and years a huge thing and<br>then last year they swapped it out<br>because this worked better and this is<br>just there's just one end-to-end thing<br>right it's just a whole bunch of those<br>recurrent neural nets all right the one<br>on the bottom right is Oh taking a<br>sentence and making an image out of it<br>yes it is extraordinary this works early<br>days as well but if you can see them the<br>center is this has seen a lot of<br>pictures of birds okay and then it gives<br>me given a sentence this small bird has<br>a pink breast and crown and black<br>primaries and secondaries and it makes<br>some images of<br>birds how long this is last yeah I think<br>it's are they really the CEO last year<br>yeah<br>okay so takes the handwriting let me<br>show you this this is fun this is<br>someone needs to give me a sentence okay<br>so it's taking kick this was really came<br>from someone trying to build a model<br>that should recognize handwriting but in<br>Reverse now I can scroll it down can you<br>see at the bottom so someone needs to<br>give me sentences different samples you<br>can mess with them all you like you know<br>have a go a lot of fun so that's<br>actually generated this so the text<br>comes in and then a recurrent neural net<br>is actually directing the way this is<br>actually done I believe is that's<br>directing the drawing motion all right<br>the actual writing rather than trying to<br>make a whole image with that sentence<br>from scratch is actually controlling if<br>you like a virtual hand alright um oh<br>and here's someone this is just the<br>other day last week I think this guy's<br>ah okay oh great<br>same idea of Shakespeare okay so you<br>feel then it's going to predict<br>something from the last few frames and<br>we're just going to take it frame and go<br>with it and keep going you could tell<br>how this was train its train on a train<br>it must be that completely you know new<br>countryside it's not perfect but it's<br>to go to the town and was it kickoff I'm<br>not sure yeah for the Shakespeare one<br>you can kick it off with a blank space<br>and it just is okay so I'm not sure<br>maybe it's kicked off with a picture but<br>maybe it's just starts making stuff up<br>and okay where's that going and like I<br>say we're going to start distrusting<br>mistrusting our I don't know the world's<br>going to get interesting so here's<br>something that's missing I feel this is<br>missing at least what's so great about<br>LST m there and they're almost<br>impenetrable in that it's very hard to<br>tell a story in a pub about what how the<br>lsdm works it's a sort of three pine<br>explanation it's it's really hard you<br>can pick out this is it wow this is kind<br>of it but it's not this there's not that<br>very simple story that I was trying to<br>tell about the basic current Network<br>there's two things going on right inputs<br>go to outputs through a neural net but<br>they're mediated by the memory and<br>memories go to new memories mediated by<br>the latest input okay<br>hard to tell that story so you have to<br>wonder how specialist the LST M how<br>unique is it there's an interesting<br>story here that two teams ran very large<br>trials of trying to find something<br>alternative architecture okay so they<br>used genetic programming which is random<br>search to try out all kinds of<br>variations<br>so this LST M I've drawn it as a diagram<br>but it's really just five equations so<br>you can make up equations randomly you<br>can try them out on a big machine<br>they train it on on the Googleplex for<br>for the equivalent of I think about a<br>century of CPU time equivalent and and<br>found the LS TM and a few other things<br>GRU if you notice done is the other<br>alternative but not much right so<br>basically this is this is a some kind of<br>special spot it seems in the space that<br>they were searching at least okay so one<br>idea is to do genetic programming for<br>100 years of CPU time and other idea is<br>to just to think about it and so we've<br>been trying to do that and this is work<br>with Paul Matthews wonderful honor<br>student at every well do is you can't<br>get away from them again this is last<br>year and this year we're pushing on with<br>it<br>so essentially we're trying to think of<br>what's a what's a minimalist model<br>that's that's better than the old that<br>restricted recurrent neural net but that<br>is understandable unlike els TM so and<br>we're going tensors on this so there's a<br>mapping from input the same story right<br>so the inputs map straight the outputs<br>in fact this is just an outer product<br>so basically inputs are going to outputs<br>through a one layer neural net it's just<br>that we do want that to be mediated by<br>what's in the memory so we involve the<br>memory by taking our collection but this<br>is that this is another way so this is a<br>tensor mapping a three-way tensor takes<br>the input and the memory and gives you a<br>new output otherwise known as a bilinear<br>form whereas a straight matrix weights<br>matrix is that it's just a linear form<br>okay so that's that story and memory is<br>going to new memory how's that happening<br>well it's just going straight through<br>really this is a little switch that says<br>you either send the this information<br>straight through or maybe you take the<br>other path and you take something from<br>the input and this is an either/or thing<br>it's not binary but it's in the 0 to 1<br>mixing between what's in the memory and<br>what is coming directly from input<br>as simple as that so how's this decision<br>made about which one you want to keep<br>this is like a bit like a read/write<br>head point you can think of it like that<br>simple way that this is this is saying<br>should I keep what's there or should I<br>overwrite it with a new a new bit how do<br>you decide what to overwrite which which<br>things should be overwritten or not we<br>figure that's a job for both the current<br>input and for the memory state so it has<br>to be a combination of both let's do the<br>same thing so take a tensor product of<br>the current memory and the input and put<br>it through a non-linearity and that's<br>your switch so that's a longer story<br>about it that's a story okay so this<br>thing has had some nice properties so<br>the gradients don't explode or vanish<br>because basically the memories<br>propagating straight through here and<br>we're just swapping stuff out or into it<br>it has as the effect that the gradients<br>don't explode we're avoiding this<br>concatenation of input and heading<br>states which always happen in the other<br>models you sort of push inputs in the<br>head and together into one space where<br>they don't really want to be in the same<br>space now the different entities it's<br>almost a like a type error to do that so<br>we avoid that and we use a tensor<br>instead of concatenation um you know<br>it's a machine learning talk when<br>everyone's frowning and and there's a<br>and there's a graph like this I guess<br>there's a lot a lot of talks have grass<br>ladders but machine learning you've<br>always got one so there's that the loss<br>function training time going left to<br>right along the bottom the training loss<br>test the amount of error the amount of<br>badness badness going away and as with<br>most talks the badness goes away on the<br>stuff we did and stays high on other<br>people did okay so the top two lines are<br>the LST M actually<br>and the EIU which was one of those other<br>random ones found that it was as good<br>- um so ask me about this task if you<br>want to know later but are there are<br>tasks very straightforward simple memory<br>type tasks for which this new<br>architecture seems pretty promising it's<br>early days we're still chasing this<br>around but it there are there are tasks<br>here's an interesting task which we also<br>happen to do well on it's the task of<br>well okay so here's the input pattern<br>vertically having over time case a<br>sequence of input pattern what happens<br>is pages from that very last couple of<br>rows if when this neuron comes on I want<br>you to I want the network to look at<br>these other bits so these ones over here<br>are and remember them and to split them<br>back on the output when that neuron goes<br>off again so the last last of the inputs<br>is a sort of trigger and this is when it<br>comes on remember something when it goes<br>off cough it up and here we're doing<br>that with three of them right there's<br>three memories going in store that<br>outcomes stall that outcomes store a<br>third thing which overlaps actually and<br>then cough it up at the end and it's not<br>perfect this is our our model trained on<br>gazillions of these examples but it's<br>it's having a good go so it had the<br>first pattern comes out pretty perfectly<br>second one close third one is actually<br>pretty good not quite there yet so work<br>to do but that's a that's to be compared<br>with the best this is the very best of<br>the the competition where we try to<br>store three memories we get three things<br>out and they come out at the right times<br>when these neurons here turn off you get<br>a pattern back but it's the same pattern<br>so this is the LS TM the the state of<br>the art that's inside in your phone<br>doing all these awesome things all the<br>handwriting stuff it can't do this kind<br>of task so it's just a conceptual you<br>know there's something it finds us very<br>hard to do<br>we would argue our architecture finds<br>this<br>a really natural thing to do and it's<br>very like variable binding right so the<br>trigger comes along you remember some<br>arbitrary thing hang on to it hang on to<br>it and then when I release the trigger<br>you've got to cough it up at the output<br>so I guess what's particularly appealing<br>about it is that it feels like the<br>foundation for other processes any<br>processes that are cabled to bind one<br>pattern to another and use like a key<br>and then manipulate the key perhaps in<br>some fashion and then get back the<br>original bound item I didn't say that<br>well but I guess we're optimistic and<br>feel that this is the beginning of<br>something interesting bridging into a<br>more symbolic realm for neural nets but<br>it's still your standard neural Nets<br>just got magic tenses them in just an<br>even tougher task when this bit comes on<br>you have to remember the the bit pattern<br>that immediately follows it on the other<br>nose and then when the neuron goes off<br>you have to cough it up again so same<br>times but now there's all this<br>distractor stuff in the middle that's<br>what the hidden representation looks<br>like and that's the output after<br>training so if you're clever you can<br>steal that pattern there it just seems<br>that pattern there I think<br>one two three four yeah it is great so<br>it's learn to do that we never told it<br>we just trained it gave lots of examples<br>in which that's the pattern and this is<br>a new example all right so I'm going to<br>wrap up quickly some all missing things<br>we've talked about why one thing I said<br>was missing was why the state of the art<br>in feed-forward nets is actually the<br>state of the art why is it red nets what<br>are they actually doing argued that it's<br>that they're solving the chateaux<br>gradient problem we said there is a<br>shattering gradient problem and they're<br>solving it and then I talked about why<br>this missing explanation for why is the<br>state of the art in<br>our neural net LS TM I don't have an<br>answer I think because we haven't found<br>a better thing and pointed to direction<br>for thinking about the better thing I<br>think thinking about the problem and<br>trying to gain insight and building<br>something that's got it is interpretive<br>all is a better way to go then randomly<br>searching here's some other questions to<br>throw out there missing things that I'm<br>not going to answer or they're very much<br>missing why are deep nets better than<br>shallow bones this is still up for grabs<br>actually so there's a there's a proof<br>just this year I think was it last year<br>but that relates actually to two versus<br>three layers doesn't generalize so<br>that's it's well why does that teacher<br>at all why neural nets or what's so<br>special about going mush through weights<br>matrix and then having an element-wise<br>non-linearity and then back to awake<br>just doing that and iterating that where<br>does that work so well well it's<br>something else I think there's a coming<br>something that is coming is various ways<br>of making forays into a more symbolic<br>style processing but within this<br>connection is within this neural net<br>world and hinted that our tensor gate<br>idea might be you know one direction to<br>take into that but lots of other people<br>are doing things that are that are to do<br>with for example having external memory<br>and having a neural network being able<br>to read and write from external memory<br>this sort of thing having neural nets<br>able to implement programs that's a hot<br>area lots of fun and my pet passion I<br>guess is trying to understand generative<br>models for really complex multi causal<br>data so you imagine you have some data<br>you look at my go-to example is a face I<br>say my face what you're seeing the<br>pixels that are appearing on your retina<br>are a combination of two completely<br>independent thing<br>one is the physical structure of my face<br>where 3d and the other is the lighting<br>environment and those two things are<br>independent completely independent of<br>fact but they interact and then you're<br>only you get to see the interactions you<br>never actually get to see a naked lady<br>lighting environment only see lighting<br>environments when they had things and<br>you never get to see directly apprehend<br>3d structure you always have it in the<br>context of a lighting environment or<br>some other environment so there are<br>there are pure things out there in the<br>world but they get mixed and then we see<br>the mixtures only can we build a system<br>which is able to go the other way and<br>having seen a lot of data disentangle<br>that to discover the underlying causes<br>that's a big tough problem an unsolved<br>dot many many more things to come<br>there's a lot missing missing is a huge<br>country thank you very much<br>kids let's talk about how to create I<br>agree with sorry I haven't read the<br>books hi today that's why someone will<br>get there I mean I do have a I have a<br>for me I have a skepticism about people<br>the idea that you can study brains and<br>that will we study individual brains<br>enough we'll understand entirely how<br>they work I think they have to be<br>coupled with computational style<br>thinking how do you achieve this kind of<br>computation you have been it's a bit<br>like the analogy would flight that you<br>understand flight yeah you look at birds<br>and that can help but then eventually<br>yes<br>right okay so I would direct you to<br>David well doozies paper called strongly<br>typed neural Nets strongly typed is the<br>the key phrase so his point is when you<br>when I map a hidden so there's a mapping<br>from the hidden state to the new hidden<br>state it's being achieved by this waste<br>matrix so what's the weight matrix doing<br>actually it's it's it's rotating and<br>skewing accident directions right that's<br>what's doing this there's a sort of a<br>native like a basis associated with that<br>transformation and the basis that for<br>that is a different basis from that<br>associated with the input pen the<br>interaction between the input and the<br>hidden layer it's a separate it's a<br>separate basis there in there and it's<br>why would you expect the hidden units<br>dynamics to to be naturally mapped onto<br>to be naturally altered just by additive<br>change has been coming in from the input<br>it's a very limiting way to do things<br>and have a look at there was paper about<br>strong and strong typing I think a lot<br>yes this is the sentences two images<br>yeah<br>the images are so there's a good okay in<br>theory they're supposed to be brand new<br>because we predict they are projecting<br>from this space of the sentence down to<br>a much more modest latent space to make<br>sense low dimensional space and from<br>that space generating the the image so<br>in theory they're they're they're new<br>images in practice have a read of the<br>paper that they're struggling so this is<br>last year and it's a that actually look<br>for ganz generative adversarial networks<br>another kind of generative model they<br>they do seem to suffer from a problem<br>called mode collapse where where the<br>generative model learns to only only<br>produce a very narrow range of what we<br>have birds yeah kind of all or rather<br>similar and so that was a problem it<br>seems to have been solved good to some<br>extent recently so I expect to be much<br>more convincing demonstrations of that<br>soon but no then the whole idea is that<br>they're not supposed to be just stuff<br>you've seen after all that would be kind<br>of boring so most of these papers what<br>they do and they should do is test so<br>they're making up generating new things<br>they should look at those new things and<br>compare them to items in the training<br>set and usually there's some kind of<br>demonstration that yeah we're really<br>making new things<br>hmm hi sorry Sean let's go okay<br>this don't we agree a terrible person<br>with a so he commissioned a Latino yep<br>is it a normally a delicious as it was<br>then Karen technology to capture the<br>optic nerve of an animal's registers<br>underlies the commutative I am not my<br>field of expertise I would say that can<br>you can you chop the eyes or something<br>instagrams a brain something else not<br>currently doable and not something I'm<br>I know it's annoying isn't it it's<br>hugely annoying that there's all this<br>progress and cold stuff and it's all<br>built with that propagation and that<br>propagation is biologically implausible<br>so that's the damn shame for the for the<br>idea that you might think about<br>computation and make your way back<br>towards brains and understand ourselves<br>better a bit of a bloke there is there<br>is a various people have tried to think<br>of alternatives which generally the<br>efficiency goes goes down I think it's<br>still a live question to what all this<br>stuff is working very nicely as brings<br>the pressure on so how's it down by real<br>yeah there<br>oh yeah what I think I think we're we're<br>back then this is I think the same thing<br>I don't know um I can tell you can tell<br>a story about very shallow knees like<br>that but so a single imagine a single<br>layer it's not going to be up to much<br>because mathematically what the each<br>neuron is doing is drawing a hyperplane<br>in the input space and saying I'm going<br>to be on for the side and off for that<br>side and hyperplane can't do much that's<br>very interesting<br>and then I've blank into that space I<br>saw two layer architecture can make<br>shapes that are much more interesting<br>than hyperplanes<br>and there's a proof that if you make a<br>second layer if you make just a two<br>layer architecture<br>widenar the enough hidden nodes then you<br>can approximate anything but does have<br>to be very very wide it's it's not well<br>understood what extra depth adds and I<br>guess this how important is it to have<br>it in layers probably not that important<br>you know if I if I understand the<br>question<br>the the these resinates are really a<br>break with that strict layer wise<br>thinking they have these skipped<br>connections and you can think of<br>multiple paths through the network some<br>of which go through many layers some<br>right now different images in a hurry<br>yes yeah well that's a good argument for<br>visual examples at least alright I see<br>that you can do no wrong but it depends<br>on Google right<br>like the station is or your example I<br>think where it has with some other<br>person some people talking that<br>knowledge which I mean they see a<br>noticeable perspective should receive<br>your 16 good but it's not being sent<br>humor from because and simply because<br>the community sense is not well suited<br>for being the kind of people reading<br>skills like humans are very good at<br>connections it is not no caption they<br>you know how we even at multiple levels<br>as multiple set and stuff like that the<br>standard neural network weaknesses<br>confidence where it is wide open kinds<br>of problems ok and what's what's the<br>question<br>I mean I'm going to agree with you so<br>you're pointing out of this yeah so<br>deepen it's there the language example<br>there is no call okay yeah it's a<br>demonstration of the neural there was at<br>least able to catch a very long<br>well it's Saturday probably not worth<br>right worth exploring it's I would say<br>it's a step towards that so yeah there<br>are things missing right very start<br>there there is symbolic style<br>manipulations that humans achieve<br>somehow but we don't achieve them when<br>the CPU and a register we do it with<br>with neurons and so I'd say these are<br>not there yet there are there lots to be<br>figured out it's not just make it big<br>enough training on math data a little<br>become conscious and actually make sense<br>as you point out the Shakespeare didn't<br>make a lot of sense but I don't think<br>that's the point<br>I didn't have caste in excellent shape<br>Oh too bad<br>yeah yeah so currently with a memory in<br>these neural Nexus is the knowledge is<br>all being stored into the weight values<br>and I didn't talk about it but there's a<br>lot of interest in systems where that<br>the knowledge is they also able to read<br>and write from some kind of notion of an<br>external memory associated memory it's a<br>hot topic<br>it's the fielding interesting things<br>okay that is probably still out there<br>okay yeah sure let's talk after we thank<br>him but thank you very much for<br>welcoming to your country and it does<br>look very big especially like this it is<br>so much missing but you know it soon may<br>shed light on some things that are<br>happening in the area thank you very<br>much<br>we've got one another than the breweries<br>think I'm on guard this one from the one<br>on the committee of the orphan branch of<br>the vibrant profession of New Zealand my<br>peers a co-sponsor of the gillum's<br>lecture series and as I think to youth<br>social history or report life and<br>doesn't like that quickly meeting with<br>is two events coming up next week in<br>register for on the items we website one<br>is of course before Givens lecture on<br>the ethics the ethics of a our that's<br>next Thursday and next Tuesday that we<br>are having the IEP I can showcase events<br>which is also free down at the on window<br>wall 6:00 p.m. next Tuesday also use<br>that discussion on the future of money<br>and we've got Rick sinister on the banks<br>the zero beings are talking about in<br>this is including ail areas the fifteen<br>will end up and client assistance so you<br>can register there also on garden to the<br>site</p></main><footer style="margin-top: 2rem; background: #0001; padding: 2rem; text-align: center;"><p>We Are The University</p><ul style="list-style-type: none; padding: 0; margin: 0;"><li><a href="/">Home</a></li><li><a href="/about">About</a></li><li><a href="/contact">Contact</a></li></ul></footer></body></html>