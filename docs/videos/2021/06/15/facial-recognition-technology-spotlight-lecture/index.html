<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><title>We Are The University</title><link rel="stylesheet" href="/styles.css"></head><body><header><h1 style="color: #fff;font-family: 'Arial Black', Gadget, sans-serif;font-style: italic;font-weight: 900;text-transform: uppercase;">We Are The University    </h1><nav><ul><li><a href="/">Home</a></li><li><a href="/about">About</a></li><li><a href="/contact">Contact</a></li><li><a href="/blog">Blog</a></li><li><a href="/videos">Videos</a></li><li><a href="/authors">Authors</a></li></ul></nav></header><main><h2 style="text-align: center;">Facial recognition technology – Spotlight Lecture [38:09]</h2><p style="text-align: center;"><a href="https://www.youtube.com/watch?v=oGnwII8ygVo" target="_blank">Watch on Youtube</a></p><p style="text-align: center;"><a href="https://www.youtube.com/channel/UCBWjJVwRap6ocot6rs1Gg-g" target="_blank">Te Herenga Waka—Victoria University of Wellington</a></p><img src="https://i.ytimg.com/vi/oGnwII8ygVo/maxresdefault.jpg" alt="Thumbnail for video titled: Facial recognition technology – Spotlight Lecture" style="width: 100%;"><div class="tags"></div><h2>Description</h2><p>Facial recognition technology—a great enabler or a threat to liberty? <br><br>What issues does facial recognition technology raise regarding our collective and individual rights and interests, and should we be concerned?<br><br>If you've been automatically tagged in a photo on social media, unlocked your phone with FaceID, or applied online for a New Zealand passport, facial recognition technology has captured, analysed, or stored your biometric data.<br><br>Use of this technology is growing in Aotearoa in both the public and private sectors. As usage and use-cases increase, so do concerns about privacy, accuracy, and impact.<br><br>What issues does facial recognition technology raise regarding our collective and individual rights and interests, and should we be concerned?<br><br>Join Associate Professor Nessa Lynch (Faculty of Law) and Professor Neil Dodgson (School of Engineering and Computer Science) as they discuss the current and future applications of this technology and consider its benefits and risks.</p><h2>Transcript</h2><p style="opacity: 0.9; font-size: 0.8em">Transcripts may be automatically generated and may not be 100% accurate.</p><p>i am professor neil dodson from the<br>faculty of engineering<br>and today we're talking about facial<br>recognition technology<br>is it a great enabler or is it a threat<br>to liberty<br>i will talk first about the technical<br>aspects and i'll hand over to my<br>distinguished colleague<br>nessa lynch from the faculty of law to<br>talk about the legal ethical moral<br>issues so i think i get the easy part<br>so let's start by thinking about<br>human beings as facial recognition<br>people<br>you are really good at recognizing faces<br>in fact<br>there are bits of your brain that are<br>especially designed to recognize faces<br>faces babies as young as 10 minutes old<br>start recognizing faces it's actually<br>inbuilt so<br>you can recognize that this is 15 photos<br>of the same person even though you may<br>never have seen this person before<br>now the computer has a bit more of a<br>problem than we do<br>and the computer might get thrown by<br>somebody wearing sunglasses<br>and the early facial recognition<br>algorithms did get thrown you just had<br>to put on sunglasses and it didn't<br>recognize you<br>they get thrown by not seeing the face<br>front on and you actually need software<br>to correct for that<br>to adjust the picture so it is front on<br>if you wear a smart hat or at the bottom<br>a silly hat<br>that also messes up facial recognition<br>though you're not having too many<br>problems as a human being the computer<br>is<br>and what really messes computers up is<br>bad<br>lighting we as human beings have<br>incredibly good visual processing<br>systems<br>that can compensate for bad lighting<br>because we've had to deal with it since<br>when we were tiny<br>computers have real problems with that<br>but there is another problem with this<br>set of 15<br>photos which is that one of them is not<br>of me<br>can anyone spot which one is not of me<br>yeah<br>that's bill gates what is bill gates<br>doing in a<br>doing here well the other day i was<br>walking across a car park<br>in wellington and this bloke called out<br>to me hey mate did you know you look<br>like bill gates<br>to which the answer was well yes i<br>actually did<br>so what's going on there he's never seen<br>me before but he sees somebody walking<br>across the car park<br>and the visual processing the facial<br>recognition in his brain is going<br>i know that person i know that person<br>who is that trying to find a match and<br>goes oh that's bill gates<br>hang on this is wellington and bill<br>gates is in america<br>it's not bill gates so hence hey mate do<br>you know you look like bill gates<br>so human beings get it wrong we've got<br>really good facial recognition but<br>we do get it wrong so how do we train a<br>computer to do this<br>now having looked into this there are<br>four different ways<br>big big mechanisms that we have tried as<br>engineers to get facial recognition to<br>work<br>and you'll be happy to know i'm only<br>going to talk about one of them the one<br>that works most effectively today<br>which is artificial intelligence in the<br>middle there that section of boxes is<br>a convolutional neural network and what<br>you do is you start with a convolutional<br>neural network<br>that knows nothing about human faces you<br>feed a face into it<br>and it tells you who it thinks this is<br>and of course it gets it wrong it knows<br>nothing about faces<br>but on the right hand side we design a<br>loss function<br>which tells the computer just how wrong<br>it is because we know who the faces<br>the system doesn't so we can tell it how<br>wrong it is<br>and by knowing how wrong it is it goes<br>and retrains the convolution of the<br>record it just<br>pushes it a little bit towards being<br>right so you put another face in<br>it gives an answer which is wrong you<br>train it some more<br>so if you put lots of faces in and by<br>lots i mean<br>millions or tens of millions of faces<br>you train it on tens of millions of<br>faces<br>where you know who the person is you<br>tell the computer this is the face you<br>tell it who it is<br>it tries to guess from the face who it<br>is and gradually it trains and it can<br>get<br>really really good at this um<br>we're not quite sure what's going on<br>inside the box once it's trained but<br>it does work so once you've got the<br>thing trained you put<br>a photograph to the system and it will<br>give you its best guess as to who this<br>is<br>and assuming that the person is somebody<br>was trained on<br>it will give you a reasonably good guess<br>not perfect but a reasonably good guess<br>so we can do this works really well and<br>it works really well on big<br>groups of people so how do we use this<br>there are two modalities for using<br>facial recognition<br>one i've called confirmation which is<br>saying is that<br>really you i want his identification<br>saying<br>who is that let's talk about these two<br>confirmation<br>this is where you have some token<br>something that says<br>this is who i claim to be so for example<br>my iphone<br>it knows i am the person holding this<br>phone should be neil dodgson<br>i have registered my face with this<br>phone so when i look at it it unlocks<br>itself and i've got really used to the<br>phone just unlocking itself naturally<br>without me having to touch anything<br>just by looking at me with its camera<br>somebody else looks at the phone<br>they are pretending to be me as far as<br>the system's concerned because they've<br>got my phone<br>equally when you go to the border you<br>hand over your passport that says this<br>is who i<br>say i am and the facial recognition<br>system then says is that<br>really you or maybe there are some<br>banking systems where<br>instead of typing in a pin use the<br>camera to recognize<br>is that really you so it's confirming<br>that you're you and these systems can be<br>tuned<br>and you can tune it from always<br>recognizing you to never recognizing you<br>obviously never recognizing you is<br>useless<br>so um recognizing you nine times out of<br>ten is pretty good so with my phone<br>recognizes me nine times out of ten the<br>apple iphone actually gives you<br>five goes so if it recognizes you nine<br>times out of ten and it's got five goes<br>to get it right<br>it gets it right almost all the time so<br>what i mean by recognizing the nine<br>times 10 years you can move the slider<br>so i could get this so it recognizes me<br>99 times out of a thousand<br>and if you're not an engineer you'll be<br>sitting there saying<br>why don't you just move the slider down<br>to the end where it always gets it right<br>and that's because there's a trade-off<br>here if it<br>always recognizes me it will always<br>recognize<br>anybody as me it will basically say oh<br>look there's a face<br>that must be neil so we actually have to<br>um<br>trade off when he gets it right and when<br>he gets it wrong so<br>here's about right gets me right nine<br>times out of ten<br>and somebody else steals my phone<br>they've got a one in a thousand<br>chancellor that will think it's me<br>that's about what you want but apple<br>tell me that's actually a one in a<br>million chance<br>that it will get it wrong and you can<br>move the slider up and down depending on<br>whether it's<br>we don't really care too much if it gets<br>it wrong but we really do worry about<br>annoying the customer<br>uh the left-hand end two at the<br>right-hand end<br>this is a military base if it gets it<br>wrong two times in three<br>that's still pretty good because we want<br>to make sure that nobody who's not<br>now the harder problem because if the<br>system knows<br>this is supposed to be you they're just<br>trying to check that the face matches<br>you<br>the harder problem is having a massive<br>database of faces<br>and we're giving it a random photo and<br>saying who is this<br>and this is where you'll see<br>computational neural network your ai<br>system comes in<br>so who is this uh the problem here is<br>there are no certainties the system's<br>working on probabilities<br>so here let's come back with the top<br>three matches there's a 78 match with a<br>picture on the left<br>a 74 match with a picture in the middle<br>a 65 match with a picture on the right<br>what do you do as a user of the system<br>do you say oh just give me the<br>best match what do you say as a human<br>being give me the top<br>three the top ten the top 50 and i as a<br>human will now work out<br>which one it should be so in this case<br>it's actually got it<br>right but the interesting thing is those<br>two first matches are sisters<br>and even other human beings occasionally<br>mistake them for each other<br>so if human beings are getting this<br>wrong and they do actually look quite<br>different to me<br>um then the computer's going to get it<br>wrong too there is no<br>certainty in facial recognition<br>and the problem gets harder the more<br>people there are in the database<br>so thinking about you and how many<br>people<br>you can recognize we estimate that the<br>human being can<br>really really recognize well<br>about 300 people that's your little<br>tribal group of 300<br>and probably the limit on human beings<br>is about 3 000<br>so that's the celebrities you see on tv<br>bill gates that sort of thing<br>there are five million people in new<br>zealand you couldn't possibly remember<br>all five million faces in new zealand<br>and it's actually quite hard for the<br>computer there are 1.2 billion people in<br>china that makes the problem even harder<br>and there are 8 billion people in the<br>world<br>so if you have a database of all the<br>faces in the world the chances of it<br>getting it wrong are incredibly<br>high so what happens when things go<br>wrong<br>well what happens when that confirmation<br>goes wrong<br>you get locked out so in this case you<br>get locked out of your home<br>if you install the facial recognition<br>lock on your house<br>you risk getting locked out when it<br>fails to recognize your face properly<br>now you're unlikely to install a facial<br>recognition lock on your house but what<br>if your<br>office installs facial recognition<br>instead of scanning an id card<br>we just go to facial recognition and<br>then it doesn't recognize your face one<br>morning<br>and you can't get in through the<br>security door or you can get into your<br>bank account because it doesn't<br>recognize your face you can't go and get<br>a new face<br>to replace the old face it has to work<br>with the face you've got<br>or what if you're at passport control<br>and you get locked out of your own<br>country<br>now what happens today in these<br>circumstances<br>is this computer says no and a security<br>guard wanders over and says<br>oh who are you let's look at your id<br>yeah that's you<br>in you go computer got it wrong what<br>happens when we trust these systems<br>enough<br>that the security guard says no computer<br>said no and that means that is not who<br>you<br>and here is a case where recognition did<br>fail in the identification<br>side of things this is robert williams<br>he in january 2020 is the first person<br>to have been arrested<br>because computerized facial recognition<br>failed<br>he was he the computer made a match<br>between a photo from a heist at a<br>jewelry store<br>and him so the police came to his house<br>they arrested him<br>in front of his two daughters and his<br>wife<br>it's america so they handcuffed him<br>behind his back and they were not very<br>gentle<br>threw him in the car took him down to<br>the station for questioning<br>they questioned him for some time and<br>eventually they showed him a printout of<br>the photo from the jewelry store that<br>would be matched with him<br>and he picked it up and looked at this<br>photo and held it up next to his face<br>and said<br>no this is not me do you think all black<br>men look alike<br>now that last bit was probably not<br>helpful<br>but the detectives looked at the photo<br>and looked at him and said<br>no that that is not you the computer<br>must have got it wrong<br>they still held him in detention for 30<br>hours and it took five<br>months for him to clear his name even<br>though<br>the detector said it wasn't a match and<br>he had an alibi for where he was at the<br>time<br>and he only got his name cleared when<br>the new york times got involved five<br>so things can go wrong when you start<br>trusting this stuff<br>so can you escape it so this is um<br>it's used by protesters to try to fill<br>facial recognition by painting your face<br>with a bunch of patterns that fool the<br>facial recognition algorithms<br>it worked really well<br>until about 2015 when the facial<br>recognition algorithms got better<br>and they went they just ignore the paint<br>and they get it right<br>so the only thing you can really do<br>these days is wear a mask<br>now are you going to walk around<br>wellington with a mask on<br>no you're not but if you're going to<br>protest<br>you might decide you want to which is<br>why certain regimes<br>which i won't name are making it<br>that's worrying okay<br>so this wouldn't be a university talk<br>without a little thought about where<br>we're going next<br>so where are we going next with this<br>this i've already talked about where we<br>are so what's next what's next is<br>recognizing emotions on your face so<br>this is something<br>you do really well as a human being when<br>you look at another person and talk to<br>them<br>you can work out their mental state from<br>how their face looks you can tell if<br>they're happy or sad<br>or interested or bored or confused<br>or tired and you can do that just by<br>looking at their face<br>well computers can do that too my friend<br>rana al kalubi who i worked with at<br>cambridge<br>has set up a company to exploit this<br>which has just been<br>bought out and the use case for this is<br>it's super amazing it's an<br>amazing enabler imagine you set your<br>child in front of an online tutoring<br>system<br>and the online tutoring system can tell<br>whether your child is confused<br>interested engaged bored<br>and based on how the child is feeling<br>the online tutoring system can adjust<br>how it teaches your child in the same<br>way a human tutor would<br>that's super exciting it could<br>however imagine now that your<br>office decides to install this on the<br>computer that you sit at for eight hours<br>to check whether you're interested bored<br>engaged concentrating tired<br>imagine if your office decides<br>that you have to be happy for the whole<br>time<br>that you are at work anybody who has<br>worked in hospitality will know how hard<br>it is to appear happy<br>to the client all the time so imagine if<br>your employer is tracking your emotions<br>all the time<br>then imagine if the state gets involved<br>and this is a news item from just last<br>week from the bbc news<br>a certain government is testing ai<br>demotion<br>emotion protection software on the<br>uighur people<br>and i will leave it to you to imagine<br>whether the leaguers were involved in<br>this<br>are volunteers or not so with that<br>horrifying thought i'm going to hand<br>over to nessa lynch<br>very welcome to victoria university and<br>to the puppeteer campus<br>and so it's so inspiring to see so many<br>people turn out for a lunchtime lecture<br>on what is quite a niche area of<br>research<br>and it's clear that there's a really<br>high degree of public interest in this<br>topic at the moment<br>um so thank you so much neil for giving<br>an insight into the science and also i<br>think<br>foreshadowing some of those ideas around<br>and the impact of society<br>and so i'm going to continue the<br>conversation and look at<br>um what are some of the legal regulatory<br>frameworks<br>and what are some of the benefits and<br>threats of this technology<br>and then we're hoping to leave a decent<br>amount of time for questions<br>and and comments so first some<br>acknowledgements so<br>a lot of the ideas and frameworks that<br>i'm going to talk about today<br>um arise from a research project that i<br>led over 2019 2020<br>and so this was generously funded by the<br>law foundation<br>and and my acknowledgment to my author<br>team so<br>um martin is here in the audience as<br>well so you can direct any difficult<br>questions to him later<br>and and acknowledge our research<br>assistant team as well<br>and so the report is publicly available<br>so i'll be giving a high level sketch<br>today but certainly<br>if you want to read up on any more of<br>the ideas and please<br>look at that report and so i also want<br>to acknowledge<br>um which is public knowledge that i'm<br>part of a team that's carrying out an<br>independent review<br>for new zealand police on the use of the<br>technology at the moment<br>and i'm also involved in giving informal<br>advice to other central government<br>agencies<br>and also i have a role on the data<br>ethics advisory group<br>so just very much emphasizing that<br>anything i say today are my<br>definitely my own views um so in terms<br>of what i'd like to do<br>over my time is i'd like to run<br>about four or five scenarios past you<br>of different use cases and those are<br>designed to<br>tease out some of the ideas around what<br>is the impact or potential impact<br>and this technology could have on our<br>rights and interests so in terms of our<br>individual rights as people our<br>collective rights<br>and our societal rights i want to talk<br>then about<br>some of the current regulation of this<br>technology and<br>and maybe foreshadow some gaps that<br>there might be<br>and then i want to spend my last few<br>minutes just looking at what are some<br>potential options for regulation in the<br>future so particularly looking at some<br>uh initiatives that are happening in<br>other jurisdictions<br>so first thing as i want to do as i said<br>is run some hypothetical<br>scenarios past you so i've got five of<br>them they're pretty simple<br>and they're definitely hypothetical and<br>so don't worry about some of these<br>things<br>and they're not enforced at the moment<br>and but i just want<br>you to start thinking about what are<br>some of the interests and rights<br>and that could potentially affect us in<br>these scenarios<br>so i'll go through them reasonably<br>quickly and and i just want you to think<br>about<br>whether you think these uses are<br>justified<br>do they cause you some disquiet or would<br>you be quite comfortable<br>so the first one um which i think neil<br>had foreshadowed already<br>so you come to work after christmas um<br>employers got a new access system<br>so all of us are always using or losing<br>our swipe cards and<br>trying to remember logons and that so<br>it's going to let you straight into the<br>building<br>let you log on to your workstation um<br>and you don't need to remember any codes<br>it's just going to print the documents<br>out when it's easy<br>all right so think about whether you<br>think that's okay<br>all right what about a second one so you<br>attend an alumni function<br>and so like you did today you come in<br>and the person the staff member greets<br>you really warmly by name<br>lovely to have you back and so what's<br>happened there is a facial recognition<br>camera<br>and has matched your face to the photo<br>that<br>the university holds from your student<br>id and<br>it appears on the staff member's ipad<br>and so they<br>greet you really warmly and and they<br>direct you over to a table where there's<br>some people who are there from your year<br>group and so you feel really<br>warmly welcomed and special<br>all right so moving um more to the law<br>enforcement so say you've got a police<br>force<br>and they've lawfully acquired a huge<br>amount of cctv footage<br>and from public spaces in a city so<br>instead<br>um and so they're looking for they have<br>a suspect and they're looking for<br>various dates and times that a suspect<br>has been in particular places<br>and so it's a serious offense it's a<br>suspected homicide<br>so instead of those officers sitting in<br>front of a computer and going through<br>uh thousands of hours of footage and<br>trying to identify their person<br>and so they're going to use the software<br>to search a facial image<br>and which will just pop out where that<br>person has been in the footage so<br>really good really time saving for those<br>people<br>all right so what about we have a<br>private venue<br>that's hosting um a trade show for arms<br>manufacturers<br>so they've had this event before and<br>there was quite a big protest<br>very disruptive some damage and so this<br>year they're going to partner with<br>police<br>and they're going to set up a van that's<br>equipped with facial recognition<br>cameras and that's going to scan the<br>crowd<br>and they've got a watch list of people<br>who have been arrested before at<br>different protests<br>and they've been arrested for a disorder<br>etc it's going to run against them and<br>if there's a match<br>and there's a police employee who's<br>going to make a decision<br>as to whether other police officers are<br>okay so last one say we've got a really<br>credible<br>information about a potential terrorist<br>attack in a city and so in this city<br>they've got a network of fixed<br>cctv cameras in public places and and<br>there's<br>the ability to switch on a facial<br>recognition<br>and technology capability so the is<br>around in public spaces and public<br>transport<br>so they have this credible threat and<br>they have the reasonable suspicion<br>and so what they do is they make an<br>application to a judge<br>and that judge is going to give<br>permission for a specific period of time<br>where we switch on this capability<br>and and then we run a series of<br>specified images against the watch list<br>all right so what we've done there<br>and hopefully through that scenario is<br>explored just<br>a limited amount of some of the use<br>cases that are put forward under a<br>number of headings<br>and so things like access security and<br>crime control<br>and etc so hopefully in your mind you<br>should be<br>thinking about some of the rights and<br>interests and considerations that are at<br>play here<br>and so if we think of some of the things<br>that we might classes as benefits<br>and so obviously we want to be safe when<br>we're going around our business and our<br>life<br>and we want uh there to be less crime<br>and we want to reduce harm and obviously<br>the state has a duty to investigate and<br>prosecute crime<br>we want we're tremendously interested in<br>efficiency<br>and we want convenience and and we want<br>security<br>but also i suppose thinking about other<br>types of rights we've got a large<br>range of human rights that arise from<br>international and national<br>sources and we know that facial images<br>uh through neil's presentation and we've<br>seen that they're particularly sensitive<br>type of data<br>and i suppose what's interesting as well<br>is unlike other types of biometrics say<br>um dna and fingerprints they can be<br>collected<br>at a distance without your consent and<br>you might not even know that it's<br>happening<br>and so we've seen through the scenarios<br>that we might be affecting a large range<br>of our interests there so we've got<br>an interest and a right in privacy and<br>to respect for our private life<br>and so that extends as well to our our<br>private life and public spaces<br>and we've got particular human rights<br>and and processes that are arise in the<br>context of criminal justice<br>and so on reasonable certain prohibition<br>against unreasonable search and<br>seizure and rules around surveillance<br>and we've got<br>rights to gather to protest and we've<br>got the right to be freed<br>from discrimination so we've also looked<br>as well about other types of constraints<br>and so we've got things like um for<br>instance consent<br>opt-in opt-out and we also saw in those<br>scenarios<br>that there were questions of human<br>oversight or questions perhaps of<br>independent authorization and by a judge<br>all right so what i want to do now um is<br>just to talk a little bit about<br>what law policy regulation<br>um and constraints that we have at the<br>moment<br>and so before we think about what<br>additional regulation we might have<br>let's think about what's there at the<br>moment um<br>so probably when we talk about<br>regulation i would personally class it<br>into two<br>parts i think first of all are the rules<br>um around how we actually collect and<br>retain and share<br>these images and the second is the<br>application of the technology itself<br>and so we've got a huge amount of ways<br>in which<br>uh both in the private sector and the<br>public sector that our images can be<br>collected<br>retained and shared and so we've got<br>formal processes so if you apply for a<br>passport<br>a driving license um if you<br>uh i suppose in the private sector<br>various access control you<br>give your photo to social media would be<br>another one<br>and we've got in the policing space<br>we've got custody images so if you're<br>arrested<br>a formal image will be taken of you and<br>can be uh<br>retained after you've been convicted um<br>so we've got many ways that our<br>image can be collected and retained both<br>by the state and the private sector<br>and so i'll admit that's something that<br>did actually i wasn't quite aware of<br>and before we started this research<br>project and i think i speak for my<br>colleagues on that research<br>project as well is that certainly i was<br>not aware of the amount of information<br>sharing<br>that goes on across the state and<br>internationally in a certain sense as<br>well around biometric data<br>and so this is for very good reasons<br>so identity fraud a genuine<br>need for information sharing and i<br>suppose after the situation<br>and where somebody reached the border<br>under corrections<br>um and fled to another country<br>and was taken back here there was a lot<br>of discussion around identity matching<br>and some legislation put into place<br>and but i think it should be quite<br>important for us to have more of a<br>national conversation around<br>some of that information sharing and the<br>extent with which it happens<br>and the extent to which and we can<br>consent or opt in and out<br>of that and so that's one conversation<br>so obviously<br>um social media and the private sector<br>is another whole area there around<br>sharing of facial images another<br>important area here is<br>the idea of the public space and so we<br>think of the public space as<br>lambton key and um the beach and other<br>spaces where we gather and so<br>traditionally the idea is there of<br>course that<br>unless you have a reasonable expectation<br>of privacy somebody<br>can take a photo of you and that's been<br>in the news a bit recently<br>um but it also extends i think more<br>recently to the online public sphere<br>so we spend quite a lot of time on the<br>internet these days it's our<br>public square and so the rules around<br>how we collect and retain data facial<br>images from the online public sphere<br>so we've also got then what we would<br>more traditionally call the certain<br>surveillance sphere which is the search<br>and surveillance<br>legislation covering law enforcement and<br>obviously the intelligence and security<br>agencies have their own rules as well<br>and so these rules are about essentially<br>when can law enforcement gather<br>information<br>um themselves or when do they need<br>independent authorization in terms of a<br>judicial officer<br>um or some type of independent<br>authorization<br>so obviously when we're collecting and<br>retaining and sharing public<br>our personal data sorry and we've got<br>our privacy act and the various<br>regulations<br>which give us guidance around the proper<br>regime for<br>collecting this and sharing it something<br>that we've seen a proliferation of in<br>the last couple of years as well<br>is government standards around dash<br>ethics<br>and the algorithm charter so these would<br>be very much at the regulation<br>or principle stage so trying to guide<br>and the public use of<br>if we use an algorithm these are the<br>rules that we will use<br>we have human oversight and it's fair<br>etc<br>uh consent can be another huge one so<br>particularly in the private sector we<br>might say<br>you've consented to enter this retail<br>premises or<br>and you've consented to buy this<br>particular program or you've opted in or<br>opted out in some way so that can<br>underpin as well<br>and the use of the technology or the<br>retention of your data<br>so the indigenous data sovereignty<br>movement<br>um again has been a development in the<br>last couple of years coming to the fore<br>so thinking about the special rules and<br>considerations where we<br>gather collect and retain data from<br>indigenous peoples<br>a social license can be another<br>constraint and this is something which<br>um i think has become very prevalent in<br>pandemic times<br>so we've talked a lot about if you<br>imagine two years ago that we would be<br>scanning in and out or<br>that we couldn't leave the country or<br>enter the country or<br>and that would we have ever thought that<br>that was the case<br>and so this is this idea that a huge<br>change in circumstances<br>the christchurch terror incident and the<br>pandemic that those could really change<br>your ideas around<br>what's acceptable and what's normal<br>a reputational risk which is obviously<br>very important for commercial<br>entities has driven um some of the<br>constraints as well<br>and so particularly in the u.s around<br>the black lives matter movement<br>we saw some of the big technology<br>companies and self-regulate<br>and say we are going to ban ourselves<br>from using<br>this technology or we're going to ban<br>ourselves from selling this technology<br>to particular<br>types of entities and so that's that<br>okay so lastly what i'd like to do um<br>and just leading into then our time for<br>questions and comments<br>um is to think about some regulatory<br>considerations<br>and so if we think about the idea that<br>we don't have a lot of specific<br>regulation for facial recognition<br>technology<br>and that it affects a lot of our rights<br>and interests<br>how how might we regulate<br>so i'm going to talk as well about some<br>overseas jurisdictions<br>and that are grappling with this<br>question at the moment<br>so something that we see a lot of in the<br>literature<br>and particularly media around facial<br>recognition<br>technology are kind of phrases like<br>reclaim your face<br>ban the scan and this idea that we<br>should completely ban<br>facial recognition technology um<br>but i suppose what i would urge here is<br>to think back to some of our spectrum of<br>usage<br>so are we going to ban neil from using<br>his face to open his own phone<br>um or are we going to ban people from<br>using facial recognition technology to<br>apply for their passport from home so i<br>think we've got to think of some of the<br>nuances of this<br>and so indeed i was reading the other<br>day about a county in washington in the<br>u.s who has banned facial recognition<br>technology in their county<br>but it was interesting that the mayor of<br>that particular town said immediately<br>but of course this is not going to<br>affect<br>our participation in some national<br>schemes like<br>using facial recognition technology to<br>find missing children and also to<br>identify faces of victims and child<br>exploitation materials<br>and i think he also mentioned about some<br>federal schemes for<br>anti-terror legislation so they had made<br>a big<br>show of we're not going to have any<br>government and facial recognition in our<br>town in our county<br>but they'd already thought about here<br>are some justifications for it<br>and it is my view i think when people<br>talk about banning<br>what they're usually talking about is<br>the live biometric tracking<br>so by that mean we mean what i was<br>talking about<br>setting up a camera setting up a van of<br>cameras and actually<br>doing live tracking of people and<br>running their face against that<br>live footage and rather than those more<br>one-on-one methods<br>and so we've certainly seen moratorium<br>moratoria<br>on that and other jurisdictions so for<br>instance police scotland<br>did a public consultation and run a<br>select committee process<br>and they've decided to actually put that<br>off so they said we're<br>going to introduce it in 2026 but we've<br>spoken to our communities and we think<br>we're actually going to<br>put this on pause we're not going to buy<br>this technology and so we've seen that<br>type of situation um happening overseas<br>and certainly obviously some of those uh<br>advocacy messages have filtered into<br>this jurisdiction as well<br>another option would be to actually<br>restrain the use of the technology<br>through legislation<br>and so some of you may have had a look<br>at the new european union draft ai<br>strategy and rules<br>and so that's really interesting because<br>it's actually tried to grapple with this<br>question of<br>how would we regulate live biometric and<br>tracking<br>so what the eu would do and would be to<br>actually restrict it to very particular<br>situations and say<br>um in a publicly accessible space it<br>would have to be<br>locating a missing child or dealing with<br>really high end offending such as<br>terrorism<br>and so really confining it down to a<br>very serious situation<br>and they would also apply that<br>interestingly to the retrospective<br>analysis<br>something that's been called for for a<br>while by the law commission and others<br>has been a review<br>uh or to implement some of their uh<br>suggested reviews of the search and<br>surveillance legislation<br>so particularly as i mentioned that idea<br>of surveillance in the public space<br>is not well regulated at the moment a<br>lot of our ideas around<br>search and surveillance um are about<br>individuals<br>rather than collective or societal<br>surveillance and so the law commission<br>has certainly made some recommendations<br>around<br>policy statements in that space and so<br>as part of our report<br>we recommended i think looking at the eu<br>rules again the gdpr<br>and thinking about biometric data as<br>being a special class<br>and so that it is highly personal<br>information and<br>should have some stricter rules around<br>it around when it can be collected<br>retained or shared<br>and so obviously one of our<br>recommendations was good quality<br>privacy impact assessment so in the<br>private sector or the public sector when<br>we are thinking about<br>implementing technology and that we've<br>thought very carefully about the<br>privacy and the impact of what we're<br>going to do<br>governance and oversight so some other<br>jurisdictions have a lot more<br>governance and oversight around um this<br>type of<br>collection retention and sharing and<br>and the technology and so for instance i<br>wouldn't particularly<br>think it's a fantastic model but<br>certainly it's been an effort<br>and so in the united kingdom and they<br>have a biometrics commissioner well they<br>used to have a biometrics commissioner<br>and a surveillance camera commissioner<br>it's now the same person and so the role<br>has merged but that's somebody who is<br>thinking about<br>um regulating the biometric data is<br>carrying out audits<br>is this reading policy statements and<br>keeping a general eye on how the state<br>is collecting this data<br>and how it's been used um<br>and the last thing i'd like to think<br>about uh or get you to think about is<br>what ways do we have for citizens to<br>raise<br>their concerns and their complaints and<br>because<br>the really the only uh common law<br>jurisdiction that's had a court of<br>appeal<br>a high level case has been england and<br>wales which was the bridges decision<br>which was um a man who took a judicial<br>review against the south wales police<br>because he had been scanned by a camera<br>a fatal recognition camera in a van<br>and so that sort of action couldn't<br>happen in new zealand because it would<br>be very hard for us<br>an individual citizen who's been<br>affected to bring a judicial review on<br>human rights grounds so we don't have a<br>lot of those mechanisms that other<br>jurisdictions have<br>for taking a complaint and so what i i<br>would adjust<br>and to think about what ways could we<br>have for citizens<br>who know they've been affected by public<br>surveillance and<br>or private sector and to have ways to<br>raise concerns so we do<br>obviously have the privacy commissioner<br>we've got the human rights commissioner<br>and maybe they need more resourcing or<br>more powers and but that is certainly<br>something that i would leave<br>um to you is how do we as individual<br>citizens<br>if we think we've been affected and how<br>do we think about<br>um making complaints about our rights<br>all right i better finish up there um<br>so that we have some time and so<br>i think neil do you want to come back up<br>again and we'll we'll take some<br>questions<br>and so thank you very much for your<br>you</p></main><footer style="margin-top: 2rem; background: #0001; padding: 2rem; text-align: center;"><p>We Are The University</p><ul style="list-style-type: none; padding: 0; margin: 0;"><li><a href="/">Home</a></li><li><a href="/about">About</a></li><li><a href="/contact">Contact</a></li></ul></footer></body></html>