<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><title>We Are The University</title><link rel="stylesheet" href="/styles.css"></head><body><header><h1 style="color: #fff;font-family: 'Arial Black', Gadget, sans-serif;font-style: italic;font-weight: 900;text-transform: uppercase;">We Are The University    </h1><nav><ul><li><a href="/">Home</a></li><li><a href="/about">About</a></li><li><a href="/contact">Contact</a></li><li><a href="/blog">Blog</a></li><li><a href="/videos">Videos</a></li><li><a href="/authors">Authors</a></li></ul></nav></header><main><h2 style="text-align: center;">IPL: Brendan McCane - The Rise of the Machines [1:03:45]</h2><p style="text-align: center;"><a href="https://www.youtube.com/watch?v=7Z5YrB7iuvo" target="_blank">Watch on Youtube</a></p><p style="text-align: center;"><a href="https://www.youtube.com/channel/UCPVjZXotZ5oX9jyXlFFRj2w" target="_blank">University of Otago - Ōtākou Whakaihu Waka</a></p><img src="https://i.ytimg.com/vi_webp/7Z5YrB7iuvo/maxresdefault.webp" alt="Thumbnail for video titled: IPL: Brendan McCane - The Rise of the Machines" style="width: 100%;"><div class="tags"></div><h2>Description</h2><p>Check out - Professor Brendan McCane's Inaugural Professorial Lecture titled "The Rise of the Machines".<br><br>Brendan's research is on the boundary between machine learning and computer vision. That is, how to get a computer to learn what is in an image based on example images. Some of his early work in this area included face detection and face recognition algorithms – similar to the algorithms that now run on all smart phones and social media platforms.<br><br>His work encompasses applications in medical and biological imaging including interpreting x-rays of the lumbar spine and the facial skeleton. Recently he has made contributions to the theory of deep neural networks and deep learning, and to self-learning robotics.<br><br>Brendan is also a passionate teacher and has an ongoing research project on how to best teach computer programming to novice programmers with a focus on mastery learning.</p><h2>Transcript</h2><p style="opacity: 0.9; font-size: 0.8em">Transcripts may be automatically generated and may not be 100% accurate.</p><p>hang amana ayodhya road rocket edema<br>tena koe toe<br>tena koe toe tenet ato katoa noreda<br>kauai Oh Co Harlan hain taka lingua Cote<br>to Milwaukee fre wanaka OA taco Ajo Nami<br>he knew a Kia koto Norte de tena koe tu<br>tena koe toe tena tato katoa kyo defano<br>good evening my name is Harlan hain and<br>I have the great privilege of being the<br>vice chancellor here at the University<br>of Otago and as always it is my pleasure<br>to provide a few opening comments at<br>this inaugural professorial lecture this<br>time for professor Brendan McCain now<br>these lectures are a time of huge<br>celebration for the university community<br>they force all of us to take just a<br>little bit of time out of our incredibly<br>busy schedules and experience from the<br>other side of the lectern some of the<br>amazing teaching that our students get<br>at this world-class university each and<br>every day it's fantastic to see so many<br>people here tonight in the moot court to<br>help to celebrate professor McCain's<br>achievement of the rank of Professor it<br>is a wonderful opportunity for all of us<br>to spend time marking this incredibly<br>important transition in your career now<br>on behalf of the University I'd like to<br>start first by welcoming members of<br>Brennan's family who are here to support<br>him so first and foremost his wife<br>Simone who is here and their daughters<br>Madeline and Zoe welcome to you his mum<br>Shirley is here and his in-laws Jim and<br>Joyce and many other friends and family<br>I think are joining us on zoom so to<br>each and every one of you know my had a<br>my welcome I'd also like to welcome the<br>rest of our audience as I look out I can<br>see staff and students from around the<br>University and also members of the wider<br>Dunedin community and as always you are<br>incredibly welcome to take part in this<br>amazing celebration<br>now at the outset of these IPL's I<br>always like to take the opportunity to<br>remind us all about how difficult it is<br>to achieve the rank of Professor in<br>order to be successful the candidate<br>must demonstrate excellence in teaching<br>in research and in service to the<br>university and the wider community now<br>as you will learn over the course of<br>this evening<br>Brendon has exceeded all of the<br>university's expectations<br>he's internationally recognized for his<br>innovative research in the field of<br>computer vision and machine learning and<br>I see we're going to hear more about<br>that this evening he is an incredibly<br>effective classroom teacher and a very<br>popular postgraduate research supervisor<br>and he's provided exceptional leadership<br>within the University and the wider<br>community commute computing community um<br>and as all of you who know Brendon<br>personally will also know he is a<br>genuinely nice and thoughtful person who<br>is always a pleasure to work with so<br>Brendon on behalf of the university of<br>otago i would personally like to offer<br>you my very warm congratulations on a<br>very well-deserved promotion to<br>Professor Nami he knew a kkeok way and I<br>will now call on Professor Richard<br>Barker the pro vice-chancellor of the<br>division of Sciences to tell us just a<br>little bit more about Brennan's journey<br>to Professor Nami he knew Akiko tono<br>Retta tena koe tu<br>tena koe toh tena Tata Tata<br>vice-chancellor Enoch way deputy<br>vice-chancellors do not go to Professor<br>McCain dinner quo professor Albert Dean<br>aqua friends and colleagues tena koutou<br>tena koutou tena koutou katoa it is my<br>great pleasure to introduce our speaker<br>tonight professor Brendan McCain to give<br>his inaugural professorial lecture at<br>the University of Otago as indicated by<br>our vice-chancellor professor McCain as<br>one of our newer professors at the<br>University of Otago having received a<br>well-deserved promotion to the position<br>in February of this year<br>professor McCain was born somewhere in<br>Australia I will let Brendan fill in the<br>details let's just say it's renowned for<br>its subtropical climate and for long<br>being ruled by Danny works most<br>well-known expatriate he graduated BSC<br>honours from James Cook University where<br>he also went on to complete his PhD in<br>1996 and was briefly employed as a<br>lecturer in the Department of Computer<br>Science at James Cook it was our good<br>fortune that Professor McCain joined the<br>University of Otago in 1996 as a member<br>of our department of computer science<br>which he went on to lead as head of<br>department from 2007 to 2012 a role he<br>was appointed into while still a senior<br>lecturer I'll come back to that in a<br>minute with four books and around 90<br>fully peer-reviewed publications<br>professor McCain is a highly productive<br>researcher his research focuses on<br>computer vision pattern recognition<br>machine learning biomedical imaging and<br>robotics some of his early work in this<br>area included face recognition and face<br>recognition algorithms similar to the<br>algorithms that now run on our<br>smartphones and social media platforms<br>if were on them and we'll hear a little<br>bit more about that from Professor<br>McCain he is recognized also for his<br>outstanding commitment as a teacher and<br>this commitment is shown by the fact<br>that not long after he joined the<br>University of Otago he enrolled in a<br>postgraduate certificate in tertiary<br>teachings he takes it<br>seriously he loves teaching and his<br>students love him he won an AUSA<br>teaching award in 2005 he was nominated<br>for an AUSA inclusiveness and teaching<br>award in 2013 he was nominated for an O<br>USA Teaching Award in 2017 as I remarked<br>on earlier<br>professor McCain was appointed head of<br>department while still a senior lecturer<br>which is an unusual appointment that was<br>in recognition of a gift he has for<br>administration although he probably<br>doesn't view this as a gift more as a<br>curse he was also asked to oversee the<br>teaching out of the design program which<br>was was a challenge and that's a<br>recognition of the quality of leadership<br>that he provides and he successfully<br>completed this in 2017 and his<br>willingness to take on these roles<br>professor McCain has been an exemplary<br>University citizen and all of us who<br>have worked with him on committees that<br>all levels in the university have<br>appreciated the thoughtful wisdom that<br>he brings to decisions he is also well<br>known for his passionate appearance of<br>unnecessary bureaucratic forms we all<br>hope he brings his a eye and deep<br>learning expertise to their elimination<br>tonight I don't expect we'll hear much<br>more about forms but rather professor<br>McCain will give a personal or potted<br>history of machine learning as it<br>applies to computer vision the part that<br>he has played in its development and<br>where we are at with robotics please<br>join me in welcoming professor Brendan<br>McCain<br>well thank you very much a very kind<br>introduction Richard and Pauline and I<br>should say start out that it is a<br>privilege to be promoted to professor at<br>this University<br>and I'm just hoping nobody finds out<br>it's not a syndrome if you really are an<br>impostor<br>so as Richard said I'm going to talk<br>about it's a very potted history in a<br>personal history of machine learning<br>sort of AI artificial intelligence ish<br>and computer vision and my very small<br>part in it I recently was looking<br>through a photo book and found this<br>photo of me I don't know was nine or ten<br>I don't actually remember that nor<br>having a desire to become a professor<br>but there you go Here I am so I'm gonna<br>go through a history history and where<br>I'm gonna start quite a long time ago so<br>this stuff started before actually<br>computers really were much of a thing or<br>a thing at all and one of the famous<br>data sets that are used in machine<br>learning was introduced in 1936 and I'll<br>explain a little bit about this so does<br>anybody recognize what these flowers are<br>their irises and in fact they are<br>introduced by Ronald Fisher who was a<br>famous statistician sort of the early to<br>mid 20th century and the task that he<br>introduced at the time was classifying<br>the difference between irises so can you<br>determine sort of in an all about sort<br>of automatic way which of the irises<br>that you're looking at if you're an iris<br>expert which I am NOT you might know<br>that this one's versicolor<br>this one's a virginica and this one's as<br>a toaster now computer vision is<br>interested in can we recognize things<br>like this from a picture of the thing<br>but they didn't really have digital<br>cameras in 1936 now more computers to<br>analyze the photos with so they require<br>a little bit of human help so what one<br>of fish's colleagues did was measured<br>some things from the irises themselves<br>so there are four measurements and I<br>didn't know what a sepal was until today<br>when I thought better find out what the<br>hell that means<br>the sepal is this sort of green thing<br>that sits just below the actual flower<br>petals supporting the petals so if you<br>measure the sepal length of Seba with<br>the petal length and the petal width and<br>the idea is maybe from these<br>measurements we can determine the<br>difference between which flower is which<br>and so the idea was is that you take<br>these measurements and then you create<br>some sort of a model or function or<br>decision procedure if you like and when<br>you come to a new flower and you're not<br>sure what it is you make those<br>measurements again you plot it on the<br>graph and you decide well which one is<br>it so this is a two-dimensional plot<br>with just two of the measurements so<br>we've got the sepal width and sepal<br>length on the y and x axis respectively<br>and you can see that the Satou sir is<br>pretty well separated that's all over<br>here so if I drew a line here I'd pretty<br>much be able to tell all the suit OSes<br>from the other ones the other ones are a<br>bit mixed in these two dimensions but<br>remember we've got four it's a bit hard<br>to both draw and visualize but we've got<br>four measurements so we end up with four<br>dimensions so you might imagine the<br>other one going into the wall or out of<br>the wall so petal length and the fourth<br>this is so this is a bit of a theme that<br>will come to a bit later so you can<br>think of this as a four dimensional data<br>set okay so that's that was early<br>there's 1936 a whole bunch of stuff<br>happened unimportant historical events<br>between 1936 and there's a bunch of<br>stuff that did happen in the 50s and 60s<br>but I'll come back to that a bit later<br>I mean 1970 when in 1969 something<br>important happened there's a publication<br>of a book called perceptrons by Marvin<br>Minsky and Seymour Papert<br>and I mention why that's important in a<br>second but in 1970 Minsky who was a<br>founder in a I said from three to eight<br>years we will have a machine with the<br>general intelligence of an average human<br>being well he was kind of right in three<br>to eight years we had many such machines<br>I was one of them but he didn't really<br>mean those sorts of machines um and<br>there was a lot of hype about AI at<br>about the time and unfortunately it<br>didn't really live up to the hype and<br>certainly we still don't have general<br>intelligence from artificial machines<br>now and and as a consequence of the the<br>hype and its lack of you know living up<br>to the hype 1970 was about the start of<br>the first AI winter I'm pretty sure this<br>was coincidental with my birth and not<br>caused by it and and what that means is<br>basically the funding for research in AI<br>sort of dropped precipitously so in the<br>70s this is just a little bit of<br>background for me so I grew up in a very<br>small town<br>in called home hill in North Queensland<br>and I tell people doesn't happen so much<br>anymore but when I certainly when I<br>first came people ask you know where<br>you're from and I say North Queensland<br>and they'd say ah the Sunshine Coast<br>I've been there this is the Sunshine<br>Coast here and I said no a bit further<br>north and I'll Bundaberg yeah I've heard<br>of that<br>this is Bundaberg and this is still in<br>the South of Queens Queens it's a big<br>place I come from up here<br>which is near Townsville which is where<br>I went to university it's a small town<br>farming community it's not a hive of<br>intellectual activity generally and in<br>fact it's famous for this and it's the<br>only place in Australia that still does<br>this so this is a cane field and when<br>practice<br>you can see this all the smoke in the<br>excellent getting more things on<br>my mother would tell you at length<br>so I grew up it was a small town and my<br>parents um who you know formed a pretty<br>important part of my childhood<br>there were six of us there are six kids<br>five of us my parents never went to<br>university but five years mum and dad<br>here fortunately my father passed away<br>earlier this year um five of us from our<br>family went to university and you know<br>it's not unfair to say this is because<br>of my mum and dad I'm encouraging us to<br>do well at school and a lot of this<br>comes from actually just encouraging<br>learning in general and reading to young<br>children so I mean I think if you want<br>to encourage your children to do well<br>academically just start reading to them<br>when they're one-day-old okay so anyways<br>things happened in the 1970s you know<br>some important stuff ái stuff nothing<br>that I'm going to talk about too much<br>actually neural networks started<br>becoming important again but I'll come<br>back to that and explain what a neural<br>network is but in 1984 again this is<br>from the statisticians really started<br>off this field and this the<br>statisticians introducing another very<br>important type of structure which forms<br>a kind of aside from your network it's<br>kind of one of the important things in<br>machine learning and these are decision<br>trees that pretty simple but they're<br>very very effective and then they're<br>simple in the sense of they're easy to<br>use the automatic procedures that are<br>used to build them just work and they<br>produce pretty good results so here's<br>the iris data again so here's an example<br>decision tree which I generated from the<br>IRS data<br>it's just a bunch of yes/no questions<br>right so the first one at the top box<br>you start on the top box and says is the<br>pedal length less than 2.5 and if the<br>answer at 2.45 and the answer is yes<br>then we've got a can't remember what the<br>color means we've got a so Tosa and if<br>it's greater than 2.45 then we check<br>another variable in this case it's the<br>pedal width and if that's less than 1.75<br>then we've got a versicolor and<br>otherwise we've got a virginica so with<br>these three very simple rules that are<br>sort of chained together we can pretty<br>much separate this whole data set now<br>these are on different dimensions than I<br>showed before right so much easier to<br>separate on these dimensions and they<br>they're pretty good right so they you<br>know gets a couple wrong but okay you<br>expect a little bit of error the beauty<br>is it's really easy they're really easy<br>to use<br>and they do generalize to rather more<br>complicated examples so here's another<br>example of a data this is a made-up<br>dataset so this is we want a decision<br>procedure for telling the difference<br>between the blue dots and the red dots<br>okay so we have to build something so<br>this is right a bit more complicated<br>this is a sort of famous spiral problem<br>because we can't easily just use a<br>simple line but if we if we use a<br>decision tree again we can run this very<br>simply we get actually a pretty good<br>result right so it's able to build this<br>sort of complicated decision surface<br>between the Reds and the blues I mean it<br>has a little bit of a limitation because<br>these are either horizontal or vertical<br>lines and so you get this sort of<br>blocking nature of the decision result<br>but generally it works pretty well so<br>other stuff happened I started<br>University at James Cook which is in<br>Townsville in 1998 this is actually a<br>graduation photo I was really inspired<br>by a brief history of time and I thought<br>I might be a physicist but I like<br>computing so I did computing computer<br>science as well I like math so I did a<br>bit of math too um I got to university<br>and the computer science the computer<br>programming labs are really exciting I<br>mean you get a problem you solve it and<br>you knew you solved it because the thing<br>would run and it would produce the right<br>answer is great and it was a little bit<br>like math because you get a problem in<br>mass and you solve it you kind of know<br>you got it right you're doing a proof<br>you're trying to prove something or<br>whatever but you couldn't run the damn<br>thing wasn't quite as exciting and<br>physics was good to accept the prax was<br>so boring it's like three hours long of<br>doing stuff I couldn't<br>now I signed up for physics for second<br>year as well then I discovered there<br>five hour prax anymore so I ended up<br>doing computer science and maths as well<br>and and this was also the start of the<br>second day I winter again because of<br>overhyping I'm pretty sure it didn't<br>have anything to do with me then either<br>I hope at least and then we get I get to<br>the end of 40 here did my honours and<br>you know I was brought up in a small<br>town and farming community mostly it's<br>they're practical people and so if you<br>go to university it's really to go to<br>university so you can get the job and<br>what are you gonna do with that and<br>actually I did my fourth year honors<br>project was in computer graphics and<br>was in bet drawing and rendering clouds<br>now in 1991 we didn't have a color<br>printer in the computer science<br>department but there was a color printer<br>in the engineering school so I went over<br>for printing out my thesis I went over<br>to the engineering school and printed<br>out the pages with my clouds on them<br>because if I could have a blue<br>background looked a bit prettier and you<br>know there'd be a few engineers who I<br>knew because they were doing some of<br>them would come around say what are you<br>doing what is it what's what's your<br>honors about is about just classes why<br>what use could you have possibly for<br>producing pretty clouds not much as it<br>turns out except that one of the PhD<br>students who as associates who was a<br>associate supervisor for actually went<br>on is working at Pixar and is kind of<br>famous for producing clouds that didn't<br>really have anything to do anyway so<br>ended 91 so I've got this decision<br>there's not many jobs in Townsville for<br>computer scientists not many interesting<br>ones anyway you go being @yt person but<br>that's pretty I thought at the time<br>pretty dull and so you know well what am<br>I going to do I could go down to<br>Brisbane or Sydney get some work and I'd<br>really like to say at this point that<br>the reason I did a PhD is because I<br>absolutely passionate about research and<br>I just wanted to find out about how<br>stuff worked well it's true that I was<br>saman it's clearly I'm batting way out<br>of my league start and someone had two<br>years left of degree to go so I wanted<br>to stay in Townsville that got offered a<br>alright so now so my area sort of<br>computer vision and what we want to know<br>is you know how can when you take a<br>picture and then decide whether this<br>thing is quick test what is it we know<br>it's an iris what sort of iris I don't<br>know what it is we want to know what<br>sort of iris that is for example so what<br>does the computer see see you've got an<br>image right we're seeing where you see<br>this thing it makes sense to us what a<br>computer sees is this which is just a<br>bunch of numbers so each of those now<br>this is actually a smaller image than<br>the image you've got but this is a<br>twenty by twenty image which is quite<br>small but it is that image so each of<br>the pixels each of the elements in this<br>picture is just a number actually three<br>numbers it's a red number a green number<br>and a blue number but we can think of it<br>as just one number now it's completely<br>completely crazy to say take this just a<br>bunch of numbers it's just this raw<br>numbers and say can we say whether this<br>is what sort of irises I mean nobody in<br>their right mind would want to do that<br>so my PhD so this the standard sort of<br>way of doing things at this time so this<br>is early 90s and this is what I did for<br>my PhD to UM was well we've got to take<br>these images and we're going to extract<br>the out something that makes sense from<br>and typically this means forming parts<br>of the image somehow<br>so these are pictures actually from my<br>PhD they're just a bunch of things I had<br>that I didn't actually use like the iron<br>just hanging around the house or the<br>office well we typically did you take an<br>image and you do some edge detection<br>this is a well-known sort of<br>mathematical well-defined operation to<br>do and then weave from those edges would<br>form parts so we just say you know which<br>forms a segment and from those parts we<br>would take measurements like we would<br>for the RS data so we measure things<br>like figure out the length of that part<br>or the width of the part or you know the<br>color of the part and then that<br>information and also you'd figure out<br>the relationship between different parts<br>and that information where it passed<br>into some recognition algorithm and at<br>the end you'd get labels out so in this<br>last picture here down here see the fine<br>cross-hatching is actually what my<br>algorithm correctly classified and the<br>coarse cross-hatching which is a little<br>bit down the bottom is what it<br>incorrectly classified so I was able to<br>sort of work and this is actually a<br>great thing about vision computer vision<br>up until about the year 2000 it didn't<br>really have to work it's a fantastic<br>area to do research in because you never<br>had to get anything - it just had the<br>kind of work and that's better than<br>anything else and so that was a good<br>enough um these are my supervisors<br>Olivia davell up the top and Terry Kelly<br>down below and they were very I have a<br>few interesting stories that I could<br>tell but I don't have time really so<br>there were a great influence on my my<br>PhD and later<br>so Terry we visited in he had moved to<br>Alberta and we visited for our first<br>sabbatical which was in 2003 which was<br>in Canada<br>and Zoe was born there in Canada um so I<br>guess that the easy story here is that<br>the the way it was thought that we<br>should do it as do sensible things right<br>so you do sense when you get parts you<br>get attributes you measure things you<br>try to recognize from that in 1997<br>that's when I arrived here at a Targa in<br>January and I just want to point out I<br>just want to highlight a few people none<br>of these people are with us any longer<br>so if you're not on this list that's<br>because and you think you might should<br>be it's because you're not dead yet so<br>Brian was a head of department when I<br>arrived in fact he's the head of<br>department that appointed me I mean he<br>was the founding head of department of<br>computer science so he's had a huge<br>influence on computer science at Otago<br>and in fact was very important in the<br>New Zealand Computer Society as well<br>various leadership roles there I came to<br>Otago in 96 the giver to do an interview<br>and I gave a seminar as part of the<br>interview process and I talked about the<br>stuff on the previous slide which is my<br>PhD Brian fell asleep I didn't think<br>this was a good sign but it's alright I<br>have continued on the Brian's ways<br>because I often now fall asleep in<br>seminars he didn't get to hear much of<br>what I said so maybe that was a good<br>thing at the time our administrator was<br>Kay Saunders and she's been the<br>administrator for the whole time I was<br>here a departmental administrator<br>including the time when I was head of<br>department and was fantastic support for<br>the whole department Kay passed away<br>unexpectedly earlier this year and she<br>is sorely missed and Kevin was my<br>closest colleague probably my closest<br>collaborator when I arrived so he had<br>just started just before me he was a<br>typical American New Yorker he was<br>neurotic it was great to be around yes<br>so unfortunately he left he had left<br>soon after and had you know gone back to<br>the u.s. he passed away but he left the<br>department quite a decent-size request<br>which we which we are grateful for and<br>vilem also passed away earlier this year<br>and he was quite a character in the<br>university as well as in the department<br>he did a lot of good things for the dip<br>grad for example but vilem was really my<br>first introduction to what you would<br>think of as a classical academic would<br>be I mean he just didn't care about the<br>world and the trappings of the world<br>really<br>he was interested in you know the<br>knowledge and he was interested in<br>helping the students and that was so I<br>actually learned a lot from bilham<br>unfortunately none of these guys anymore<br>okay so when I arrived at Otago and I<br>was just establishing myself as a look<br>at the end of the 20th century there's a<br>lot of emphasis on doing real-time<br>processing so we're trying to get things<br>work quickly well or not work but if<br>they're not working at least they're not<br>working quickly rather than not working<br>slowly so so I did a bit of work on real<br>time processing so this is a couple of<br>examples so this is like eye tracking so<br>the funny looking lines are the tracking<br>of their eyes and this is this is me<br>actually this is doing sort of skeleton<br>tracking and this has become not<br>necessarily my stuff but<br>this has become a standard thing it made<br>its way into Xboxes and various other<br>things you notice that at the end there<br>kind of work but didn't quite but that<br>was still alright in 2001 but so I went<br>and I presented this at one of the good<br>conferences in computer vision called<br>cvpr<br>and at the same time there's this paper<br>that was presented by these guys viola<br>and Jones and it was about first face<br>detection so I just want to just briefly<br>talk about this a little bit and they<br>did some they did a couple of things<br>that were really interesting and quite<br>novel one is they they didn't they did<br>and this becomes a bit of a theme they<br>replace doing smart things with doing<br>stupid things fast and actually this<br>kind of works pretty well so for face<br>detection what they did I said well<br>we're not how we're going to determine<br>if there's a faith if their faces in the<br>image is we're just going to take these<br>little parts there it's a little box<br>like a the top left there and we're<br>going to scan that across the whole<br>image and in every little box position<br>we're going to say is this a face so now<br>it's turned what is a detection problem<br>into a recognition problem actually<br>right so you get here it's gonna say is<br>this a face or is this not a face you<br>say it's not a face okay so that's<br>pretty straightforward and if you you<br>think about all the possible boxes there<br>are in the image there's a lot of them<br>if you've got a thousand and it's<br>relatively small image now a thousand<br>pixels by a thousand pixels you've got<br>about a million boxes if you want to do<br>that real time which is 25 frames a<br>second you've got about 25 million times<br>you've got to decide is there a face<br>here or is there not and that's pretty<br>hard to do quickly and accurately so<br>they introduced a couple of interesting<br>innovations one is this idea of a<br>cascade<br>of classifiers so they quickly throw out<br>things that they can tell the difference<br>they're definitely not a face right so<br>the first box here they asked this<br>question but it's a simple question is<br>this a face definitely not a face it's<br>not a face we don't consider that box<br>anymore and if it is a face we'll go<br>into a more difficult question well it's<br>kind of a bit more like a face and the<br>other things but is this one a face and<br>as we go further along and say we might<br>just cut out 50% of the non faces every<br>time so you get 50% non faces second<br>time you get only 25% left the third<br>time twelve and a half percent left you<br>don't have to go down many of these<br>boxes before you end up with no non<br>faces left but the decision becomes more<br>difficult as you go along<br>and so these classifiers in the boxes<br>have to be more complicated but that's<br>okay because you're doing them on<br>relatively few boxes so the end<br>classifier is quite complicated but<br>there's hardly any boxes you have to run<br>that on and so you can get this mic<br>going really really quick because that<br>first one we can do in only a few<br>operations and the second one might be a<br>few more but that's okay<br>and each box here in each box is an ADA<br>boost classifier and these are kind of<br>typically built using decision trees but<br>there's just take a bunch of them and<br>sort of sum them up and take an average<br>okay so it's a combination so these ADA<br>boost classifier is a combination of<br>simple classifiers usually decision<br>trees now ADA boost is also cool so it<br>can with and again it just works so<br>whatever problem you've got you can just<br>plug in just ask it to run and it'll<br>produce what's a pretty good result<br>robustly so adaboost is cool and there's<br>also the other thing that they did was<br>instead of having sort of sensible<br>processing<br>of the data like trying to find its best<br>segment the image to find part like eyes<br>nose etc they would just say well we're<br>not going to do that we're gonna<br>generate a whole heap of data and let<br>the algorithm decide what's the most<br>important stuff so this becomes much<br>more data-driven and they use these<br>things which are these sort of filters<br>so you can imagine a filter here if you<br>sum up all the pixels in the white box<br>and subtract the sum of all the pixels<br>in the black box and the pixels in the<br>black box around the eyes are their bit<br>darker the pixel in the white box is<br>around the forehead so it's a bit<br>lighter you end up with a number at the<br>end and this number will be high if it<br>happens to be over the eyes and low if<br>it's not and similarly here if we've got<br>on the bridge of the nose that tends to<br>be brighter than the eyes itself and so<br>if you sum up the black why subtract the<br>black again you get a high number if it<br>happens to be on the eyes but you don't<br>make a decision beforehand about which<br>one of these is going to be important<br>you just let the algorithm decide and<br>pick the important ones that help<br>separate the training data that you have<br>so we're heading towards more much more<br>data-driven so this is sort of doing<br>stupid things fast and more recently<br>I've used exactly this algorithm<br>basically to find eyes and cows from<br>infrared images because they like having<br>their photo taken now you might say why<br>the hell would you do that well it turns<br>out that if you've got an infrared image<br>these it calculates temperature and the<br>value of the infrared image in the eye<br>gives you a good indication of the cow's<br>body temperature you can't take it from<br>the outside because it heats up from the<br>Sun if n shade whatever you can get<br>inside the body which you can sort of an<br>image of in the eye I can tell you what<br>the body temperature the cow is that can<br>tell you if the cows sick or not before<br>there are any other symptoms and so you<br>can separate out the sick cows and sort<br>of an an automat<br>or tell the farmer you know this cow<br>looks like it might be getting sick get<br>him away from the rest of the herd so<br>you don't get the sickness spreading<br>through everyone okay so there's one<br>other before I get to the main bit<br>there's one other thing that I need to<br>talk about and that support vector<br>machine so remember how I said that the<br>decision services decision tree that<br>sort of vertical and horizontal that's a<br>bit of a limitation for support vector<br>machines we can be much more general but<br>again these things just work but what I<br>want to do what I'm going to do so<br>here's a line which is a nominal<br>decision surface and I'm gonna run this<br>animation which is just going to rotate<br>the line and I just want you to tell me<br>when to stop when you think what's the<br>best line to separate the red and other<br>color alright so there are many possible<br>lines that could separate this one the<br>current one obviously does not separate<br>right because on one side of the line<br>they're a red and that other color and<br>on the other side there are also red in<br>the other color so we want one side of<br>the line to be just red one slow the<br>other the other side of the line so you<br>got to tell me when to stop stop<br>sure that's pretty good actually<br>why do we stop now I think I'd probably<br>go a little bit further maybe there why<br>is that a good line to separate those<br>two and this is a problem that support<br>vector machine Souls it says so the idea<br>as it says it just chooses the closest<br>few points and says what's the best way<br>of separating these close viewpoints and<br>I'm going to ignore all the rest of the<br>points so these we would end up with<br>these few points here and these few<br>points here and it doesn't matter what<br>the rest are doing and I'm just going to<br>put a line sort of halfway in between<br>those and these things are called the<br>support vectors and that's why it's<br>called a support vector machine<br>support vector machines are very cool<br>again they just work they work really<br>well and they can do very complicated<br>things like this spiral without any<br>trouble at all they have hardly they<br>have very few parameters to tweak so you<br>just you don't have to worry about it<br>very much the problem is that they don't<br>scale very well to very large datasets<br>for technical reasons so if you have a<br>lot of data you need to do other things<br>but you can and I had a student and we<br>were able to cascade them learn from a<br>lot of data and again you know even in<br>2008 this is doing AI detection also<br>nose detection sometimes in ear kind of<br>worked okay so that's the machine<br>learning so and in parallel on the<br>computer vision side something happened<br>in the late 90s and really carried on to<br>2012 this is not my work this was<br>someone from UBC and he introduced<br>something called sift and this generated<br>a whole bunch of derivative research and<br>and and but sift is more like going back<br>to deciding what the important things is<br>as a researcher right so making not so<br>much being data-driven but being let's<br>do sensible things they're quite<br>complicated the sensible things but<br>nevertheless what effectively what it<br>does it finds points in images that<br>stable when you have different views of<br>the same scene so for example here's a<br>good one here have in the middle of a<br>curtain we have this the points are<br>represented by these big circles and<br>sort of the direction of the point this<br>is a bit abstract it was just go with me<br>here the direction of the point and sort<br>of give us an idea of its identity so<br>you see in this scene in the curtain<br>here we've got a quite a big point<br>angles this way in a different view<br>we've got the same point being found so<br>it's quite a different scale but the<br>angles the same and the relative size of<br>this thing is about the same and we'll<br>find that there's several other points<br>that also do this it is one here and<br>here one here and here one here and here<br>and we can use those to match one image<br>to the next and we can use that for<br>doing image retrieval like finding the<br>closest image that we've got to an input<br>or query image or we can do that use it<br>for 3d reconstruction once we match<br>points we can then use some fancy<br>mathematics to figure out what the 3d<br>structure on the scene is so this was<br>pretty much all that was going on in<br>computer vision from the late 90s to<br>about 2012 and one of my students again<br>about the same time did some research on<br>this and we were looking at doing<br>localization in doors it's like a Google<br>Maps for indoors and he had a a phone<br>app to do that and it you know worked<br>pretty well actually so here's a summary<br>up to 2012 ish so we had on computer<br>vision side we had sift features and<br>derivatives and machine learning we had<br>these ensembles and sort of using<br>decision trees and support vector<br>machines those things were great they<br>just worked and what combination of the<br>two often had these classifier Cascades<br>just to make things work faster so that<br>was that was all very good at about the<br>same time there was this image net<br>challenge that was introduced so this is<br>introduced in 2010 and this was a really<br>big data set so we're trying to get up<br>to human level recognition of things<br>really so there's a thousand categories<br>in this data set so that's a lot I mean<br>humans recognized over there ten<br>thousand objects or so but a thousand is<br>within the ballpark and there's a lot of<br>images so 1.2 million images in the<br>original<br>set now it's up to I don't know 10 or 14<br>million okay so there's a lot of data um<br>it's a lot of people were working on and<br>they have competitions right every year<br>you'd have a competition and how well<br>can you get your computer vision<br>algorithm to work so here's the results<br>from 2011 to 2017 so 2011 things were<br>working kind of okay and there's an<br>error rate of about 25% and these over<br>these categories down here this dotted<br>line dashed line is human level<br>performance okay so we get about 5<br>percent wrong according to the labels in<br>2012 so most of the most of the<br>algorithms were again 25 percent or so<br>or worse except for this one outlier<br>which is called Alex net Alex was a PhD<br>student of mine he used a neural network<br>to and as of a special structure<br>admittedly but nevertheless it was a<br>neural network and he made use of one<br>important thing and that was the<br>increasing power from graphical<br>processing units so playing game people<br>played games and video kept creating<br>better and better graphical processing<br>units so they could make pretty and<br>prettier pictures for you know blowing<br>up demons and stuff and but Alex used<br>this for doing you know Network training<br>from there so this was 2012 so this was<br>a sea change so from here everything is<br>in neural networks absolutely everything<br>including computer vision there the<br>computer vision conference is now almost<br>everything is neural networks so in<br>2013 everybody started off with Alex net<br>added a few extra things did a bit<br>better 2014 a bit better the best one<br>getting close to human performance 2015<br>better than human performance so this is<br>pretty remarkable<br>I should explain a little bit about what<br>neural networks are they're pretty<br>simple actually in principle they're<br>just computing simple functions that you<br>might be able to compute at the primary<br>school actually we're first sort of<br>quite early on so about 1943 started<br>with analog physical analog circuits but<br>the modern ish artificial neural<br>networks are pretty straightforward so<br>you have some input and you can imagine<br>these are the dimensions from the iris<br>data and they just get fed along into<br>this hidden layer and the hidden layer<br>just a neuron in the hidden layer would<br>just say well I take my weight on this<br>arrow and I multiply it by my input and<br>I sum it up to the weight on any other<br>arrow coming in to me multiplied by that<br>well that's input multiply sum and I get<br>another number out and I passed that<br>number forward and that gets repeated<br>for however many layers you got and this<br>is a sort of loose analogy to how our<br>brains work it's a pretty loose analogy<br>our brains don't really work this way<br>but they're kind of almost there's<br>another complication here in this hidden<br>layer and these we have to apply some<br>non-linearity but I'm not gonna to get<br>any extra power but I'm not gonna really<br>talk about that the difficult part is<br>how do we specify what those weights are<br>and we use a learning method called back<br>propagation and that was invented in<br>1960 and then invented again in 1961<br>and in 1962 and in 1969 and in 1960 74<br>and it really took off in 1985 which was<br>the last time it was invented and this<br>produced the explosion in neural network<br>research and also the hype cycle so this<br>is part of the hype cycle that created<br>the second AI winter that started them<br>about 1988 oh we went to so what about<br>how they see so remember how I said that<br>it wouldn't make sense to just use the<br>raw input of the image I mean that's<br>just a dumb thing to do because surely<br>we can do better than that<br>well actually it turns out that the<br>best-performing<br>neural networks just do exactly that<br>they take the raw input so they actually<br>just take the images and this is<br>something that so deep learning which<br>I'll talk about in a second is something<br>that is very hot at the moment I get<br>people from and anybody who's doing any<br>machine learning get people from other<br>areas coming and saying would like to<br>apply deep learning to problem X I say<br>right so give me the raw data no it's<br>gonna work much better if we do it this<br>way where we extract something something<br>something something and it usually takes<br>a few years to convince people that<br>actually just the raw data makes it work<br>better not more understandable I should<br>point out but it works better in terms<br>of the classification resolved so if we<br>were doing if we're doing recognition<br>from an image within your network we<br>just have a bunch of these neurons that<br>sort of take the input if we've got a 20<br>by 20 image like this that's 400 input<br>neurons this gets fed into a hidden<br>layer they compute some function over<br>all those input numbers just multiplying<br>and adding and then do<br>some simple veneer that gets passed on<br>and maybe at the end at the output you<br>have the three things that you want to<br>classify so you have a neuron for Soto<br>sir a neuron for versicolor and in Iran<br>for virginica and you say okay well<br>whichever one gets the biggest number<br>we're going to call this thing that okay<br>so that would be that would be a shallow<br>Network a deep network is just has<br>multiple of those hidden layers so we<br>just have errors you might say why would<br>you bother doing that well we're not<br>quite sure it works better Lac is here<br>somewhere I think and I have done a<br>little bit of work and we've discovered<br>some things that the the deep networks<br>are more efficient at approximating<br>certain functions than shallow networks<br>are and other people have done work in<br>this area as well and so the the<br>theoretic firry seems to be pointing<br>towards deep networks are inherently<br>better at least no worse than shallow<br>ones but there's still a lot of open<br>questions so they take this raw input<br>but they actually learn interesting<br>things so they learn a representation of<br>what's important in in an image and this<br>gets shared across all four classes and<br>so this is from quite an influential<br>paper from 2014 you don't mind um this<br>is the first layer of first hidden layer<br>of a specific neural network and it's<br>this is the filters this is what they're<br>computing over the image kind of so you<br>can think of these as like this thing is<br>like computing an edge this is computing<br>a difference between colors just in a<br>local neighborhood in the image and this<br>gets passed to a second layer and then<br>we formed more complicated patterns so<br>we get like gratings that these neurons<br>respond to down to the the third layer<br>we get these this looks like people<br>looks like buildings and here at the<br>fourth layer we've got several dog<br>detectors all right so these are all<br>different angles and different faces of<br>dogs that neurons are the neurons in the<br>artificial network are responding to so<br>if you've got a picture with a dog in it<br>that neuron will spike and fire and so<br>this helps for for recognition so that<br>all sounds great<br>neural networks sounds fantastic<br>unfortunately your networks suck they<br>have so many parameters you can't you<br>just you get enough space they give you<br>enough rope to shoot yourself in the<br>foot the heart and head all at the same<br>time so here's the spiral problem so<br>these are solutions that actually neural<br>networks have produced different<br>architectures now I didn't try extremely<br>I mean this is a relatively simple<br>problem really the last ones pretty good<br>but there's so many parameters that can<br>be tweaked and if your thing isn't<br>working you don't know which ones to<br>tweak to make it work so that the<br>difficulty with neural networks is<br>actually developing this intuition is<br>engineering really it's how to engineer<br>and your network to make it work and<br>that's quite that takes quite a long<br>time to develop that intuition I do not<br>have it I do not want to acquire it<br>because I think that's boring but my<br>students come and they love doing this<br>stuff or that's what they want to do<br>because it's the only game in town at<br>the moment neural networks and so<br>students calm we've done medical imaging<br>we've done<br>David might even be here see here is<br>over there<br>measuring anxiety from eg tried to<br>identify had pictures of endangered<br>species I've even done a little bit are<br>you gonna get some sound voiceover but<br>she's worried about learning stuff very<br>interesting we can Craig might be here<br>as well we can learn musics to learn how<br>to play simple games these are Atari<br>games is pretty hot at the moment people<br>love using neural networks to learn how<br>to play Atari games they all can learn<br>better to do better than humans can<br>it's pretty useful we can learn hi toe<br>might be here as well we can learn how<br>to navigate and three-dimensional mazes<br>the interesting thing about these last<br>two examples is that these specific<br>learning systems are self learning so<br>they kind of teach themselves a little<br>bit like how we do and I guess that's<br>where the field is going so in 2019<br>Brendon becomes a professor AI is at a<br>I'm a little bit worried because there<br>might be a third AI winter but maybe not<br>I want to finish off just the last two<br>things first off you know we as<br>academics we do research and we're more<br>or less successful at it but if you<br>think about the whole structure of<br>research the research problem there are<br>many many people doing research in that<br>very if you take one of those people out<br>it makes hardly any difference before<br>almost everybody makes hardly any<br>difference to the the trajectory of<br>research but where we do make a<br>difference is with our students because<br>we spend a lot of time with students if<br>you think about like for me personally<br>this year in undergraduate classes I had<br>300 students in two different papers<br>they may not all have seen me lecture<br>and some of them may not have seen me<br>very much at all but nevertheless<br>there's a lot of them but also post<br>grads so here are all the poster I think<br>I've got everybody all the post grads<br>who I've had some supervisory effort<br>with either as a primary or as a<br>secondary supervisor so there's quite a<br>lot they've gotten out to do great<br>things in the world um the three at the<br>bottom kind of notable because they've<br>sort of moved as in two colleagues which<br>is the best I suppose but everybody else<br>has gone on to do productive stuff out<br>in the real world actually so it was a<br>big thanks to all my students most of<br>the work so you get you know you're<br>getting to be a senior academic when you<br>get asked to do a talk and the only<br>thing you can talk about is the stuff<br>that your students have done because you<br>don't have any time to do any of that<br>anymore<br>but also there are colleagues and so<br>these are most of the<br>of co-published things with and you<br>can't do research unless you're in some<br>community and the community is important<br>so thank you to all my colleagues as<br>I'm gonna say a couple words first boy I<br>feel a little bit funny here because of<br>course Brendan was my head of department<br>before I became a professor couple<br>things Brendan subjects but I mentioned<br>you're slowly falling asleep in seminars<br>and that I mentioned that's right boss<br>was the stains for this book York<br>Department of the most famous Dan Scott<br>and one of the things he was most famous<br>work is hopefully he was within five<br>minutes the beginning of any seminar<br>he's asleep but training but transitive<br>to presence<br>you shouldn't I think towards the end<br>they're receiving some of the ways in<br>which these systems might might be<br>forwards into them the title of the talk<br>protects remain<br>caught the reference to as well it's<br>been notable in the last few years that<br>Brendan stuff is really interested in<br>swimming and swimming long distances and</p></main><footer style="margin-top: 2rem; background: #0001; padding: 2rem; text-align: center;"><p>We Are The University</p><ul style="list-style-type: none; padding: 0; margin: 0;"><li><a href="/">Home</a></li><li><a href="/about">About</a></li><li><a href="/contact">Contact</a></li></ul></footer></body></html>