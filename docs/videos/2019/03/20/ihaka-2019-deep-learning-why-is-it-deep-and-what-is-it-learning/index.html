<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><title>We Are The University</title><link rel="stylesheet" href="/styles.css"></head><body><header><h1 style="color: #fff;font-family: 'Arial Black', Gadget, sans-serif;font-style: italic;font-weight: 900;text-transform: uppercase;">We Are The University    </h1><nav><ul><li><a href="/">Home</a></li><li><a href="/about">About</a></li><li><a href="/contact">Contact</a></li><li><a href="/blog">Blog</a></li><li><a href="/videos">Videos</a></li><li><a href="/authors">Authors</a></li></ul></nav></header><main><h2 style="text-align: center;">Ihaka 2019: Deep learning: why is it deep, and what is it learning? [1:01:17]</h2><p style="text-align: center;"><a href="https://www.youtube.com/watch?v=Pu479rErYlQ" target="_blank">Watch on Youtube</a></p><p style="text-align: center;"><a href="https://www.youtube.com/channel/UCUKg41qkUTUQXGDzklgpmlQ" target="_blank">University of Auckland | Waipapa Taumata Rau</a></p><img src="https://i.ytimg.com/vi_webp/Pu479rErYlQ/maxresdefault.webp" alt="Thumbnail for video titled: Ihaka 2019: Deep learning: why is it deep, and what is it learning?" style="width: 100%;"><div class="tags"><span style="background: #0003; border-radius: 0.3em; padding: 0.3em; display: inline-block; margin: 0.2em; font-size: 0.8em">#The University of Auckland</span><span style="background: #0003; border-radius: 0.3em; padding: 0.3em; display: inline-block; margin: 0.2em; font-size: 0.8em">#University of Auckland</span><span style="background: #0003; border-radius: 0.3em; padding: 0.3em; display: inline-block; margin: 0.2em; font-size: 0.8em">#UOA</span><span style="background: #0003; border-radius: 0.3em; padding: 0.3em; display: inline-block; margin: 0.2em; font-size: 0.8em">#Auckland University</span><span style="background: #0003; border-radius: 0.3em; padding: 0.3em; display: inline-block; margin: 0.2em; font-size: 0.8em">#Auckland</span><span style="background: #0003; border-radius: 0.3em; padding: 0.3em; display: inline-block; margin: 0.2em; font-size: 0.8em">#New Zealand</span><span style="background: #0003; border-radius: 0.3em; padding: 0.3em; display: inline-block; margin: 0.2em; font-size: 0.8em">#University</span><span style="background: #0003; border-radius: 0.3em; padding: 0.3em; display: inline-block; margin: 0.2em; font-size: 0.8em">#Statistics</span></div><h2>Description</h2><p>Professor Thomas Lumley<br>University of Auckland<br><br>Even ten years ago, neural networks did not have particularly impressive performance as classifiers. Statisticians regarded them as just one of many black-box approaches to prediction: a relatively unattractive one because of their computational requirements and their opacity.  Something changed: deep learning is not only trendy, but genuinely superior to older approaches for image classification and generation, and for some other problems.  I will talk about how deep convolutional nets are structured and give some intuition for how they can be effective, but also why they are brittle and can fail in remarkably alien ways. <br>    <br>Thomas Lumley is Professor of Biostatistics at the University of Auckland and a member of the R Core Team. He is a user and teacher of machine learning, rather than a researcher, and has an interest in the public understanding and social impact of statistics.</p><h2>Transcript</h2><p style="opacity: 0.9; font-size: 0.8em">Transcripts may be automatically generated and may not be 100% accurate.</p><p>tonight we are fortunate and grateful to<br>have Tomas speaking to us the original<br>speaker JJ had to cancel unfortunately<br>so a pretty short notice<br>Thomas kindly stepped into the breach so<br>tonight I speaker Thomas something he's<br>been a professor of biostatistics here<br>at Auckland for eight and a half years<br>before that he was at University of<br>Washington in Seattle didn't tell me<br>what he was doing there but I'm sure<br>he's keeping himself busy and he his<br>work has been recognized by being elect<br>as a fellow of the American Statistical<br>Association and of the museum War<br>Society<br>he has very wide range of interests and<br>expertise we used to have a minister<br>Steven Joyce who they called the<br>minister of everything because he was<br>into so much stuff and I reckon we<br>should call Thomas the professor of<br>everything because he's he knows so much<br>about so many things and tonight he's<br>going to talk about and tell us about<br>neural networks he's welcome Chris<br>Thomas Longley<br>you're a Tartar assalamu alaikum today<br>i'm going to talk about how deep your<br>networks work and don't work in<br>primarily an image recognition a lot of<br>statisticians have missed out on the big<br>changes in neural nets in the last<br>decade and it really is the vast decade<br>this is new Google Pagerank<br>didn't use neural networks the Amazon<br>recommender from the early Amazon Hong<br>Kong didn't use neural networks the<br>Netflix prize candidates didn't use<br>neural networks it's a they only started<br>working very recently and a lot of<br>statisticians haven't caught up on the<br>big changes so this is a system called<br>visual dialogue you can go to that<br>website upload a picture and have a<br>conversation with the visual chat bot<br>about the picture so this example is<br>from Janelle Shane who is a photonics<br>research of it also has a large sideline<br>so the duck rabbit picture and you<br>upload it and the visual chat bot says a<br>close-up of a black and white bird<br>that's really impressive if you think<br>about it from the point of view of how<br>would you build a regression model that<br>could do that and you can ask more<br>questions do you see any rabbits yes how<br>many rabbits - so now you know maybe<br>or more straightforward pictures what is<br>it<br>cat what is it doing it's just sitting<br>there I am that's like a really<br>impressive for a computer to be able to<br>so she has this as a sort of example<br>that whenever you're seen a really<br>impressive and successful example of<br>image recognition or you know image<br>processing deep learning a good question<br>to sort of think of asking yourself you<br>know if you could ask this algorithm how<br>many giraffes are there in that picture<br>so this raises two sort of questions how<br>is it so good and how is it so bad and<br>I'm going to try and explain a little<br>bit of both of these I mean to some<br>extent we don't really understand why<br>deep neural networks are so good we've<br>got some ideas but we don't really<br>understand and we don't completely<br>understand why they're so bad though<br>again we've got some ideas this is a<br>quote from a textbook by the leading<br>group of statistical machine learning<br>researchers Rob tips Ronnie will be<br>giving the last of these years he hacker<br>lectures and they say that this is from<br>their introduction to their chapter on<br>your networks in their book on<br>statistical learning they say we're<br>going to describe the most widely used<br>vanilla neural net sometimes called the<br>single hidden layer back propagation<br>network or single layer perceptron and<br>there's been a great deal of hype<br>surrounding these making them seem<br>magical and mysterious<br>they're just nonlinear statistical<br>models I think the word just is doing a<br>lot of work in that sentence<br>but the other thing about that sentence<br>is the date it was written four years<br>later that sentence would have been<br>written differently I think so this is<br>from the submission description for Alex<br>net so there's an international<br>large-scale visual recognition challenge<br>that's run every year where you get in<br>2000 in the early years like this the<br>test set was a hundred and fifty<br>thousand photographs who around the web<br>labelled with presence or absence of a<br>thousand categories of thing in the<br>image and this includes reasonably broad<br>categories like car and reasonably<br>narrow categories like Beth account they<br>had a thing for dog breeds because<br>there's quite good training data out<br>there people take pictures of dogs and<br>they will hook with them with the breed<br>and they you the sort of people who do<br>that are usually right about the breed<br>so they and then you're the algorithm<br>was allowed five guesses and you got<br>scored by what proportion of the images<br>one of those five guesses was right Alex<br>net con 85% accuracy the runner-up and<br>the sort of previous top of the list<br>standard was ten percentage points worse<br>so this was enough of a breakthrough<br>that everybody noticed it's not that<br>this was the first neural network to use<br>these techniques or anything but it was<br>a breakthrough in a big public<br>competition so that you know even the<br>economists noticed and so in that sense<br>it is does correspond to the rise of the<br>machines so the what I'm hope from the<br>first part of this lecture is that<br>you'll understand what more of the words<br>in this description mean so it's a large<br>deep convolutional neural network 60<br>parameters it consists of five<br>convolutional layers some of which are<br>followed by max falling layers and three<br>globally connected layers with a final<br>thousand way softmax it was trained on<br>two NVIDIA GPUs for about a week and<br>then other tech stuff take-take-take<br>pick so I'm going to explain some of<br>those words and how old they help the<br>team there Geoffrey Hinton was one of<br>the people who believed in neural<br>networks for the did through the decades<br>when they didn't work really very well<br>he was at that time at the University of<br>Toronto this is a neuron the sort of<br>nucleus with other thing in the middle<br>it's got it's sticking out where signals<br>come in and it's got one bit sticking<br>out where signals go out and the bit<br>where the signals go out is either on or<br>off it has zero one and has binary<br>activation so the idea is we should be<br>and it sort of looks like that some<br>people think that you can't abstract it<br>to a computer that the sort of<br>biological stuff is important and<br>magical and mysterious and so on Roger<br>Penrose thinks that and this so it's a<br>point of view that's worth considering<br>because Roger Penrose thinks it I'm not<br>sure personally that I think it was<br>worth considering otherwise but we in<br>any case this these are the sorts of<br>things that deep learning neural<br>networks work with it whether or not<br>they do the same things as the ones in<br>your brain the function in the middle is<br>was in early days a logistic function<br>nowadays it's that sort of broken line<br>function that was the non saturating<br>neurons in the jargon oh yeah so what we<br>have is a straightforward generalized<br>linear model or non linear model so one<br>of the problems though is if you've got<br>a model like this the output is monotone<br>in all the inputs so if you increase X<br>and y turns on then if you keep<br>increasing X more Y won't ever turn off<br>again and that's not a good feature for<br>a lot of complicated tasks in statistics<br>we traditionally get around this by<br>transforming the data we put in we<br>multiply X's together to make<br>interactions or we put in<br>transformations of the X's or we put in<br>things like spines or whatever to allow<br>flexible transformations the trendy word<br>for this nowadays is feature engineering<br>and the idea of multi of neural networks<br>is that you might be able to get<br>automatic feature engineering by getting<br>earlier regression models earlier<br>the idea goes a long way back this is<br>the mark 1 perceptron at Cornell it had<br>400 pixel sensor 400 photo cells<br>randomly connected to neurons the<br>weights the coefficients in that<br>regression model were encoded in the<br>settings of resist tunable resistors<br>potentiometers those things there on the<br>right and the updates were done by<br>electric motors that twiddled the<br>potentiometers was very 1950s but this<br>is the basic structure that you've got<br>input things that have photo cells<br>you've got a layer in the middle and<br>then you've got output units and so<br>you've got two sets of tunable weights<br>there's the weights going from the input<br>to the Association and from the<br>Association to the output and the idea<br>was that you could this thing could<br>learn what the right ways of doing the<br>now mostly image recognition was the<br>idea but you know that what hasty and<br>deutronium Friedman said about hype it<br>has been around for a while the embryo<br>of an electronic computer that the Navy<br>expects will be able to walk talk see<br>write reproduce itself and be conscious<br>of its existence and yeah right so there<br>has actually been a lot of that in the<br>in the neural network history of neural<br>networks both that's the sort of<br>ridiculous hype and also under<br>estimating how hard vision was there is<br>a rumor I haven't don't know if it's<br>true or not but it's so often cited that<br>computer vision was assigned as a summer<br>project once back in the early days so<br>if you've got that structure where<br>you've got one intermediate layer of<br>regression models plugged into the input<br>and the output that is you can prove<br>this is a universal approximant it can<br>match any function of the inputs<br>arbitrarily closely given enough things<br>in the hidden layer and you can also<br>show it has also been shown that in<br>general finding the best weights the<br>best coefficients for the single hidden<br>layer model is an np-hard problem so<br>both of those would seem to argue that<br>superficially anyway it turns out that<br>being a universal approximant is less<br>impressive than it sounds that and $4<br>will get your cup of coffee and while<br>you'd guess that having more layers<br>would make the optimization harder it<br>actually doesn't basically because<br>you've got enough room for things to get<br>out of each other's way and so you don't<br>get you're less there's less opportunity<br>so there is actually some benefit to<br>going for deeper than one hidden layer<br>that people didn't do it for a long time<br>partly because people didn't think it<br>would be useful and partly because it<br>does genuinely require more data and<br>more computing but these are the ones<br>that statisticians have been learning<br>about for a long time and being sort of<br>unimpressed with I mean they work sort<br>of but you know if you compare them to<br>random forests or boosted trees or<br>okay so this is a the sort of ecoli of<br>machine learning the amnesty digits<br>problem 60000 training images and 10,000<br>test images of handwritten digits half<br>of them written by US Census employees<br>and half of them written by US primary<br>school children initially that though<br>that's how the training test split went<br>you can see that was probably not a good<br>idea so now the training tests split has<br>been you know stratified so you have<br>some of each in both groups and you can<br>and the idea is that the input is this<br>784 element vector and the output is a<br>number from 0 to 9 and there's two sorts<br>of problems here there's the problem of<br>recognizing the digits but on top of<br>that some of the digits have more than<br>one form so fours for example there are<br>two sort of perfect complete platonic<br>fours open at the top and closed at the<br>top there are two or possibly three<br>platonic sevens so and also a lot of<br>people don't write very well<br>so these are a random sample if you look<br>for the worst ones they're worse the<br>human error rate on this data set is a<br>few tenths of a percent so you could<br>imagine that a multi-layer perceptron a<br>multi-layer neural network could could<br>be useful if you set up the first layer<br>so it detects edges and arcs and you set<br>up the second layer so it detects limes<br>and loops and corners and things like<br>that and you then set up the third layer<br>so it says well if you've got a loop<br>near the top and you've got a line on<br>the right then it's a nine so there's a<br>good story about how a three layer or a<br>neural network good work yeah and it's<br>not completely unlike how the visual<br>cortex actually works that you have<br>layers of cells and the earlier ones<br>detect simpler things on the later ones<br>to take more complicated things it's not<br>as simple as this and the neurons aren't<br>as simple as these neurons but you know<br>it's enough that it makes a good story<br>for why these might be a good thing to<br>try since we have an obligatory a studio<br>Kerris this is what JJ would have been<br>showing you so this is code to produce a<br>multi-layer neural network multi-layer<br>perceptron for these data you can see<br>that if you speak R this is quite easy<br>to understand if you speak Python you'd<br>probably prefer the native Kerris but<br>you know statisticians are more likely<br>to speak our we've got an initial layer<br>with 256 units in it there are one with<br>48 then one with 16 then the output<br>layer has 10 the activation is the<br>function that transforms in the middle<br>of each neuron and most of them are that<br>restricted linear unit the thing that's<br>two straight lines the one at the ends<br>has to pick a number then we compile it<br>the the computations are not running are<br>they're not even running Python I mean<br>they're compiled to run on whatever<br>hardware you have and then we train it<br>and it runs for a little while and we<br>end up with ninety seven point two<br>percent accuracy and that's you know not<br>terrible<br>it's about comparable to what you get<br>from random forests so how do we do this<br>training the sort of standard way<br>statisticians optimize things is the<br>newton-raphson algorithm the second<br>derivative you've got too many<br>parameters here so you've got too many<br>choose two elements of the second<br>derivative and so we're not doing that<br>what we do is a really simple algorithm<br>called gradient descent where you take<br>the derivative of the error with respect<br>to the weights and subtract a small<br>multiple of that off the weights in fact<br>we don't even do that we do that for<br>small batches of data because that way<br>you can update the weights part of the<br>way through rather than waiting until<br>the end of a batch before you update<br>them we need to be able to work out<br>those derivatives well there's the chain<br>rule the derivatives of say the arrow<br>with respect to the first layer well<br>you've got error with respect to third<br>layer third layer with respect to second<br>and so on all the way back so you can<br>easily it's not just that you can easily<br>compute these derivatives you can easily<br>write a program that will compute these<br>derivatives you can compile write a<br>program that will compile the<br>description of the network to that<br>string of derivatives it's the simplest<br>case of automatic differentiation and<br>it's even simpler here if you're using<br>those restricted linear units then the<br>output is either 0 or linear in the<br>input and so the derivative with either<br>0 or the derivative with respect to the<br>weight is X or the derivative with<br>respect with respect to X is the weights<br>so<br>it's just a matter of moving things<br>around so we have tensors the things you<br>do that might be two-dimensional like<br>matrices or might have more or fewer<br>dimensions and we have flow the<br>computations mostly involve moving the<br>same thing around very fast and so<br>that's why the Google tends to flow<br>library is called that and is useful for<br>deep learning so we might think then<br>that what we're finding is the deeper is<br>better and that's true up to a point but<br>layers that are further from the output<br>are exponentially slower to Train<br>because you have to feed through so many<br>more steps of the chain rule before you<br>get to the those weights there are<br>ridiculous levels of overfitting as so<br>the alux net network has got 60 billion<br>parameters it doesn't have 60 million<br>training images not even close and you<br>can say that okay the first layer learns<br>to detect edges but there's a lot of<br>neurons in the first layer and learning<br>to take edges 28 times 28 times is bad<br>enough but once you get to bigger images<br>that's just not a good use of anyone's<br>time<br>so while deeper is better there are<br>problems with doing that so we have the<br>idea of convolutional layers where you<br>don't why an ER on up to the whole<br>previous layer if you've got an image<br>you wire it up to just the bits nearby<br>in the previous layer it's where the<br>relevant information is probably going<br>and you don't and you use the same<br>weights for every node in the lair so<br>they're not weights for the node Atlit<br>they're not weights for the input at you<br>know three across and seven down they're<br>weights at the input three across and<br>seven down from this pixel so you've got<br>one set of weights you sort of slide it<br>over the whole image so a layer of nodes<br>now recognizes a single feature<br>something like an edge and you can<br>afford to use a lot of them in parallel<br>to recognize a lot of features we've cut<br>down the number of parameters you need<br>for a layer so far that we can now have<br>a lot of layers this is the first level<br>of alex net<br>it's got 11 by 11 pixel patches that<br>with 96 of them so each of these is a<br>picture of a set of weights for an 11 by<br>11 patch and they start off being mostly<br>monochrome the first few over the last<br>few are mostly colored they've probably<br>been sorted in some way to make them<br>look pretty and you can see that these<br>are plausible as features that they're<br>plausible for detecting edges or stripe<br>enos or various sorts of textures or<br>different sorts of color contrasts so if<br>you had to design basic features that<br>you wanted to pick up anywhere on an<br>and because at each one of these has<br>only got you know 121 parameters instead<br>pooling there was this thing about<br>pooling the prop you still if you've got<br>these convolutional layers information<br>can't spread very far so in one sense<br>yes the information that this is a cat I<br>mean there's a lot of local information<br>that tells you that this is a tale or a<br>claw or whatever but at some point you<br>need to know that it's got claws and a<br>tail and so information does need to<br>spread pooling lets you spread<br>information sort of semi locally you<br>take you say rather than saying is there<br>a edge at precisely this point you say<br>is there an age anywhere in this small<br>region so you take the maximum output<br>from a set of neurons so that spreads<br>information a bit faster spatially and<br>it doesn't take any extra parameters so<br>it's free from the point overfitting<br>point of view that's still too many<br>parameters you know there's a sort of<br>rule of thumb you shouldn't have too<br>so there's a very brutal so in<br>statistics we think of regularization<br>with you know l1 or l2 penalties you<br>know nice things differentiable things<br>like that there's a very simple and<br>brutal approach that they use in deep<br>neural networks which is just to<br>randomly leave out some of the<br>parameters when they're doing the<br>computation so each time you run a<br>sample through to compute its<br>contribution to the gradient you just<br>decide not to use some of the<br>connections randomly with each each time<br>and that does give you regularization<br>because the output of a node can't rely<br>on just a few of the inputs because<br>those in those connections might not be<br>there so it has to rely on quite a lot<br>as you have to get sort of smooth<br>the spatial patterns that get picked up<br>and it turns out that it works and it's<br>also very cheap to do it actually speeds<br>up the computation especially if you do<br>it for later layers because you then<br>don't have to do all the work once you<br>know that this chain rule product thing<br>is zero you don't need to evaluate the<br>earlier bits in it so it's cheaper than<br>free and reasonably effective and if we<br>take a toy example well it's not a toy<br>example it wasn't a toy example<br>originally it's a toy example now of the<br>digits we can say okay give me a<br>convolutional layer with 32 features in<br>it<br>give me then feed that into another<br>convolutional layer with 64 features in<br>it then in pooling and some drop out and<br>then feed it into a couple of dense<br>layers that are completely connected<br>that's a much more complicated model it<br>doesn't have noted it's probably got<br>fewer parameters though than the earlier<br>one we looked at and it gives much<br>better accuracy so the error rate is<br>just over 1% it takes about 15 minutes<br>to fit which is kind of annoying but it<br>does actually give you better accuracy<br>and things that take one minute to fit<br>and it takes quite a long time to write<br>60 thousands handwritten digits down so<br>Brian ripped one of the things that<br>Brian Ripley said when I was taking a<br>class in statistical computing from him<br>was that you know one of the difficult<br>things in the in modern statistics this<br>was in the 1990s was to find useful ways<br>now this isn't that much better than<br>standard method you know if you looked<br>at boosted trees or something you can do<br>almost as well as this it's not sort of<br>clearly superior because the problems<br>too easy<br>and if you make it bigger and bigger<br>then you and get more and more data then<br>you can come up with you can see how you<br>could expand this to come up with Aleks<br>net and that takes you know a week of<br>computation on two GPUs is a lot and you<br>couldn't 20 years ago nobody could do<br>that sort of computing nowadays of<br>course if you're you know deep mind or<br>Google or Microsoft that's peanuts this<br>is the structure of Google's version<br>three inception image recognition<br>network all those little colored blobs<br>are layers the yellowish colored blobs<br>the convolutional layers and there's<br>various other sorts of layers in there<br>and then the red ones at the end of the<br>output you can see there are things<br>where think we're like we're partial<br>computations can skip a layer what<br>you're doing there is allowing earlier<br>things to get passed through so<br>effectively you can work on computing<br>the residual rather than the function<br>itself if case that's easier so that's I<br>haven't counted the letters here but<br>there's quite a lot of them so this is<br>what this is deep learning if someone<br>can do the James Earl Jones voice and<br>it's improves over the previous networks<br>by in their one of their test examples<br>for example being a lot better at small<br>differences in dog breeds so it can tell<br>you that if the breeds of those two one<br>of them's a Malamute and the other ones<br>and again we have a sort of this wise<br>linear regression model that can<br>so what's happened and what's happened<br>here is that these people have devised a<br>small perturbation that you can apply to<br>images<br>they'd have designed it for a particular<br>neuron no image recognition neural<br>network and if you apply that to all the<br>images you can see it doesn't make much<br>difference that three-toed still looks<br>like a camel and the anima course still<br>looks like a coffee pot so it doesn't<br>it's almost undetectable to humans but<br>for the network it's been designed for<br>it can give you an error rate of 80 to<br>90 percent and some even more<br>impressively for another neural network<br>that it wasn't designed for it can still<br>some of that is because they're<br>basically all the same underneath but<br>they're not that much all the same<br>underneath it's still actually<br>reasonably impressive and the problem is<br>that space is big n data points can only<br>really fill up log n dimensions and so<br>these neural networks must have some<br>very low dimensional structure relative<br>to the space they're in and that means<br>that if you could find perturbations<br>that go sort of across that low<br>dimensional structure they're going to<br>get added up through the neural network<br>and they're going to do horrible things<br>to the prediction accuracy while not<br>affecting the prediction accuracy of<br>people and you can then go and engineer<br>perturbations that do that and then<br>apply them to other neural networks as<br>well and see that they also are fairly<br>bad there so these are sort of optical<br>illusions for deep convolutional<br>networks incidentally these also show<br>that deep convolutional networks don't<br>actually recognize images the way the<br>human brain does because that does not<br>statistical myth is that you can't be<br>nonparametric in high dimensions space<br>that's one of the problems why neural<br>networks do so badly in some cases<br>another problem was actually noticed by<br>philosophers quite a long time ago quite<br>as gavagai problem and if you think<br>about this picture of a hillside with<br>sheep or giraffes the neural network<br>doesn't know that sheep is supposed to<br>be a type of animal for all it knows<br>sheep could be a type of geography<br>topography or it could be a type of<br>weather or it could be a type of farm or<br>whatever and so if it sees a lot of<br>pictures like this that are labeled<br>sheep then it says okay well this is<br>what humans humans say sheep when they<br>see this sort of em when they take<br>pictures of this sort of hillside and<br>it's just that a lot of the time when<br>people are taking pictures of sheep its<br>sheep on this sort of hillside not sheep<br>in the sort of Australian outback so<br>it's got a hard time because it doesn't<br>know that sheep are supposed to be the<br>animals rather than the hillsides and<br>there's a more serious so that's a sort<br>of basic training data problem there's a<br>more extreme training data problem with<br>a lot of image analysis settings this<br>was an Android car pothi is a academic<br>at the moment and he a neural network<br>researcher and he had this example he<br>writes a blog about neural network<br>things and example looking for best<br>selfies he started off with a<br>out-of-the-box image recognition network<br>I can't remember which one it was<br>trained it on 2 million selfies from<br>Twitter where the sort of quality score<br>for each one was how many likes it got<br>normalized to some extent by how many<br>views it would have had then he took a<br>50,000 image test set that the neural<br>network had never seen before<br>and this is the top hundred of that<br>50,000<br>as a representative sample of humanity<br>there are some noteworthy things about<br>this group of people there are some male<br>selfies that are further down than 100<br>in the top thousand I don't there I<br>think there's one or two who there's a<br>there's a few east asian people and one<br>or two south asian people but basically<br>it's white women with long hair young<br>white women with log here because that's<br>what the training data says and<br>the what all the neural network will do<br>is learn to match what you told it was<br>the right answer and it can be very<br>creative in doing that and so you have<br>obvious problems with bias training data<br>even quite subtly so there have been<br>problems with medical x-ray training<br>data looking for duygu getting a neural<br>network to diagnose pneumonia where it<br>noticed s-- you there some of the images<br>are a bit different because they were<br>taken on a portable x-ray machine and<br>those were much more likely to be cases<br>of pneumonia because you use the<br>portable machine when the patient is too<br>sick to get up and go to the x-ray<br>department so there are things like that<br>neural networks are perfectly happy to<br>cheat so firstly they'll believe<br>whatever you tell them about what you<br>want if you tell them that this is the<br>sort of thing that counts as a good<br>selfie they'll believe you<br>if they tell them this is what pneumonia<br>looks like they'll tell you and they<br>will cheat and so there's a real problem<br>with getting most of the really<br>successful neural networks have been in<br>situations where there is a very large<br>quantity of free training data sitting<br>out there on the internet and in some<br>situations that's okay and in other<br>situations it really isn't so in a<br>summary the deep neural networks<br>can take advantage of hierarchical<br>feature structure in images as you've<br>seen they could also do that with<br>language they've they're also they've<br>been very good at language recognition<br>to the extent that people who've been<br>working in computational linguistics for<br>decades are astounded by how good they<br>are and they're especially and<br>especially for problems with high<br>signal-to-noise so it's difficult to get<br>a computer to tell a husky from a<br>Malamute but it's actually not it's a<br>high signal-to-noise problem there's the<br>the distinction is clear it's just not<br>sort of obvious training them<br>efficiently needs a lot of expensive<br>hardware especially if you want<br>something with the you know very very<br>very very deep networks like the Google<br>one so doing research in this sort of<br>thing is now it's something you can only<br>do in a few places because you can't<br>compete if you don't have the hardware<br>they haven't done as well in a lot of<br>other applications in particular they've<br>been fairly disappointing in medical<br>diagnosis now it's possible that that's<br>because the training data is the<br>limitation or it's possible that the<br>structure of the problem just isn't<br>right that image recognition is<br>sufficiently different that it works<br>better they've also done well in<br>situations where you can generate<br>unlimited training data so deepmind<br>who are the people who produce the<br>chess-playing and go playing neural<br>networks the nice thing about games like<br>that is you can generate as much<br>training data as you like by having the<br>computer play against other computers<br>once it gets somewhat good there's no<br>and the training date so training date<br>is a problem you can get around you can<br>do if you want to do image<br>classification for different sets of<br>images so for example if you want to<br>classify infrared videos of different<br>New Zealand predator species then you<br>can start off with the standard Google<br>classification thing and train it on<br>smaller amounts of local data but you<br>still need the data you need data where<br>you've got the image and the true<br>classification and free training data<br>off the internet is pretty reliable for<br>dog breeds there are other situations<br>where it really isn't a good thing to<br>location they continue a question as<br>that's this so I was quite intrigued can<br>I so I was quite intrigued with the<br>1950s machine that you showed us and I<br>also know that the reason why a eye has<br>become popular is because we now have<br>the hardware and now we have the data<br>right but like I'd like to ask what<br>would the next revolution the next step<br>be to bring us into the next age because<br>we had AI winter now we're at AI spring<br>how do we go to the summer so I won are<br>you this as this isn't my research area<br>I mean I teach it's not my research area<br>I mean one thing would be to actually<br>get somewhere with AI I mean this isn't<br>AI this is this is as you can see from<br>the giraffes or whatever this is this<br>doesn't have any representation of the<br>thing of the things and if you could<br>have sisters working with to get there's<br>some work already to get neural networks<br>that have at least nowhere in the image<br>they're looking sort of it and so a<br>system like this but had some out that<br>had some idea what it was doing would be<br>another important step and the other one<br>from a different point of view would be<br>a system that could defend its choices<br>that could explain why it was right<br>because you know if it's just google<br>image search you don't care so much but<br>there's a lot of applications of machine<br>decision-making where you really do want<br>to be able to explain why it shows that<br>and I think those are both hard problems<br>but I don't think they're intrinsically<br>insoluble problems<br>I wanted to us how much of the success<br>of deep learning do you think is because<br>the algorithms do actually work<br>similarly to the brain and how much of<br>their lack of success or limitations do<br>you think has anything to do with their<br>failing to to act like the brain if<br>anything that's I'm not sure so there's<br>the the adversarial perturbations thing<br>shows that there are ways that they<br>don't work like the brain but I mean I<br>could certainly I mean you know it could<br>be true I don't know I mean it's the<br>sort of thing that it's plausible that<br>it that that that's a part of the<br>explanation but I don't know whether<br>it's a little bit of it or a lot of it<br>and the this is a the this is an excerpt<br>for their 1990 short story but they're<br>you know on the question of whether you<br>actually you know is it really plausible<br>to have machines that think as opposed<br>imaginary aliens being horrified at the<br>idea of the things that behave as if<br>they're intelligent that they do it<br>would meet so um neural nets obviously<br>have this huge number of degrees of<br>freedom in terms of the number of<br>weights you could you could you can play<br>with but also this vast space in terms<br>of numbers of layers the the nature of<br>those layers a number of units in the<br>layers so I would imagine there might be<br>some way of optimizing the structure of<br>a neuron there for a particular problem<br>and say taking this an example that<br>Google inception model did you think<br>that was arrived at by trial and error<br>or intuition or do you think that was<br>approached in using some optimization<br>method I think it was a mixture I mean I<br>think it was some of it was expert<br>guesswork and some of it was comparing<br>different structures to see what worked<br>better I think there is interest in the<br>question of how can you get neural<br>networks to design your own networks for<br>you so that you don't have to do at<br>yourself and eventually we're going to<br>be at the point where that's what people<br>with all this computing power will be<br>able to do because they're already<br>training the networks will stop being<br>hard<br>there any risks associated with kind of<br>um saying you gave those examples of<br>where I perform really badly but the<br>training data was kind of separate to<br>what the task was I know there's<br>transfer learning where you kind of<br>refit based on an already trained model<br>have you done any work where say you're<br>trying to predict something and you<br>apply transfer learning and it actually<br>wasn't terribly performing in<br>counter-examples<br>I not personally transfer learning seems<br>to work surprisingly well but the bigger<br>I think a bigger problem is that the if<br>you've got the reason you want to do it<br>is you've got a relatively small<br>training set and if you've got a<br>relatively small training set then it<br>may be badly unrepresentative in various<br>ways as well you know the same reasons<br>that make it small might also make it<br>unrepresentative and I think that would<br>be a big concern but I don't I mean I've<br>seen impressive examples of it I haven't<br>actually tried it on any substantial<br>problems myself when you love like<br>executes deep neural network it takes a<br>lot of time compared to any other<br>algorithms so at the end of the result<br>you will get to know it is over if it's<br>fitting so do the number of hidden<br>layers affect overfitting or under<br>fitting of model so the number up to a<br>point yes I mean so a lot of if the the<br>Alex net paper is extremely well-written<br>it's something that the that people who<br>are interested in this sort of thing<br>should read and they go one of the<br>things they explain is how you know<br>which that the what sorts of changes why<br>they made these various changes and they<br>introduced really high levels of dropout<br>because they wanted to have the<br>additional layers and the additional<br>layers were causing overfitting and so<br>that's and they found that it was better<br>to have the extra layer with more drop<br>out than to not have it I don't I mean<br>it's not<br>I presume it doesn't go on forever that<br>people have written many people who<br>design these sorts of things have<br>written about how they make those<br>choices and a lot of them have written<br>about it in readily accessible blog<br>posts which is always nice this deep<br>neural network mostly suitable to the<br>image recognition problems since if<br>there are the any of the generic use<br>cases as a sales forecasting it could be<br>the weather forecasting so these DNN how<br>it is going to work so currently the two<br>really successful cases deep neural<br>networks for images and recurrent neural<br>networks are texts general forecasting<br>forecasting they're not terribly good at<br>there's two sort of time series<br>forecasting statistical time series<br>forecasting<br>is one area where statistical methods<br>really do compete well and that's partly<br>because you don't have enough data and<br>partly because things change the other<br>weather forecasting there's enormous<br>power in weather forecasting in knowing<br>of the navier-stokes equations so you<br>know we know the basic physics at the<br>you know that drives weather forecasting<br>and so if you've got extra computing<br>power you're probably better off running<br>the navier-stokes equations at on a<br>finer scale or running a larger set of<br>ensembles or whatever because we know<br>that's how the weather works deep neural<br>networks or a brute force and ignorance<br>approach like a lot of statistical<br>modeling they're there they work in<br>situations where you don't know what's<br>going on in any detail like language I<br>mean we you know the rules based<br>linguistic language passing can't do as<br>well at the moment as the brute force<br>and ignorance approach but in situations<br>where we really know what's going on<br>that's the way to do the computations<br>hi okay so I have no I know nothing<br>about machine learning so just two<br>random questions and so sometimes the<br>machine fails to recognize the pictures<br>but do you think it might be due to<br>because you are training them using 2d<br>pictures so I'm in reality that when we<br>teach the children that is a dog for<br>sweet I mean student can see there's a<br>3d picture of dog so do you think maybe<br>it'd be better if training the machines<br>in like 3d 3d Java might be that I mean<br>the limitation is there isn't very much<br>of it so the thing about 2d photographs<br>is that there are lots of people around<br>the world who for fun go out and take<br>pictures of things and label them with<br>what they're pictures off and that's the<br>thing that's what powers image<br>recognition these things like Flickr<br>this quantity of so I mean so it so it<br>might work better but would have to work<br>much better give them that there's a lot<br>sometimes I mean when you see that the<br>time to Gotland recognize things wrong<br>and can you somehow do you have the<br>mechanisms that somehow tells it it's<br>wrong and what is right so you can if<br>you've got errors I mean the fitting the<br>fitting algorithm basically does that<br>and so if you have a new set of images<br>that have got errors in them you can you<br>could feed them back here in and read<br>and train the computer on those as well<br>that's whether that's worth doing or not<br>would depend on how much extra<br>information you get from it I mean<br>sometimes it's but yes I mean you can<br>you can tell it that you and that's how<br>the fitting out rhythm works and you can<br>do it with new data as well as with all<br>data thank you<br>you said you can't be nonparametric in<br>high dimensions and that sentence was<br>opaque to me Oh is there anything you<br>could tell me that would help me<br>understand what you're saying with that<br>so there's a lot there's some<br>in low dimensions so suppose you've just<br>got a one-dimensional at one dimensional<br>weekly curve you can say okay the value<br>on Monday of the value at you know<br>twelve o'clock on Monday I only want to<br>look at points that are near twelve<br>o'clock on Monday and I'll take some<br>sort of average of those and that all<br>that relies on is knowing which points<br>in year one that it doesn't rely on<br>anything else about the shape of the<br>curve and but it relies on there being<br>some points that are close together and<br>some that are far apart and in one<br>dimension that works in two dimensions<br>that sort of works in a large number of<br>dimensions it doesn't work because there<br>are no points close to each other<br>and that sounds weird but you think of a<br>cube think of a one by one cube then if<br>you split it into cubes of size a half<br>there are eight of them if it were a 10<br>dimensional cube when you split it into<br>cubes of size a half there'd be a<br>thousand and 24 of them and if it was a<br>hundred dimensional there'd be like a<br>bazillion of them and so if you think<br>how many points you don't have points in<br>the same half by a half cubelet and in<br>one dimension you are and in three<br>dimensions you are and in ten dimensions<br>you probably aren't and in a hundred<br>dimensions you definitely aren't so you<br>can't rely on nearby points in high<br>dimensional space because there aren't<br>any and so one of the sort of bizarre<br>but easily provable things is that in<br>high dimensional space points are all<br>approximately the same distance from<br>self-driving cars rely on sort of image<br>recognition at least<br>of objects or or different situations<br>how does that relate to this it's so<br>they've got the same I mean they've got<br>image recognition very much of this sort<br>at one of the they've been training data<br>collections of things like traffic signs<br>to get accuracy and related in picturing<br>them the they're getting video which is<br>requires a lot more data but also<br>probably contains more information and<br>they've got to make their decisions in<br>real time that's not so much of a<br>problem because training and neural<br>network takes forever they're getting<br>predictions out of it's pretty fast so<br>if you think about that hundred lay in<br>your network I showed you that's just<br>each layer is just a linear combination<br>with a threshold and so there's no<br>problem applying that the classifier<br>fast it just takes forever to train so<br>you need all these cars driving around<br>for a long time taking recordings to get<br>you the training data and if you've got<br>good enough training data you then need<br>to spend months fitting your detection<br>algorithms that having the recognition<br>happen in real time isn't the problem<br>the problem is having the recognition<br>particularly for rare things because<br>you're not going to have many examples<br>of your training data or say a child<br>running out onto the road or whatever<br>and that's something you really want to<br>get right there's also there's also the<br>generic problem that people tend to<br>program self-driving cars to follow the<br>road rules and as you'll know as a<br>cyclist if you literally fought if you<br>if everyone literally follows the road<br>rules it doesn't work and it works<br>because car I mean cars manage because<br>people your cars give way to each other<br>more than the road will say they should<br>do they don't give way to bikes more<br>than the road will say they should I<br>mean you'll be aware of this problem so<br>then so that's another issue with<br>self-driving cars but the big problem is<br>rare events I think that you have you<br>need to have seen in law<br>examples of everything that could go<br>wrong off the top of your head would you<br>have any recommended reading like papers<br>or books on networks so I would I mean a<br>lot of books the problem with books is<br>that this is a very rapidly evolving<br>field and so it's hard to find an<br>amazing the books I think a useful for<br>is showing you how to use software but<br>blogs I think are also very useful so<br>paper so conference papers and blogs I<br>mean one of the most useful things I<br>read was the Alex net paper when I was<br>learning about this to teach that's 369<br>but also blog and Ray Kappe nathie's<br>blog is very very good and so a mixture<br>of logs and conference papers or archive<br>papers that I just sort of find by<br>searching or by people mentioning the<br>research in this yeah so the things that<br>recognize speech tend to be recurrent<br>neural networks where the input is the<br>output is ready and again as part of the<br>input and so they have a sort of implied<br>leap structure rather than actual date<br>structure or as well as possibly so they<br>this I mean they look they're similar in<br>some ways and very different in others<br>and that the sheep that's probably a<br>training data issue that the that<br>there's somebody partly that the<br>training data contains a lot of pictures<br>of things that people take pictures of<br>but like giraffes and sheep but also<br>partly with the text generation that if<br>you think about being the visual chat<br>bot nobody in general as someone asks<br>are there any giraffes in this picture<br>the answer is yes because people don't<br>ask that sort of question without<br>pictures that don't have Jerusalem and<br>so that there's again a training data<br>problem but I think there is a there I<br>think that both shape of giraffes are a<br>known weakness of some of these our<br>agents<br>thank you very much could we think to us<br>we're here again same time next week for<br>another speaker and next time we're<br>going to be trying to give away t-shirts<br>like these ones this thing so keep an</p></main><footer style="margin-top: 2rem; background: #0001; padding: 2rem; text-align: center;"><p>We Are The University</p><ul style="list-style-type: none; padding: 0; margin: 0;"><li><a href="/">Home</a></li><li><a href="/about">About</a></li><li><a href="/contact">Contact</a></li></ul></footer></body></html>