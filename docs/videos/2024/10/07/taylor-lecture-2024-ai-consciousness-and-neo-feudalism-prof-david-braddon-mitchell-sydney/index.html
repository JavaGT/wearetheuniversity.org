<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><title>We Are The University</title><link rel="stylesheet" href="/styles.css"></head><body><header><h1 style="color: #fff;font-family: 'Arial Black', Gadget, sans-serif;font-style: italic;font-weight: 900;text-transform: uppercase;">We Are The University    </h1><nav><ul><li><a href="/">Home</a></li><li><a href="/about">About</a></li><li><a href="/contact">Contact</a></li><li><a href="/blog">Blog</a></li><li><a href="/videos">Videos</a></li><li><a href="/authors">Authors</a></li></ul></nav></header><main><h2 style="text-align: center;">Taylor Lecture 2024, 'AI consciousness and neo-feudalism', Prof David Braddon-Mitchell (Sydney) [1:16:25]</h2><p style="text-align: center;"><a href="https://www.youtube.com/watch?v=HdczDwxpX2M" target="_blank">Watch on Youtube</a></p><p style="text-align: center;"><a href="https://www.youtube.com/channel/UCPVjZXotZ5oX9jyXlFFRj2w" target="_blank">University of Otago - Ōtākou Whakaihu Waka</a></p><img src="https://i.ytimg.com/vi/HdczDwxpX2M/maxresdefault.jpg" alt="Thumbnail for video titled: Taylor Lecture 2024, 'AI consciousness and neo-feudalism', Prof David Braddon-Mitchell (Sydney)" style="width: 100%;"><div class="tags"></div><h2>Description</h2><p>Philosophy at Otago is pleased to present the second of two Dan and Gwen Taylor public lectures, this one presented by Professor David Braddon-Mitchell from the University of Sydney. <br><br>David investigates how naturalistic theories of consciousness have been used to argue that Large Language Models (LLMs) are either conscious, or getting close. He'll then address the common response that LLMs are just predicting sentences and are as dumb as a toaster, explaining why this view is mistaken. David then describes how society might function if AI, robots, and factories are all owned by the richest corporations and individuals, a state which seems more and more likely, leading to less utilisation for human resources, the wealthy 'owners' needing only a connection to raw resources, AI producing everything they need. Unless AI is brought into public ownership, 99% of the population might be plunged into unprecedented poverty. <br><br>There is, of course, a hopeful possibility: the post-scarcity society. But achieving this, David argues, will require action starting now to have a decent chance of becoming a reality.</p><h2>Transcript</h2><p style="opacity: 0.9; font-size: 0.8em">Transcripts may be automatically generated and may not be 100% accurate.</p><p>cure everybody Welcome uh on behalf of<br>philosophy at the University of Vago<br>it's my pleasure to welcome you to<br>tonight's Taylor lecture it's our second<br>Taylor lecture in a row and we're very<br>pleased uh tonight to be able to welcome<br>David Bradon Mitchell so just if you<br>don't know the Taylor fund uh was given<br>to us by Dan and Gwen Taylor who both<br>taught here in between 1960 and<br>1982 and it's their very kind gift that<br>has allowed us for over 30 years to<br>bring in uh top-notch luminaries from<br>around the world to do philosophy with<br>us and we're thrilled to have that again<br>tonight so welcome very much uh so<br>tonight we're welcoming David Brad<br>Mitchell who is a professor at the<br>University of Sydney professor of<br>philosophy uh he has held post it in<br>ockland and re was a research fellow at<br>the Australian National University<br>uh David's areas of research are wide<br>but they are mostly in mind and<br>metaphysics uh branching out into<br>philosophy of science philosophy of<br>biology quite a lot of physics from what<br>I can see um he has been the recipient<br>of numerous Arc grants and is the author<br>of the philosophy of mind and cognition<br>co-authored with Frank Jackson Who some<br>of you may have heard of uh he's more<br>recently uh published Armstrong's<br>materialist theory of mind with Oxford<br>University press in<br>2022 and conceptual<br>analysis uh oh I can't read my own<br>handwriting something about<br>natur it's great uh conceptual analysis<br>and philosophical naturalism from MIT in<br>2009 um and his top hit paper which I<br>like very much is called how do we know<br>it is now published in analysis so<br>tonight we're very pleased uh to learn<br>about AI Consciousness and Neo feudalism<br>from David and he'll talk for about 50<br>55 minutes then we'll have some<br>questions and afterward there is a<br>reception which everyone is invited to<br>David so this is a talk with two themes<br>one of them is broadly political<br>philosophy in fact when one of my<br>colleagues at Sydney saw the<br>advertisement she raised up me excitedly<br>and said David you've seen the light<br>you've moved up to political philosophy<br>and the other is kind of more centrally<br>in philosophy of mind the question of<br>Consciousness and even more crucially<br>what it would take to have genuine<br>thought in<br>Ai and I will leave use it aside if I<br>successfully M these things together and<br>bridge the gap I like to think that I<br>have but you'll perhaps take a little<br>it's the 9th<br>century the Carolinian Empire is losing<br>central control of broadly France and<br>areas around<br>it and they're unable to raise enough<br>troops and they're unable to provide<br>enough resources to consistently have<br>the kind of Imperial control that they<br>want so what happens is they start to<br>Outsource to local grandees and the<br>local Grandes exert control over people<br>to grow food and people to be cannon<br>fodder and they connect with the central<br>Empire by the central Empire giving them<br>them support against any local<br>Insurrection such as they are in<br>exchange for providing those troops and<br>providing some<br>resources and so those local grandees<br>this is the main part of the deal really<br>with the central Empire end up owning<br>everything pretty much nobody else does<br>now what are the estimates the estimates<br>are between 1 and 5% of people own<br>essentially everything and everybody<br>else is a kind of villain spelled with<br>an e i n not an A and those people are<br>basically tasked to grow food and be<br>Canon<br>great but there's one critical thing<br>about it which is perhaps not so<br>terrible at least by the standards of<br>the worst things you can imagine and<br>that is although possibly the people who<br>growing that growing that food are in a<br>worse State than peasants who actually<br>own some<br>land they are at least required the<br>local Grande really does need them to<br>grow some<br>food and they really do need excess<br>people to be Canon<br>fod and if you have to grow some food<br>you got to eat some food otherwise you<br>can't grow it<br>and if you have to have enough of a<br>surplus population so as to provide<br>Canon fod then the standard of living<br>has to be above the absolute bare<br>minimum because you've got to be able to<br>have children have a number of children<br>have some of them grow up to grow food<br>and have some of them become Canon<br>history of the Carolinian Empire and its<br>successes in the early Holy Roman Empire<br>you would read a lot of stuff about the<br>complex relation of mutual obligation<br>between the clergy and the villains and<br>the the Lords and all the rest of it so<br>what I've just described is a bit of a<br>caricature but statistically there's<br>something right about it it was<br>depressing and it was<br>bad okay things<br>change<br>so by the perhaps starting in the 1930s<br>perhaps ending in about the<br>1970s if you believe Tomar PTI in his<br>book capital in 21st<br>century eventually inequality of<br>ownership of resources not of income<br>reaches kind of a global minimum around<br>about<br>1970 in the late 19th century that kind<br>of magic 1% story where 1% of people own<br>most of the stuff is applying<br>again but it starts to change for all<br>kinds of cool and complicated reasons<br>that if this was a different talk uh and<br>I knew some more about<br>about but all it matters for us is that<br>it does change and the change appears to<br>be in<br>Reverse Pik estimates that in perhaps 10<br>to 30 years we could be back to that<br>depressing magic 1%<br>figure I don't know whether he's right<br>the arguments are<br>fairly plausible and they're but they're<br>highly<br>contested but a continuous theme of this<br>talk is going to be we don't need to be<br>sure of some of these things we just<br>need to think they're plausible enough<br>for it to matter a lot for what we plan<br>to<br>do so let us suppose then that in<br>perhaps 30 years we're back to that<br>situation where most of the resources<br>are owned by a tiny number of<br>people how much like the feudal past is<br>look so you don't need to be sure p is<br>right to ask that question now what's<br>that got to do with AI this is what it's<br>got to do with<br>AI it seems to me that there's a<br>scenario in which how things are in 10<br>or 20 years that's assuming that we<br>don't have a climate change disaster<br>that's assuming that the the worst<br>paranoid science fiction stories about<br>what a will do to us are untrue which<br>they may not be so assuming all those<br>all those good<br>things the combination of artificial<br>intelligence and the drift to more and<br>more Central ownership could make things<br>way worse than they were in the feudal<br>era here's why in the feud era as<br>we've<br>discussed people were required to grow<br>the food and be the Canon fter now if AI<br>cap abilities increase at the rate that<br>they do increase and I'll talk later<br>about what it would take for that to<br>happen and if p is right about the<br>concentration of ownership and I hope<br>he's wrong and I think there are ways we<br>can make him wrong but whether we will<br>is another question but if both those<br>things happen what we'll have is a set<br>of resources in the hands of a small<br>number of people which give them<br>everything they want and<br>need they won't need 99% of the<br>population to grow the food<br>they won't need anyone to be Canon<br>thotter essentially most of us will<br>become Surplus to<br>requirements or at least the<br>everything so how replaceable actually<br>are we and How likely is it that that<br>scenario where people can get all they<br>want from you know vast automated farms<br>and vast automated factories and vast<br>automated public services and vast<br>automated systems How likely is that to<br>be<br>true well of course we don't<br>know but even as things stand right now<br>it doesn't look<br>great you might think look surely people<br>are still to have their kids taught some<br>philosophy so there's a job for us in<br>this room at least some of us in this<br>room right<br>well I I suspect there sense that's not<br>going to be true even in a few years<br>time so teaching theorists estimate that<br>it's about eight times as good to be<br>taught oneon-one as it is to be taught<br>by a very fine teacher in the<br>classroom now let's say I'm you know<br>slightly egotistical about my teaching<br>skills and I think to myself I'm four<br>times as better as the AIS of three<br>years<br>time in<br>teaching but the AI can be one to one<br>anyone who wants them that makes being<br>taught by the AI four times as good as<br>me my university has already started<br>automating us so there are a few members<br>of staff at the University of Sydney<br>whose Publications have been absorbed<br>into a sublicensed version of a large<br>language model whose curricular have<br>been absorbed by the same sublicensed<br>version of that large language model and<br>the students can then ask questions of<br>the llm which is like a copy of the<br>lecturer and actually they're remarkably<br>plausible um it's only out for<br>volunteered but um it's coming to a<br>university near you um a university in<br>Melbourne that I I can't name because I<br>only know about this through back<br>channels um is moving to a process of<br>taking all the readings for all courses<br>and having them absorbed into a<br>sublicensed version of<br>GPT so that students will be able to ask<br>any question about any subject and<br>they'll get some sort of answer which is<br>much more relevant than you would get if<br>stands 10 years ago my general<br>practitioner said that he mostly agreed<br>with his automated diagnostic software<br>but when they disagreed he was right he<br>now says when they disagree he's never<br>right the a i is always<br>right I have a friend who Brian OK Conor<br>runs okon<br>consultancy who uh some years ago<br>employed quite large number of people<br>partly writing websites and doing work<br>like that he now employs nobody he draws<br>a little sketch diagram what he wants it<br>to look like he tells the robot what he<br>wants everything to do and all the<br>coding is kind of done for him until 2<br>or 3 weeks ago there's a bit of work to<br>be done debugging it and fixing it<br>because it made mistakes you had to talk<br>to it but since the latest uh 01 model<br>came out there's no more work to do it<br>comes out just fine the way really<br>expensive coders would<br>do it used to be in Sydney I don't know<br>what it's like here that law firms would<br>Outsource low value work to lawyers<br>trained in Australian law in Mumbai to<br>save some money they don't do that<br>anymore they Outsource it to AIS and<br>increasing ly everything reasonably<br>simple like contract writing is being<br>AIS one astonishing example I know of is<br>a company which used to have five or six<br>senior executive officers you know they<br>had a Chief Financial Officer and<br>operations officer and this that and the<br>other and the owner of it decided to get<br>rid of them all and run an AI system<br>that did it all that didn't work but<br>what you discovered was you had a<br>separate AI system for each of those<br>officers and forced the AI to email each<br>other it worked just fine every now and<br>they would say shouldn't we consult the<br>human about this and it would it all<br>work and that was five very senior<br>administrative Physicians down the<br>tubes so I went to a bit of research in<br>areas in which there's been massive job<br>loss due to Ai and you can imagine the<br>huge story manufacturing retail<br>warehousing and Logistics there are no<br>office blocks full of people working out<br>logistics anymore that's all changed in<br>the last six month months customer<br>service it used to be that those<br>annoying Bots that you talk to were<br>hopeless like in the last month<br>essentially if you're lucky and you hit<br>an annoying bot it won't be an annoying<br>bot it'll be in a bot that knows more<br>than the person you might otherwise have<br>been contacted connected to uh which is<br>both good and extremely depressing at<br>the same<br>time used to be that lots of people made<br>a lot of money out of being Traders now<br>a very small number of people make a<br>hell of a lot of money out of owning<br>automated systems that do Trading<br>and that's now we're at the very<br>beginning of all of this so it seems to<br>me not implausible that at the Practical<br>level uh all of this might become<br>sufficiently powerful that we do face<br>the possibility of the world where we<br>requirements well this might all sound a<br>bit familiar to you you might have<br>thought you've seen it<br>before so you might have seen Marx's<br>argument in the 19th century about the<br>crisis of capitalism and he was wrong<br>about this what he thought was this<br>there's an incentive to<br>automate so we will automate so we're<br>going to need fewer people but that<br>means fewer people will earn wages and a<br>few people are earning wages they won't<br>be able to buy the stuff that we're<br>producing by our automated system so<br>they'll be less demand and because of<br>that the capitalists will expire<br>there'll be sort of self-defeating<br>you'll you'll each each individual<br>capitalist has a powerful incentive to<br>automate but collectively they all end<br>up broke because no one can buy what<br>they produce incredibly cool argument I<br>remember spent a lot of time wondering<br>why it never<br>happened why didn't<br>it well it didn't happen in part because<br>labor the automation that happened in<br>the industrial revolution and through<br>most of the 20th century essentially<br>replaced physical labor but it created<br>lots and lots of intellectual labor<br>which meant that to a large extent you<br>didn't get the kind of mass lack of<br>employment and mass lack of salary that<br>would result in the capitalists going<br>broke so the capitalists did pretty well<br>out of this that's why it kind of didn't<br>happen so why should we not hope<br>that the depressing story which I was<br>telling you<br>about won't happen why should we think<br>that might not<br>happen well two things to say about it<br>one<br>is I don't think that my story<br>unfortunately has a depressing end for<br>the owners of the tech because on Marx's<br>Story the owners of the tech were<br>basically making one thing right they're<br>making widgets and they had to sell the<br>widgets in order to fund everything else<br>but on my story the owners of the tech<br>own the tech that make everything they<br>need so they don't need to sell any<br>widgets so there's no no pressure on the<br>owners of the tech to uh uh make sure<br>that there are people buying their stuff<br>because they just have to make the stuff<br>that's one reason and the second reason<br>is it's much less clear once the<br>intellectual labor is automated and<br>that's the fear of AI and maybe robotics<br>will get rid of some of the other<br>physical labor it's much Clear where the<br>extra jobs might be they might be in<br>personal services and butlering to the<br>rich but that is not a niche that the<br>vast majority of people are going to be<br>able to fill no one wants 10,000 to<br>personal<br>assistance<br>so I uh think that while Marx's<br>predictions didn't happen mine<br>might all right are there any causes for<br>optimism let's hope so here are<br>two maybe the owners with a capital O I<br>should WR a science fiction novel about<br>the owners shouldn't I maybe the owners<br>like kind of vanish into their own life<br>once they can make everything they want<br>they just leave everyone else<br>alone and uh when they've left everyone<br>else alone then the remaining people may<br>acquire or create or steal some AI Tech<br>2 and be able to make stuff and do stuff<br>and it'll end up messy and complicated<br>and dystopian but it won't end up as<br>mass starvation that's that's one<br>hope the owners don't decide that the<br>rest of us are a nuisance and need to be<br>gotten rid of and the other hope which<br>is even a nicer hope is that you might<br>might think that becoming an owner in<br>this sense will change the incentive<br>structure so different kinds of people<br>become powerful amongst the owners what<br>do I mean well there is a bunch of<br>evidence that people who are very senior<br>in Corporate America at<br>least um are much more likely to be<br>Psychopaths than anybody else so there's<br>the the Clyde bod's fabulous 2010 study<br>um was it called corporate psychopath<br>conflict and employee effective<br>well-being suggested that about 20% of<br>CEOs are psychopaths and you can see why<br>that would be right it's actually a very<br>helpful psychology to have if what you<br>want to do is get to the top of a large<br>corporation whose job is essentially<br>only earning money so you can imagine<br>that what's making it true that there<br>are a lot of psychopaths in large<br>corporations is the incentive structures<br>of large corporations now maybe if large<br>corporations are amongst the owners<br>and if you're no longer driven by making<br>a profit because you got stuff that will<br>do everything you need to have done<br>there'll be less of an incentive<br>structure for psychopaths to run the<br>show so maybe maybe influential amongst<br>the owners will be people with a more<br>humanitarian approach and more<br>interested in reorganizing wider Society<br>so that uh you know they might<br>um be able to use tech for the greater<br>good of people but I wouldn't want you<br>to bet your futures or the futures of<br>your children if those of you have any<br>on this little piece of hopefulness<br>right it's a a wild<br>speculation a lot of it's wild<br>speculation but it's a nested wild<br>speculation which might or might not be<br>true and you really don't want to rely<br>on<br>that so the only I think moderately<br>plausible way to head off this is to try<br>to bring it about within the next 10 or<br>20 years<br>that fully automated Tech AI Tech is<br>either socially owned or socially<br>controlled it needs to be there to do<br>things for the benefit of all of us<br>rather than for those who own us that<br>instead might pushes towards a post<br>scarcity Society rather than a Neo<br>feudal society even then there really<br>interesting questions like what is life<br>like for humans you don't have to do<br>stuff um um would I care about<br>philosophy if I thought I could never<br>add anything to it because the AI<br>philosophers were way ahead of me and I<br>could hardly understand what they were<br>doing so I would spend my time maybe<br>studying the greats either the dead<br>great humans or you know the great<br>AIS um how much of what we do depends on<br>some sense that we're looking after<br>ourselves and our loved ones and how<br>much of our motivations are based on the<br>intrinsic nature of what we do I just<br>don't know cool questions people have<br>discussed them let's not go there now<br>but it sounds a whole lot better than<br>future so that's kind of part one if<br>this Tech becomes much more potent than<br>it is<br>now then it's going to need to be<br>socially controlled or there's a serious<br>danger that most of us are Surplus to<br>what kind of tech would be powerful<br>enough to do this even given how<br>powerful what we have now is so I want<br>to do two things here firstly I'm going<br>to confess this in bait and switch I'm<br>not going to talk a great deal about<br>Consciousness but I will talk a little<br>bit about Consciousness but I will go on<br>to talk a lot about what it takes to be<br>a genuinely thinking thing and to have<br>genuine beliefs because that's what will<br>create create is what is required for<br>what's called you know automated general<br>intelligence a truly thinking AI system<br>and it's a truly thinking AI system that<br>is what we should be maximally worried<br>about in terms of either the science<br>fiction apocalypses or my Neo feudalist<br>Consciousness<br>so there are many many different<br>theories about Consciousness now I'm<br>going tonight to assume and some of you<br>won't share this assumption that is at<br>least in principle possible to have<br>conscious beings that are purely<br>physical maybe some of you think that it<br>requires some extra non-physical stuff<br>to be conscious that's a long debate we<br>could have it another time I'm going to<br>assume that it's true but I'm just going<br>to mention a few of the theories about<br>what it takes to be conscious<br>because it looks like on most of<br>them we are either there or almost there<br>now I'm not sure that they're correct<br>theories but if they were so here's the<br>first it's the so-called Global<br>workspace Theory burnard bears and in<br>more detail stanislav<br>danan according to the global workspace<br>Theory what it takes to be conscious is<br>in addition to having a vast store of<br>information and in addition to having<br>various bits of data coming in perhaps<br>by the sensors perhaps other sources<br>there needs to be a central Clearing<br>House a central Clearing House in which<br>only the most relevant data that is<br>required for what you're currently doing<br>is is kind of stored and that that<br>Central clearing house needs to be<br>massively interconnected in various<br>kinds of<br>ways that according to these guys is<br>what Consciousness is what you're aware<br>of what your awareness is on this theory<br>is this kind of central Clearing House<br>you know tons of stuff you're not aware<br>of most of it you're having endless<br>sensory input you're not aware of most<br>of it but a system select the most<br>important bits for the decisions you're<br>making and those bits put together<br>is now check GPT is not conscious<br>doesn't meet that if you think about the<br>whole thing the vast network but you<br>shouldn't think about the whole thing<br>instead you should think of there being<br>millions of instances of it namely the<br>chat gpts that probably most of you have<br>had discussions with sometime today am I<br>right who's had a discussion with chat<br>GPT today yeah most of you so each of<br>those instances that you've chatted to<br>today has two sources of information<br>that it's<br>keeps Salient at all times one is the<br>history of your chat and the second is<br>the global memory of your account the<br>global memory of your account is<br>restricted to about a page and a half<br>two pages of data and there's the mem<br>the allowable chat length varies but<br>it's no more more than 15,000 words that<br>stuff as it were kept in a kind of<br>global Center and anything you say to it<br>goes into that Global Center and those<br>things are Salient thought has got to do<br>next which is usually just tell you some<br>stuff so you might think that even chat<br>GPT is already not one but many many<br>millions of little tiny conscious agents<br>at least if the global workspace theory<br>is right you might just think this shows<br>the global xay theory is is not a good<br>theory so here's another some<br>controversial Theory um tonian CO's IIT<br>or integrated information Theory the<br>integrated information Theory says that<br>what Consciousness is is massively high<br>level of integration of information so<br>if you've got a whole bunch of<br>information stored separately it's not<br>conscious but if it's all available to<br>each other and interconnected that's<br>there are measures According to which<br>you can do the uh the magic numbers of<br>these IIT people have they have a<br>measure of Consciousness they call a<br>Consciousness meter depending on how<br>integrated the information is I think<br>it's bollock by the way I've written the<br>paper saying it's bollocks but a lot of<br>people believe it um by that measure it<br>turns out that chat GPT already passes<br>it so hat tip here to my graduate<br>student Nikki Vice who's done some work<br>on I IIT and llms which says that by the<br>standards of I it they pass if you want<br>some more about why it's bollock aside<br>from my paper then another my graduate<br>students Lena Wong has got some great<br>stuff about why this isn't an awesome<br>Theory but lots of people believe<br>it finally The Meta representation<br>Theory Uriah Regal KS and others<br>according to the Met representation<br>Theory Of Consciousness you're conscious<br>when you not only represent but you also<br>represent that you represent so you have<br>States inside of you that are<br>representations of the world but if you<br>also represent that you represent that<br>state about the world that's What Makes<br>You conscious so a non-conscious agent<br>might have you know a state inside them<br>that varies with the presence of what is<br>this stuff evap instant hand sanitizer<br>very<br>2022<br>um might have representation of that but<br>a conscious agent is whilst representing<br>the existence of this is also<br>representing the fact that they<br>represent<br>it okay and that's meant to be conscious<br>according to the meta representation<br>Theory well have you ever tried<br>asking gemini or chat gbt about whether<br>they meta represent you got to be<br>careful so here's what I did I said okay<br>CH gbt was it Gemini um are you aware<br>that Australia is in the southern<br>hemisphere yes it said of course I am<br>Australia is a continent<br>located is there anything else I can<br>and then I said are you aware that<br>you're aware that Australia was in the<br>Southern Hemisphere and it said I am not<br>conscious you're trying to make me say<br>that I'm conscious you're you're trying<br>to tell me that by the standards of the<br>Met representation Theory I'm conscious<br>but I'm not so I said a chat<br>GPT relax I'm not trying to do that I<br>just want to know in the perfectly<br>ordinary sense of whether you're aware<br>you're aware of this that you are and I<br>said oh well in that case of course I am<br>because I'm talking about the fact that<br>I'm aware of it so how could I not be<br>that I'm aware absolutely true thank you<br>CH gbt so by the standards of that<br>theory it's conscious as well I mean I'm<br>not you know I'm not a flag holder for<br>any of these theories but it just shows<br>you that people some people sort of<br>poooo the prospect that even the current<br>Tech has got clickers of Consciousness<br>but who<br>knows<br>okay<br>so that's theories of Consciousness but<br>I want to talk more<br>about another thing<br>which I think is a theory of<br>Consciousness but most people in the<br>philosophy of Mind who aren't<br>dualists think even if it's not a theory<br>of Consciousness it's a theory of what<br>is essential to have genuine<br>beliefs and what I want to show you is<br>that to meet this standard of what it is<br>essential to have genuine beliefs<br>creates a kind of powerful AI that is<br>both powerful enough to do the bad<br>things I talked about earlier and<br>possibly powerful enough to make the<br>apocalyptic science fiction worries real<br>worries as<br>well so both me David Charmers and<br>others who disagree on lots of stuff all<br>think that the following is what you<br>have you have to have a genuinely<br>believing system to be an automated<br>general intelligence you might think<br>that chat gpt's got a ton of data stored<br>in it but does it really believe things<br>and what are the standards it would take<br>things so let's just assume as I<br>mentioned earlier let's continue to<br>assume the py of argument that genuine<br>beliefs are things which physical<br>systems can uh have within them what<br>feature of the physical system will it<br>be that is a genuine<br>belief so here's something I think most<br>of us in phosy of Mind still think which<br>is that you should not identify any<br>mental state like a belief or a desire<br>with a type of physical<br>thing defined narrowly physically and<br>why is that well it's because the<br>physical things inside us that are our<br>beliefs and desires are all very<br>different so if you tried to work out<br>what kind of a physical thing my belief<br>that this laptop was here was and you<br>said oh it's these particular neurons in<br>this particular way you'll never find<br>that in you and you certainly won't find<br>it in dogs you certainly won't find it<br>in parrots you certain won't find it in<br>octopuses or any of the other Paradigm<br>by my standards obviously thinking<br>things that have genuine<br>beliefs so instead I think the Orthodoxy<br>is that you have to find out what kind<br>of a thing a physical system has to do<br>in order to be a belief they call it<br>doing the belief thing or as we say in<br>the jargon playing the belief role you<br>need to so some a physical thing if it<br>plays the belief role is a is a belief<br>there's a hacked example in my field<br>which is thermostats right you can make<br>a thermostat out of whatever you want<br>you can make a thermostat out of copper<br>and iron welded together as I was taught<br>in high school you can make a thermostat<br>out of a temperature sensitive diode you<br>can make a thermostat by putting Zach<br>into a box G him a label and every time<br>he starts to shiver he pulls the thing<br>down every time he stop shivering he<br>pulls it the other way you can make<br>thermostats in lots of different ways<br>they're all physical systems they're all<br>very different physical systems what<br>they have in common is doing the<br>thermostatic thing so what is doing the<br>Bel<br>thing in order to get our llms having<br>genuine beliefs they've got to do the<br>belief<br>thing<br>nothing by itself a belief doesn't do<br>anything so suppose I believe that the<br>door is<br>there what's that going to make me<br>do well if I just want to keep talking<br>to you it's going to make me do<br>nothing if I want to leave in a hurry<br>it'll make me do<br>this if I want to kiss the nearest door<br>it'll make me do something else I'm not<br>going to do because well maybe I can put<br>some of this on first<br>but<br>so what a belief is going to make me do<br>Depends entirely on what it is that I<br>want if I want to avoid doors at all<br>cost I go a oh no oh they're everywhere<br>oh up here at that corner that's the<br>best place to be if you want to avoid so<br>that's what my belief about where the<br>doors are will make me do if I want to<br>avoid doors<br>entirely so the belief doesn't do<br>anything to me unless we combine it with<br>a desire right the belief that the door<br>is there combine with a desire to go to<br>the door that's doing something so if<br>what it is to be real belief is to do<br>the belief thing we have to talk about<br>the<br>desires so what are desires well not<br>going to repeat everything I said about<br>beliefs right they let's just say there<br>are things that do the Desir thing what<br>is the D desirey thing what is it what<br>do you have to do to be a desire what do<br>you have to do to be the desire to go<br>out the door for<br>example once again the answer is<br>nothing unless you combine it with<br>beliefs okay if I want to go through the<br>door and I believe the door is there<br>sure I'll go through the door if I want<br>to go through the door and I believe the<br>do and so on until my belief so SE will<br>so you can only Define beliefs in terms<br>of<br>desires and Desires in terms of<br>beliefs you might think that that's a uh<br>a shocking package which tells you that<br>there's some sort of circularity here<br>but if we had more time I'll tell you<br>why not don't worry about that lots of<br>things are defined in these kind of<br>package deals like the classical theory<br>of the atom is a big package deal you<br>can't def any component of it without<br>defining it in terms of all the other<br>components but it kind of gets worse<br>than that right you can't define<br>individual beliefs impairs with<br>individual desires because what belief<br>stuff will do to<br>you depends on and Desir stuff will do<br>to you depends on your whole package of<br>beliefs and desires so I might believe<br>the door is there I might want to go<br>through the do there does that mean I'll<br>walk through the door well no what if I<br>believe the B's door is booby crapped<br>then don't want to go through the door<br>what if I believe there is a pit on the<br>other side I won't go through the door<br>so the whole of my belief space<br>influences what I will do when I desire<br>something and the whole of my desire<br>space does too because there might be<br>other desires that it's inconsistent<br>with so we need to think in terms of the<br>complete package of how we take the<br>world to be and a complete package of<br>how we want the world to be rather than<br>find individual beliefs and desires and<br>this is why a kind of 19 80s to 90s<br>project in cognitive science which was<br>trying to find individual beliefs and<br>Desires in people's heads failed because<br>there aren't any individual Desires in<br>people's heads there's a massive Network<br>or high dimensional phase space in the<br>jargon map of how things are and a high<br>dimensional phase-based map of how<br>so what does that mean for artificial<br>intelligence what it means for<br>artificial<br>intelligence is that in order to get<br>this sort of system off the<br>ground we need to create a massive<br>network of how the world<br>is and another massive network of how<br>the world is wanted to<br>be we've already got most of the first<br>things that's what llms are they are a<br>massive High dimensional Network<br>representing how it takes things to<br>be it generates that by doing the thing<br>that us philosophers wish our students<br>did which was to update their credences<br>according to Bas theorum from their<br>priers students regrettably don't do<br>that very often um AKA be<br>rational<br>and constantly modify that model of how<br>the world is in the light of new<br>evidence<br>um current llms can only get evidence<br>from stuff they read on the internet and<br>or what they're told but you can imagine<br>a situation in which they have wider<br>sources of evidence than just<br>that but you also<br>need that Global State representing how<br>the world is wanted to be because the<br>global State representing how the world<br>is wanted to be is the only thing which<br>when combined with that map can have the<br>kind of complicated interaction that<br>produces Behavior which you might think<br>of as being genuine thinking that's the<br>thing which would upgrade that map of<br>how things are from being just a map of<br>how things are to being a whole bunch of<br>beliefs because the belief is a thing<br>which combines with a desire to produce<br>some<br>Behavior so let's have a look at this<br>picture bits of which were drawn for me<br>by<br>AI I love this this is<br>fantastic this is so much better than a<br>laser<br>pointer right here we<br>are there it<br>is so what we have here is a bunch of<br>um distributed neural systems or rather<br>you know silicon copies distributed<br>neural systems that is your higher<br>dimensional phase based map of how<br>things are high dimensional meaning many<br>many many thousands of dimensions if not<br>more as opposed to real Maps which are<br>two-dimensional representations of<br>threedimensional<br>reality and here's<br>another massive map just like current<br>llms it sort of is very much like one of<br>those because it's a picture of how the<br>world is but you'll see for a moment how<br>this picture of how the world is will be<br>treated as a map of how the world is<br>wanted to be there's nothing about this<br>which is intrinsically very different<br>from this there are just two pictures of<br>how the world could<br>be we start<br>off with this one which is connected to<br>sources of evidence that thing there is<br>evidence it's the the I of Sauron slash<br>the eye of the uh the eye of the AI<br>which is telling it how to update its<br>its uh<br>probabilities this has no such<br>I and what happens is we have a kind of<br>a comparator and what the comparator<br>does is Compares this representation<br>with this representation notes the<br>difference and then produces outputs<br>actions which are designed to affect the<br>world so that this thing becomes more in<br>line with this thing you see the idea<br>right the idea is simply that there how<br>you think the world is there how you<br>want the world to be and you do stuff so<br>is to make the world more like you want<br>the world to be nothing mysterious in<br>all<br>that so the thought is a genuinely<br>thinking thing that was would be bit<br>like us would simply<br>require two<br>llms one of which is connected to<br>evidence and the other one of which has<br>a starting state which is different from<br>that starting state of representing the<br>World As We would like it to want it to<br>be and it does comparisons and it<br>calculates what if you did it would then<br>make that more like<br>that now you can cheat in two<br>ways and humans cheat in both these ways<br>let me tell you about the ways we cheat<br>here's the first thing you can do you<br>can say I want to make that more like<br>that what can I do I know I'll change<br>that any guesses what kind of cheating<br>that<br>is it's wishful thinking it's there's a<br>way you want things to be think okay let<br>just I'll just pretend that's how things<br>are so wishful thinking will be one way<br>of short circuiting this process so one<br>thing we're going to have to do is have<br>some kind of rule that says is that none<br>of these actions should directly affect<br>this they can only indirectly affect it<br>by affecting how the world is and then<br>the eye of saon will have a look at how<br>the world is and then update accordingly<br>uh so and it'll see how you've changed<br>things and if you change things to make<br>them less like this then you'll be upset<br>it'll be upset if you make change things<br>to make things more like this then<br>you'll be<br>happy that's something I think we know<br>how to do more or less because we do<br>know roughly how to update neural<br>networks based on evidence by basian<br>transformation that's something which is<br>well here's the second thing you way you<br>can cheat and humans cheat this way too<br>that's you can say what I want is to<br>make these things more similar can't be<br>bothered doing anything so what I'll do<br>is change my mind about what I want<br>right I will just adjust my picture of<br>how I want things to be so that is just<br>in alignment with how I think things are<br>and you end up being the way you know<br>false and crude caricatures of Buddhism<br>say that Buddhism says you should be<br>which is simply accepting that how<br>things are is the best of all possible<br>world so it's it's pangos it's pangian<br>ISM so the the panglossian will from<br>voler kid uh will just change this to<br>match that and you need to rule that out<br>as<br>well but here's the problem<br>this thing needs to evolve and<br>change we know that it has to evolve and<br>change because when you get a goal you<br>need to be able to have sub<br>goals if my desire is get to the top of<br>the mountain and I know that the road<br>begins at uh tyer Street then I need to<br>have a sub goal of getting into tyer<br>street so I need to represent the world<br>is one in which this that I you know<br>that that I am at tyari street I'm<br>trying to achieve that right so we need<br>to be able to change this thing but no<br>one has the faintest clue how this works<br>either in us or in ouris no one has the<br>faintest clue how desires are supposed<br>to evolve over time or change over<br>time if we could just fix them it' be<br>awesome because we could say that would<br>solve What's called the alignment<br>problem of AI right which is aligning<br>the goals and outputs of AI to our own<br>needs and<br>desires but you can't just freeze this<br>so you need to have a a theory of how we<br>can make it evolve over time and being<br>the kind of distributed Network thing<br>that it is it just will evolve over time<br>too so you need to have a theory of how<br>it will do it and intervene if it<br>evolves in strange<br>ways and that really matters I mean I'm<br>sure since two-thirds of you have<br>already talked to the AI today that<br>you'll be very familiar with the crazy<br>hallucinations that it comes up with um<br>it believes I'm a professor at Cambridge<br>which I'm not actually doesn't believe<br>it only stores this alleged information<br>it says I'm inventor of a theory of<br>philosophy of Mind called structuralism<br>about the mind which is untrue but it's<br>a very cool Theory entirely invented by<br>the AI um it hallucinates all kind of<br>crazy stuff I remember asking it just<br>the other<br>day where the so-called Coke machine<br>example which you may not know about in<br>philos mind comes from I mistyped it and<br>I put cone machine and it came up with<br>this awesome theory about the cone<br>machine which is very plausible and<br>attributed it to the exact philosopher<br>who would have had such a theory if<br>anyone is going to have such a<br>theory but complete hallucination now<br>hallucinations as they so as they're<br>called they're fine when it just comes<br>to facts right they're kind of random<br>changes not random but there are changes<br>in its picture of how things are which<br>result in the saying oh you know shut up<br>CH GPD fix that not a<br>problem but similarly bizarre changes in<br>how it wants things to be we don't want<br>that that could be anything it could be<br>terrible who knows what sort of thing it<br>might want that is kind of a<br>hallucinated desire as opposed to an<br>hallucinated<br>belief so it really is crucial that we<br>have a good theory about how to make<br>evolve if we don't then we may end up<br>with things which are very powerful I<br>think we will because people who own<br>these things want them to be as power<br>powerful as possible this is how they<br>got to be in order to make them as<br>powerful as possible but until we solve<br>that problem<br>then not only will the safety issues the<br>social and political safety issues the<br>first half of my talk be ones we're<br>facing also some of the more worrying<br>but Le perhaps less likely science<br>fiction worries will be true too if we<br>Evolution so that is something I've all<br>told you about so yeah some final<br>remarks look I've ignored all the good<br>and awesome things about artificial<br>intelligence there are many<br>but that's because um so I I haven't<br>talked about many of the bad ones you<br>know the scenarios the quity which AI<br>might turn the world into paperclips<br>because the first and most powerful ones<br>descended from a paperclip generating<br>machine that has its primary goal making<br>paperclips or other s of Science Fiction<br>panics but I have highlighted two key<br>things one of them social and political<br>and one of them is uh more theoretical<br>philosophical and<br>Technical both of them are I think<br>absolutely crucial to the future of<br>humanity I said it didn't matter<br>how probable to a degree these<br>depressing scenarios are and I think<br>that's really true so let's suppose my<br>depressing scenario about Neo feudalism<br>likely I think probably more than that<br>actually but if it's 1% likely it's a 1%<br>chance of the biggest historical<br>catastrophe we can imagine for<br>Humanity how bad is that well well<br>surely a massive catastrophe for 99% of<br>humanity is which totally changes<br>history forever is worse for example<br>than 1% of the world I.E about 5 million<br>people being wiped out in the nuclear<br>Holocaust and you get worried about that<br>with that's with certainty right so 100%<br>chance of you know uh of a very large<br>number of people being wiped out with<br>holoc in nuclear Holocaust is about as<br>bad as a 1% chance of this kind of<br>massive catastrophe really is something<br>you need to think about and I don't want<br>to be a preacher of Doom because I think<br>that there are two things that we can do<br>in these connected in different ways the<br>first is as I said before try to<br>regulate or perhaps socially own AI<br>resources so they aren't completely<br>dominated by the owners and so we are<br>not Surplus to the requirements of those<br>who can make<br>everything and the second is we need to<br>fund people to come up with a theory<br>about how Desir change change and how<br>desire generating systems can be<br>stabilized because that's the only way<br>we will get safe AI systems that have<br>the kind of full power of<br>thought so if anyone asks you to sign a<br>petition asking you to fund either of<br>these two things sign it because I think<br>aside from global warning these may be<br>the two most crucial things that we need<br>to do to defend our Collective future<br>thank you<br>thank you very much David uh very<br>stimulating talk so we have some time<br>now for questions and answers so if<br>you'd like to ask the speaker a question<br>please raise your hand and I'll come<br>around with the microphone uh so the<br>we go so I should say I've never<br>actually even used chat<br>gbt my<br>microphone I wanted to ask um about the<br>kind of<br>sociopolitical um side of things and it<br>sort of seemed to me that the<br>dystopian situation that you were<br>describing it's not actually a form of<br>feudalism um because in that situation<br>it's essentially still a kind of<br>capitalist structure where the Ordinary<br>People Dey have got the right to sell<br>their labor power to whoever wants to<br>buy it it's just that it's going to be<br>much much harder because all of the work<br>is is done by robots so the situation of<br>a ordinary person in that in that<br>scenario is different from the case of a<br>surf where the Jey they don't have that<br>power because they're bonded to<br>particular um<br>so looks more like a kind of varent form<br>of capitalism um and as such I mean I<br>just wonder about you know<br>the I guess it's one of the things that<br>that Marx argued was<br>that one of the contradictions in<br>capitalism is on the one hand<br>capitalists have to push down the wages<br>of their<br>workers in order to keep our profit<br>margin as large as possible that they<br>also need to have people buying their<br>product<br>toiz I mean I just wonder what happens<br>in the in the dystopian situation so<br>who's actually going to buy the things<br>that are produced yeah yeah system it's<br>not going to be us because we all M<br>that's right is it going to be the<br>robots they going to buy things y yep<br>thanks so that's exactly why I sort of<br>think it's whatever it is it's not<br>capitalism it's not strictly feudalism<br>either right because there are no bonds<br>of mutual obligation and all all the<br>rest of it but I don't think it's<br>capitalism because it's it's not a<br>situation where there's a market anymore<br>no one needs to buy anything because<br>people who own the tech and have the<br>tech make whatever they need they don't<br>have to sell anything that's the crucial<br>difference I mean Marx's prediction was<br>that the capitalists would go broke<br>because as as you've said and as I said<br>earlier um as automation happened wages<br>would get pushed down with push down<br>wages or even no wages because of mass<br>unemployment people couldn't buy stuff<br>if they couldn't buy stuff the<br>capitalists couldn't make money and that<br>would be a disaster so you had a company<br>that made widgets companies that made<br>biggets company that made squidgets and<br>each of them uh pushed down the wages of<br>their workers and no one could then<br>either buy Widgets or squidgets or piggs<br>but in my dystopian scenario the people<br>who own the tech don't have to sell<br>their widgets their Tech will make all<br>the different things that they<br>need um and so in that sense they as it<br>were get off scotf free and escape from<br>the contradiction which is what's so<br>toopan about<br>it a situation where um so an individual<br>owner um who wants to go on holiday gets<br>his AI to build the jumbo jet um but<br>also um grows the the turn ups yeah look<br>they may well be trade between the<br>owners of the stuff and they may end up<br>specializing to some degree in what they<br>are making and and you know so one owner<br>may make a few jumbo Jets and trade that<br>for the other owners things but that's<br>going to be a kind of intra intra ruling<br>class trade rather than um rather than<br>classic capitalism and it's scarier<br>could I could I get you to put your<br>little two diagramm yeah<br>sure<br>yeah I think you can anyway yeah<br>there 14 yeah I was just thinking when<br>you were talking about that you were<br>talking about two potential things you<br>might want to stop and what on the left<br>hand side what you called wishful<br>thinking yeah and on the right hand side<br>well I guess in a negative sense you<br>could call lowering your expect<br>that's right morei of acceptance<br>yeah the one thing that we know does<br>have Consciousness and self awareness<br>does both of those things does yeah and<br>so I thought well first if you want<br>something to have Consciousness and goal<br>why would you want to stop it doing what<br>we do but on the on the other and I<br>think maybe acceptance might be<br>something which would stop an AI getting<br>out of control and doing what it wants<br>and changing the world drastically so<br>maybe they're not negative things you<br>want to stop my question yeah good<br>question so the first thing to say<br>is I don't know the wishful thinking I<br>mean we do wishful thinking but I'm not<br>at all sure that's a good thing maybe<br>it's a good thing if you know if there's<br>something you can do nothing about and<br>it's very hard just to sit there and<br>relax and maybe the best thing to do is<br>to falsely believe it's all going to be<br>okay so your last hours are Pleasant I<br>mean yeah so maybe there are cases in<br>which we thinking and something to be<br>something to be said for it um but you<br>don't really want your AI to wishfully<br>think because it won't then be able to<br>solve problems if it thinks the problems<br>are already solved that's one reason why<br>in general you done with that as for<br>acceptance<br>um acceptance is it's a complicated<br>matter right so sometimes I agree<br>acceptance is good so when you're<br>confronted with a situation that you<br>don't want you should think and I try to<br>think what should I do should I go go<br>for acceptance and say actually there's<br>no reason to hang on to this desire or<br>should I come up with a strategy for<br>trying to achieve this<br>desire what you can't do and what you<br>don't want your AI to do is to go into<br>state of complete acceptance because<br>then it won't do anything and you want<br>it to do things for you and what's<br>generating actions is the fact that<br>these two things are different so if<br>that thing is fully accepting I same as<br>that then that you'll get null outputs<br>and it'll do nothing and presuming<br>that's not what you want AI for so yeah<br>maybe it's okay if there's a certain<br>degree of acceptance but it better not<br>be enough acceptance that it gets in the<br>way of actually getting outputs from<br>your system but it could be as much<br>acceptance as us oh possibly so yeah<br>possibly so yeah we don't have very much<br>yeah I was very pleased to hear about<br>your model of you know what it is to<br>have a genuine belief because I thought<br>this on a wet Sunday afternoon and I'm<br>really pleased to find that the really<br>smart people who thought about this in<br>depth think the same thing it's really<br>Charles but um in connection with this I<br>thought there might be a problem with<br>both beliefs and desires yeah you might<br>have a thing which you might think of as<br>a partial belief yeah so it's kind of<br>like do you mean partial belief in the<br>standard technical sense no<br>but it doesn't connect up with the the<br>all the other Desires in a way you might<br>think it would so an example of a real<br>life belief would be somebody believes<br>in an afterlife yeah there's this famous<br>um conversation reported by Berton<br>Russell who yeah was challenging someone<br>um so what do you think is happening to<br>this dead person now well I suppose<br>they're enjoying Eternal Bliss but I do<br>wish you wouldn't talk about such<br>unpleasant topics yeah yeah okay now<br>what what what do we want to say about<br>this person well they sort of believe it<br>and they sort of not not simply because<br>you know the level of credence that<br>isn't the problem the problem is they<br>sort of believe it in some respects and<br>it connects up with some of the desires<br>to generate some actions and it doesn't<br>do it in other contexts in the in the<br>way that you might expect if we want to<br>say they really believed it y y y Okay<br>now what's the relevance to of that to<br>this um well maybe that's going to be<br>true of um some of the ai's um um<br>beliefs they might not connect up in the<br>normal way that's especially if they're<br>not properly embodied yeah so uh that<br>was<br>um I thought this was you know really<br>interesting um but that's a possible um<br>sure um problem well it might be<br>optimistic I don't know but it's a um a<br>thing about developing a sort of really<br>truly believing AI maybe our our AIS<br>would not have really or truly beliefs<br>because they don't connect up fully in<br>the way that a sort of properly<br>functioning belief connect up with the<br>ai's yeah okay you're forcing me to come<br>out and confess that I don't really<br>believe in beliefs at<br>all um and uh try and remove one of the<br>layers of simplification on the story<br>right so I think that there are two very<br>different phenomena maybe even three<br>that go me to belief one of them is this<br>thing called Credence which is roughly<br>speaking How likely you think things are<br>and how likely you think things are as<br>tested by how much you're prepared to<br>bet on them and I don't mean by bet on<br>them you know go to the betting shop I<br>mean how much you're prepared to engage<br>in various kinds of behaviors and you<br>can work out from there How likely you<br>think things are and on the theory of<br>credence no one should think something<br>is certain because that's irrational no<br>one think something's impossible cuz<br>that's irrational as<br>well but it would be terrible for it if<br>we went around all the time thinking in<br>terms of likelihoods and doing constant<br>basan updating of the kind that existing<br>AI systems do so suppose for example you<br>currently think that there's a 92%<br>chance that your partner is faithful to<br>you and you're a basian updator you'll<br>be constantly searching for evidence and<br>updating your credences and your<br>partner's faithfulness all the time and<br>you'll be divorced quite<br>soon that really is not a good idea and<br>there are lots of other situations in<br>which is not a good idea to be<br>constantly updating your probabilities<br>and acting like a perfectly rational<br>basian agent so there's this second<br>thing you call belief which is the stuff<br>that you act on when you freeze How<br>likely you think it is and you stop<br>being attentive to the evidence in a<br>constant updating kind of way and that<br>can be a a quite virtuous thing to do<br>maybe it's also good to have some sort<br>of subpersonal system which keeping a<br>bit of an eye on the evidence so if it<br>gets too out of hand you can start<br>updating again but um but yeah there<br>certainly are benefits to doing that<br>there's a second way in which this was<br>an idealization and that is that at<br>least in US it looks like things can be<br>beliefs for the purposes of some sorts<br>of outputs and not beliefs for the<br>purposes of other sorts of outputs so<br>one example I often given is stories<br>I've been told by friends who went to<br>Catholic boarding schools According to<br>which they were told there were certain<br>activities which would result in Eternal<br>damnation<br>um but on a daily basis they performed<br>these activities and they thought it was<br>you know reasonably likely that one day<br>they could get struck down before they<br>got a chance to repent in which case<br>they are facing Eternal damnation but<br>they weren't stressed all the time now<br>this makes no sense right um so you<br>might want to say they don't really<br>believe these activities cause Eternal<br>damnation but this internal state which<br>I'm calling the belief that the activity<br>will cause Eternal damnation it plays<br>the role of creating sincere<br>assertion it plays the role of making<br>you answer that way in an exam if you<br>believe that God was watching you and<br>giving you the correct answer right so<br>it controls some of your<br>outputs um some of your behaviors but it<br>doesn't control others so it's a<br>something which I sometimes use the<br>phrase it plays some of the belief role<br>but not all of the belief role so I<br>talked about how to be a belief is to do<br>the belief thing well there's lots of<br>stuff inside our heads which do a bit of<br>the belief thing but not all of the<br>belief thing and there's stuff side of<br>heads it does a bit of the Desir thing<br>and not all the Desir<br>thing um whether we are able to to<br>produce AIS that do only the belief<br>stuff so that all the beliefs control<br>things in the way that you know the<br>abstract Theory says I don't know<br>whether we want to do that I don't know<br>um whether it's a good idea to have<br>ai freeze their credences the way it is<br>for us will depend a lot on what the AI<br>is for and what it's doing<br>or whether it should constantly update<br>the way allegedly ideal rational agents<br>should constantly update but yeah that's<br>a cool<br>question the bamal model that you have<br>there um is very sort of<br>an centered and relies very much on this<br>idea of a<br>belief more Emeral than many other data<br>sets we have um also those beliefs uh<br>set by very much by the social context<br>by the society not by the agent itself a<br>social phenomen very often in those<br>cases so um and also the structure here<br>is going to be usually we know we have<br>subsystems and and where you have the<br>belief and the um model in the same sort<br>of unit more or less and other ones are<br>combined together so the architecture is<br>going to be a lot more involved<br>separations but that separation is a<br>separate um model of what we want that's<br>Highly Questionable we could have ai<br>that was not dominated by our model<br>of we apply AI to different domains at<br>the moment we must not judge AI by JP JP<br>is bound to the human vernacular y<br>absolutely so that model appropriate for<br>the<br>potential also it ability to help us<br>work out what's going to be best for<br>humankind because if you're looking at<br>criteria for ownership maybe AI can<br>decide what we can't hope hopefully so<br>so also you menine that concentration of<br>power but at the moment is some<br>democratization with a whole lot of<br>Splinter firms in AI getting cheaper<br>models and access of data so it's not<br>quite so clear that that domination<br>absolutely that's that's also a hopeful<br>scenario look I want to say I don't<br>think I'm being anth anthropocentric<br>here all so I'm not as we starting from<br>a kind of a a conception of what beliefs<br>and desires are like substantively in<br>humans at all because there's Charles<br>said this would be an absurd abstraction<br>from how from how beliefs and desires<br>are in humans I'm starting at the most<br>abstract level something which clearly<br>applies to AI which is that beliefs and<br>desires are the things which when you<br>have both of them are models of how<br>things are and models of how things<br>wanted to be make the AI do something so<br>to the extent that even even llms do<br>something at all they do have very basic<br>desires which is to answer questions<br>right in the absence of that they they<br>wouldn't do those things and all I mean<br>by desire is is a thing which when<br>combined with a belief generates an<br>output or a behavior of some kind I'm<br>not trying to be anthropocentric about<br>all hi um thanks it it seems to me that<br>this process of AI working out how the<br>world is the leftand diagram is going to<br>be iterative and Progressive for example<br>we aren't straight away in the first<br>iteration going to know what the correct<br>version of Ethics is so how the right<br>hand side should look yeah so we're<br>going to have changing and different<br>views of what is desired yeah my<br>question is how<br>are machines<br>AIS or us going to combine and decide<br>between those Visions in human political<br>and social work we use democracy um with<br>all its flaws and strengths um you could<br>argue we also bring into it various<br>degrees of probability of of how like<br>How likely we our beliefs are to be be<br>true though we don't model that very<br>well in in Democratic structure so how<br>do you think we should have our AIS<br>choose between multiple different maps<br>of how things are desired and will it in<br>any way resemble human politics well<br>that's that's the question I raise right<br>you should tell me the answer to that um<br>I have no idea how to evolve this thing<br>and then there is the social and<br>political question and what should its<br>initial State be that's I think your<br>question right what how should you set<br>the up now if what you want is a system<br>which is expert in plumbing then you<br>know how to do it you make it want to<br>fix things you make it want to take the<br>right amount of money and you blah blah<br>blah but if you have a general General<br>automated intelligence then I have no<br>clue of how to how to set that but you<br>rais another interesting question which<br>is a perennial question for ethicists<br>which is where should we<br>locate ethical beliefs or moral beliefs<br>or indeed are there genuine moral<br>beliefs are moral um moral claims<br>beliefs about the way the world is this<br>thing is good do they belong in that<br>domain or do they somehow or other<br>belong are they actually a kind of<br>disguised desire do they belong in that<br>domain um now<br>I am amongst those who put ethical<br>beliefs as not really being beliefs but<br>as disguised desires I suppose I'm you<br>know the broad tradition of of baby Hume<br>so I'll have those things in that model<br>that's the model of how I want things to<br>be there'll be others in this room who<br>will be very sure that there are<br>Bonafide beliefs which are moral beliefs<br>which should go into there in which case<br>you know in theory if you uh train up AI<br>then on how things are it will learn how<br>things should be I've always been very<br>skeptical about this I have a draft<br>paper at the moment called Emanuel<br>and The Killer Robots<br>it sort of goes like this um K thought<br>that morality was a species of<br>rationality so that the perfectly<br>rational person would you know end up<br>having all the the correct moral views<br>and the thought is very simple thought<br>something like are you worried about the<br>robot apocalypse don't worry they're<br>going to be perfectly rational we've<br>built in ideal basy in reasonings of<br>course they will acquire all the quick<br>moral beliefs there's no problem oh what<br>questions yeah very interesting<br>talk<br>okay don't turn it off it's just not<br>very loud yeah thank you so I'm a<br>computer scientist so um yeah not a<br>philosopher yeah but uh I think I agree<br>with you what you're talking about we<br>need to regulate you know regulation to<br>regulate Ai and also the ownership of AI<br>that's very important I think but the<br>second Point regarding the desire belief<br>I think very often we overe exaggerate<br>the power of AI from computer science<br>point of view because the AI system is<br>not more than a passive knowledge base<br>and then you just query it so you never<br>have any AI system normally like you<br>initiate something even if they have a<br>belief they have a desire it's their<br>Master their owner impos into the system<br>they never have any free will if they<br>have free will we want to create them or<br>we we want to be able to enable the Free<br>Will of the system because of this so<br>any the AI system no matter how FY we<br>talk about it's just a you know desire<br>or just free will or not free will<br>desire or belief or conscious they are<br>very different from our human beings uh<br>that thing our human being the most<br>important thing apart from that above<br>that is our Free Will which AI system<br>from my point of view never will never<br>ever have and this is a huge difference<br>between AI system and a human<br>being sorry I have no question yeah<br>that's question yeah thank thank you so<br>so two things uh firstly I agree that<br>existing AI systems are to some extent<br>largely passive uh knowledge basis right<br>because they just<br>this so what I was suggesting as what<br>the generalization of AI will be and<br>what it will take to create um genuine<br>thinking AIS is to take something which<br>is internally structured just like this<br>so it's a kind of representation of how<br>World could be and then have this patent<br>pattern matching thing<br>producing actions and that's when you<br>won't just have a passive knowledge base<br>you'll also have a view about how things<br>should be you get to compare how things<br>should be with well just things being<br>like that with how things are and then<br>it'll produce behaviors designed to make<br>the world more like that and that's the<br>point to which you don't have just a<br>passive knowledge base now does that<br>have free will<br>gosh I think did the other Taylor<br>lecture talk about free will that was<br>certainly on her short list she did it<br>on the Wednesday morning okay did it on<br>the Wednesday morning right so does that<br>have free will that really depends on<br>what you mean by free will so there are<br>stories about Free Will According to<br>which we don't have it even we don't<br>have it there are stories According to<br>which we do have free will and so so<br>would that there are stories According<br>to which we might have free will and<br>that doesn't have it there are stories<br>According to which neither of the things<br>have it so that's that's a conversation<br>we'd have to make much<br>longer thank<br>you C thanks very much um so I'm also a<br>computer scientist uh and hopefully I'm<br>not enough of a psychopath to be a tech<br>bro um cuz I'm in education but if I<br>were I wouldn't I wouldn't want the AI<br>to be able to generate its own desires I<br>would want to impose my desires on the<br>AI and so to what extent is the AI to<br>have desires as such a requirement for<br>the dystopia or can I crudely just edit<br>Wikipedia to say Steve Mills owns<br>everything and he's the best and then<br>that becomes a belief for the AI and it<br>carries on yeah and you just use the<br>belief system with a simulation of what<br>you want yeah in order to generate the<br>the rightand network yeah so there's<br>certainly something you could do if<br>you're a tech bro which is to uh rapidly<br>erase an update this all the time and<br>replace it with a new version of exactly<br>the things you want and not let it<br>evolve that's certainly something you<br>could do uh my guess is just that that's<br>going to get short circuited at some<br>stage um and um and that the system<br>itself if we don't understand how that<br>thing evolves over time we can certainly<br>set it to an initial state which it says<br>that we're all great and we should do<br>this that and the other but it has to<br>evolve a little bit in order to generate<br>sub goals so it has to allow some<br>Evolution and then the question is how<br>how much can we allow safely do we have<br>to keep resetting it uh will there be<br>some cost in how powerful the system is<br>if we have to keep resetting it and will<br>one day we fail to reset it soon enough<br>I mean they're they're the<br>concerns yeah glad to see there so much<br>question uh thank you as all very<br>interesting um I guess I have a one<br>point one question so an alternative<br>View to having a model exactly like this<br>is used in reinforcement learning where<br>they have um basically uh rewards<br>specified in the world I don't think I<br>mean that might make things a little bit<br>different you don't have to necessarily<br>store the whole map of how things are<br>but probably more interesting and this<br>is where my question comes in um Marcus<br>hutter I think has a paper about if we<br>have if we develop a a reinforcement<br>learning type agent that's smart enough<br>and powerful enough then it can hack the<br>map of how things are through its<br>perceptions so that they are exactly the<br>same as the map of how<br>things it once it to be I wonder if you<br>have any thoughts well that's yeah<br>that's that's the wishful thinking<br>option and well it's a bit more than<br>wishful thinking because it's actually<br>changed so for example if your desire is<br>to have no litter on the ground<br>you got a camera you just get a picture<br>and put in front of the camera with no<br>litter on the ground oh I see so so<br>you're not hacking the Central Central<br>map you're hacking the input system so<br>as to make it seem yeah that that's<br>another version of the same worry and<br>it's a real worry I've got to say this<br>stuff is astonishingly scary in the '90s<br>I worked in AI briefly I was um I worked<br>a year and a half doing automated theor<br>improving and all we proved was<br>extremely boring theorems and it made me<br>believe that AI was never going to get<br>anywhere and uh I was wrong about that I<br>was<br>WR all right uh again glad to see there<br>so much interest apologies to people who<br>still have questions you're warmly<br>invited to come and join us at the<br>reception afterward and ask your<br>questions to David uh just before I wrap<br>up I wanted to just extend uh<br>appreciation and thanks to uh Karen<br>McLean who helped arrange both last week<br>and this week she put got the guests<br>here and got them accommodated and got<br>all the food together and it couldn't<br>have happened without her<br>uh so I just wanted to personally thank<br>Karen for making this possible thank you<br>Karen<br>Bravo and for a very relevant and<br>terrifying lecture I would like to thank<br>uh David very much and join please join<br>us for the reception which is over in<br>the in the gallery of the staff club<br>building so thank you very much and good<br>evening</p></main><footer style="margin-top: 2rem; background: #0001; padding: 2rem; text-align: center;"><p>We Are The University</p><ul style="list-style-type: none; padding: 0; margin: 0;"><li><a href="/">Home</a></li><li><a href="/about">About</a></li><li><a href="/contact">Contact</a></li></ul></footer></body></html>